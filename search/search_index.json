{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal Notes Collection","text":""},{"location":"#textbook-reference","title":"Textbook Reference","text":"<p>Abstract Algebra: Abstract Algebra, Theory and Applications</p>"},{"location":"todo/","title":"Broken Link","text":"<p>This is an internal-only page, for use when wanting to link to a page that has not been written.</p>"},{"location":"math/abstract-algebra/16-rings/","title":"Chapter 16 - Rings","text":""},{"location":"math/abstract-algebra/16-rings/#section-161-rings","title":"Section 16.1 - Rings","text":"<p>Definition. A nonempty set \\(S\\) is a ring if, with two binary operations called addition and multiplication, the following are satisfied:</p> <ol> <li>Addition is commutative. \\(a + b = b + a\\) for \\(a, b \\in R\\)</li> <li>Addition is associative. \\((a + b) + c = a + (b + c)\\) for \\(a, b, c \\in R\\)</li> <li>There exists a zero-element \\(0_R\\) in \\(R\\) such that \\(a + 0 = a\\) for all $a \\in $</li> <li>Every element \\(a\\) has an additive inverse \\(-a \\in R\\) such that \\(a + (-a) = 0_R\\)</li> <li>Multiplication is associative. That is, \\(a(bc) = (ab)c\\) for \\(a, b, c \\in R\\)</li> <li>The Distributive Property holds. That is, \\(\\forall a, b, c \\in R,\\)</li> </ol> \\[ a(b+c) = ab+bc \\\\ (a+b)c = ac + bc \\] <p>Definition. If there exists some element \\(1_R \\in R\\) such that \\(1a = a1 = a\\) for all \\(a \\in R\\), we say that \\(R\\) is a ring with unity or identity.</p> <p>Note that some books impose the condition that \\(1 \\neq 0\\). If \\(1 = 0\\), we can show the ring only has one element.</p> <p>Definition. If \\(ab = ba\\) for all \\(a, b \\in R\\), the ring is said to be a commutative ring.</p> <p>Definition. If a ring \\(R\\) is commutative, \\(R\\) is an integral domain if and only if for every \\(a, b \\in R\\), \\(ab = 0\\) implies that either \\(a = 0\\) or \\(b = 0\\).</p> <p>Definition. An element \\(a \\in R\\) is called a unit if there exists some \\(a^{-1}\\) such that \\(a a^{-1} = a^{-1} a = 1\\).</p> <p>Definition. A ring \\(R\\) with identity is called a division ring if every nonzero element in \\(R\\) is a unit.</p> <p>Definition. A commutative division ring is called a field. That is, in a field, every element has an inverse.</p> <p>Definition. A subset \\(S\\) of ring \\(R\\) is a subring if given any \\(r, s \\in S\\), then \\(rs \\in S\\) and \\(r - s \\in S\\).</p>"},{"location":"math/abstract-algebra/16-rings/#section-162-integral-domains-and-fields","title":"Section 16.2 - Integral Domains and Fields","text":"<p>Definition. If \\(R\\) is a commutative ring and \\(r \\in R\\), then \\(r\\) is said to be a zero divisor if there is some nonzero \\(s \\in R\\) such that \\(rs = 0\\).</p> <p>Definition. A commutative ring with no zero divisors is called an integral domain.</p> <p>Example. Consider the set \\(\\mathbb{Z}[i] = \\{m + ni | m, n \\in \\mathbb{Z}\\}\\). This ring is called the Gaussian integers. Prove that the Gaussian integers are not a field, and are an integral domain.</p> <p>Example. Proposition 16.15: Cancellation law. Let \\(D\\) be a commutative ring with identity. Then, \\(D\\) is an integral domain if and only if for every nonzero \\(a \\in R\\), \\(ab = ac\\) implies \\(b = c\\).</p> <p>Theorem. 16.16: Every finite integral domain is a field.</p> <p>Definition. For any non-negative integer \\(n \\in \\mathbb{N}\\) and \\(r \\in R\\), we say that \\(nr = r + \\ldots + r \\text{(n times)}\\).</p> <p>Definition. The characteristic of a ring is the least possible \\(n \\in \\mathbb{N}\\) such that \\(nr = 0\\) for all \\(r \\in R\\).</p> <p>Example. For every prime number \\(p\\), \\(\\mathbb{N}_p\\) is a field of characteristic \\(p\\).</p> <p>Lemma. 16.18: Given \\(R\\) is a ring with identity, the characteristic of \\(1\\) is the characteristic of the field.</p> <p>Theorem. 16.19: The characteristic of an integral domain is prime or zero.</p>"},{"location":"math/abstract-algebra/16-rings/#section-163-ring-homomorphisms-and-ideals","title":"Section 16.3 - Ring Homomorphisms and Ideals","text":"<p>Definition. Given rings \\(R\\) and \\(S\\), and a mapping \\(\\varphi: R \\rightarrow S\\), we say that \\(\\varphi\\) is a ring homomorphism if the following are satisfied for all elements of \\(R\\):</p> \\[ \\begin{align}     \\varphi(a + b) &amp;= \\varphi(a) + \\varphi(b) \\\\     \\varphi(ab) &amp;= \\varphi(a) \\varphi(b) \\end{align} \\] <p>Definition. If \\(\\varphi\\) is one-to-one and onto, it is an isomorphism.</p> <p>Definition. For any ring homomorphism \\(\\varphi\\), the kernel of \\(\\varphi\\) is the set</p> \\[ \\ker \\varphi = \\{ r \\in R | \\varphi(r) = 0 \\} \\] <p>Definition. Proposition 16.22: Let \\(\\varphi: R \\rightarrow S\\) be a ring homomorphism. Then,</p> <ol> <li>If \\(R\\) is a commutative ring, then \\(\\varphi(R) \\subseteq S\\) is a commutative ring.</li> <li>\\(\\varphi(0_R) = 0_S\\)</li> <li>Let \\(1_R\\) and \\(1_S\\) be the identities in \\(R\\) and \\(S\\). If \\(\\varphi\\) is onto, then \\(\\varphi(1_R) = 1_S\\)</li> <li>If \\(R\\) is a field an \\(\\varphi(R) \\neq \\{0\\}\\), then \\(\\varphi(R) \\subseteq S\\) is a field.</li> </ol> <p>Definition. A subring \\(I \\subseteq R\\) is asn ideal of \\(R\\) if, when given \\(a \\in I, r \\in R\\), then \\(ar\\) and \\(ra\\) are both in \\(I\\). That is, \\(rI \\subseteq I\\) and \\(Ir \\subseteq I\\).</p> <p>Definition. Given a commutative ring \\(R\\) with identity, and \\(r \\in R\\), the set</p> \\[ \\langle a \\rangle = (r)R = \\{ ar : r \\in R \\} \\] <p>is an ideal in \\(R\\). Specifically, \\(\\langle a \\rangle\\) is a principal ideal.</p> <p>Example. Theorem 16.25. Every ideal in \\(\\mathbb{Z}\\) is a principal ideal.</p> <p>Example. With \\(\\varphi: R \\rightarrow S\\), \\(\\ker \\varphi\\) is an ideal of \\(R\\).</p> <p>Remark. 16.28: We are working with two-sided ideals. If rings are not commutative, we may deal with left ideals and right ideals.</p> <p>Theorem. 16.29: Let  \\(I\\) be an ideal of \\(R\\). Then, the factor/quotient ring \\(R/I\\) is a ring with multiplication defined by</p> \\[ (r + I)(s + I) = rs + I \\] <p>Theorem. 16.30: Let \\(I\\) be an ideal of \\(R\\). Then, the map \\(\\varphi: R \\rightarrow R/I\\) defined by \\(\\varphi(r) = r + I\\) is a ring homomorphism of \\(R\\) onto \\(R/I\\) with \\(\\ker \\varphi = I\\).</p> <p>Theorem. 16.31, First Isomorphism Theorem. Let \\(\\psi: R \\rightarrow S\\). Then, \\(\\ker \\psi\\) is an ideal of \\(R\\). Consider the isomorphism \\(\\varphi: R \\rightarrow R/\\ker \\psi\\). There exists an isomorphism \\(\\eta: R / \\ker \\psi \\rightarrow \\psi(R)\\) such that \\(\\psi = \\eta \\varphi\\).</p> <p>Theorem. 16.32, Second Isomorphism Theorem. Let \\(I\\) be a subring of \\(R\\) and \\(J\\) be an ideal of \\(R\\). Then, \\(I \\cap J\\) is an ideal of \\(I\\) and</p> \\[ I/I \\cap J \\cong (I + J) / J \\] <p>Theorem. 16.33, Third Isomorphism Theorem. Let \\(R\\) be a ring and \\(I, J\\) be ideals of J. If \\(J \\subsetneq I\\), then</p> \\[ R/I \\cong \\frac{R/J}{I/J} \\] <p>Theorem. 16.34, Correspondence Theorem. Let \\(I\\) be an ideal of \\(R\\). Then, \\(S \\mapsto S/I\\) is a one-to-one correspondence between the set of subrings \\(S\\) containing \\(I\\) (that is, \\(I \\in S\\)) and the set of subrings of \\(R/I\\). Furthermore, the ideals of \\(R\\) containing \\(I\\) correspond to the ideals of \\(R/I\\).</p>"},{"location":"math/abstract-algebra/16-rings/#section-164-maximal-and-prime-ideals","title":"Section 16.4 - Maximal and Prime Ideals","text":"<p>Definition. Consider ring \\(R\\) and proper ideal \\(M \\subseteq R\\). Then, \\(M\\) is a maximal ideal of \\(R\\) if the ideal \\(M\\) is not a subset of any ideal except \\(R\\) itself. That is, given any ideal \\(I\\) properly containing \\(M\\), \\(I = R\\).</p> <p>Theorem. 16.35: Given a commutative ring with identity \\(R\\), \\(M\\) is a maximal ideal if and only if \\(R/M\\) is a field.</p> <p>Definition. Consider ring \\(R\\) and proper ideal \\(P \\subseteq R\\). Then, \\(P\\) is a prime ideal if given \\(ab \\in P\\), either \\(a \\in P\\) or \\(b \\in P\\).</p> <p>Theorem. 16.38: Let \\(R\\) be a commutative ring with identity \\(1\\). Then, \\(P \\subseteq R\\) is a prime ideal of \\(R\\) if and only if \\(R/P\\) is a field.</p> <p>Let us assume that \\(P\\) is an ideal in \\(R\\) and \\(R/P\\) is an integral domain. Take two elements \\(ab \\in P\\). Now, consider \\(a + P\\) and \\(b + P\\) in \\(R/P\\) such that \\((a+P)(b+P) = 0+P = P\\). As \\(R/P\\) is a field, either \\(a + P = 0 + P = P\\) or \\(b + P = 0 + P = P\\), meaning either \\(a \\in P\\) or \\(b \\in P\\). Thus, \\(P\\) is as prime ideal.</p> <p>Now, assume the opposite. Let \\(P\\) be prime. Now, we want to show that \\(R/P\\) is an integral domain.</p> <p>Consider two elements \\(a + P\\), \\(b + P\\) in \\(R/P\\). We know that</p> \\[ (a + P)(b + P) = ab + P = 0 + P = P \\] <p>Thus, \\(ab \\in P\\). By symnetry, assume \\(a \\notin P\\). Thus, \\(b \\in P\\) by the definition of a prime ideal, so \\(b + P = 0 + P\\), meaning \\(R/P\\) is an integral domain.</p> <p>Theorem. 16.40: In a commutative ring with identity, every maximal ideal is also a prime ideal.</p>"},{"location":"math/abstract-algebra/16-rings/#section-165-applications-to-computer-science","title":"Section 16.5 - Applications to Computer Science","text":"<p>Lemma. Let \\(m, n \\in \\mathbb{B}\\) be given. Then, for any \\(a, b \\in \\mathbb{Z}\\), there exists some \\(x\\) that satisfies</p> \\[ \\begin{align}     x &amp;\\equiv a \\pmod{m} \\\\     x &amp;\\equiv b \\pmod{n} \\end{align} \\] <p>Theorem. Chinese Remainder Theorem. Let \\(n_1, \\ldots, n_k \\in \\mathbb{N}\\) be given such that \\(\\gcd(n_i, n_j) = 1\\). Then, for any integers \\(a_1, \\ldots, a_k\\), the system</p> \\[ \\begin{align}     x &amp;\\equiv a_1 \\pmod{n_1} \\\\     x &amp;\\equiv a_2 \\pmod{n_2} \\\\     \\ldots     x &amp;\\equiv a_k \\pmod{n_k} \\end{align} \\] <p>has a solution. Additionally, all systems are congruent modulo \\(n_1 n_2 \\ldots n_k\\).</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/","title":"Chapter 17 - Polynomial Rings","text":""},{"location":"math/abstract-algebra/17-polynomial-rings/#section-171-polynomial-rings","title":"Section 17.1 - Polynomial Rings","text":"<p>Throughout this chapter, we will assume that \\(R\\) is a commutative ring with identity.</p> <p>Definition. Any expression of the form</p> \\[ f(x) = \\sum_{i=0}^n a_i x^i = a_0 + a_1x + a_2x^2 + \\ldots + a_n x^n \\] <p>where \\(a_i \\in R\\) and \\(a_n \\neq 0\\) is called a polynomial over \\(R\\) with indeterminate \\(x\\). The elements \\(a_0, a_1, \\ldots, a_n\\) are the coefficients of \\(f\\). The coefficient \\(a_n\\) is the leading coefficient.</p> <p>Definition. A polynomial is known as monic if the leading coefficient is equal to \\(1\\).</p> <p>Definition. The degree of \\(f\\) is the largest nonnegative number such that \\(a_n \\neq 0\\), written as \\(\\deg f(x) = n\\). If no such number exists, that is, \\(f(x) = 0\\), we say the degree of \\(f\\) is $-\\infty%.</p> <p>Definition. We denote the set of all polynomials with coefficients in \\(R\\) as \\(R[x]\\).</p> <p>Two polynomials are equal if and only if their corresponding coefficients are equal. When combined with standard addition and multiplication, \\(R[x]\\) forms a ring.</p> <p>Theorem. If \\(R\\) is commutative and has identity, so does \\(R[x]\\).</p> <p>Definition. The ring of polynomials with \\(n\\) indeterminates and coefficients in \\(R\\) is defined as \\(R[x_1][x_2][\\ldots][x_n] = R[x_1, x_2, \\ldots, x_n]\\).</p> <p>Definition. The evaluation homomorphism is the homomorphism \\(\\varphi: R[x] \\rightarrow R\\) defined as \\(\\varphi(p(x)) = p(\\alpha)\\) for some \\(\\alpha \\in R\\).</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/#section-172-the-division-algorithm","title":"Section 17.2 - The Division Algorithm","text":"<p>Theorem. Given \\(f(x), g(x) \\in F[x]\\), where \\(F\\) is a field and \\(g(x) \\neq 0\\), there exist unique polynomials \\(q(x), r(x) \\in F[x]\\) such that</p> \\[ f(x) = g(x)q(x) + r(x) \\] <p>where either \\(\\deg r(x) &lt; \\deg g(x)\\) or \\(r(x)\\) is the zero polynomial.</p> <p>Corollary. Let \\(F\\) be a field. Then, an element \\(\\alpha \\in F\\) is a zero of \\(p(x) \\ in F[x]\\) if and only if \\((x-\\alpha)\\) is a factor of \\(p(x)\\).</p> <p>Corollary. Let \\(F\\) be a field. Then, a nonzero polynomial \\(p(x) \\in F[x]\\) with degree \\(n\\) can have at most \\(n\\) distinct zeros in \\(F\\).</p> <p>Definition. A monic polynomial \\(d(x)\\) is the greatest common divisor of polynomials \\(p(x), q(x) \\in F[x]\\) if \\(d(x)\\) evenly divides both \\(p(x)\\) and \\(q(x)\\). We write \\(\\gcd(p(x), q(x)) = d(x)\\). This polynomial is unique.</p> <p>Definition. Two polynomials are relatively prime if their greatest common divisor is \\(1\\).</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/#section-173-irreducible-polynomials","title":"Section 17.3 Irreducible Polynomials","text":"<p>Definition. A non-constant polynomial \\(f(x) \\ in F[x]\\) is irreducible over a field \\(F\\) if it cannot be expressed as the product of two non-identity polynomials \\(g(x)\\) and \\(h(x)\\) in \\(F[x]\\), with the degree of both polynomials strictly less than the  degree of \\(f(x)\\).</p> <p>Lemma. Let \\(p(x) \\in \\mathbb{Q}[x]\\). Then, with \\(r, s \\in \\mathbb{Z}, a(x) \\in \\mathbb{N}[x]\\), we can write \\(p(x) = \\frac{r}{s} a(x)\\).</p> <p>Lemma. Gauss's Lemma. Let \\(p(x) \\in \\mathbb{Z}[x]\\) be monic such that \\(p(x)\\) factors into two polynomials \\(\\alpha(x), \\beta{x} \\in \\mathbb{Q}[x]\\), with the degrees of both strictly less than the degree of \\(p(x)\\). Then, there exists two polynomials \\(a(x), b(x) \\in \\mathbb{Z}[x]\\) such that \\(p(x) = a(x)b(x)\\), and \\(\\deg \\alpha(x) = \\deg a(x)\\) and \\(\\deg \\beta(x) = \\deg b(x)\\).</p> <p>Corollary. Let \\(p(x) \\in \\mathbb{Z}[x]\\) be monic with constant term \\(a_0\\). Then, if \\(p(x)\\) has a zero in \\(\\mathbb{Q}\\), then it also has a zero \\(\\alpha\\) in \\(\\mathbb[Z]\\). Furthermore, \\(\\alpha\\) divides \\(a_0\\).</p> <p>Theorem. Eisenstein's Criterion. Let \\(p\\) be prime, and suppose that</p> \\[ f(x) = a_n x^n + \\ldots + a_0 \\in \\mathbb{Z}[x] \\] <p>Then, if \\(p | a_i\\) for \\(0 \\leq i &lt; n\\), but \\(p \\nmid a_n\\) and \\(p^2 \\nmid a_0\\), then \\(f(x)\\) is irreducible over \\(\\mathbb{Q}[x]\\).</p> <p>Theorem. If \\(F\\) is a field, then every ideal in \\(F[x]\\) is a principal ideal.</p> <p>Theorem. Let \\(F\\) be a field, and suppose \\(p(x) \\in F[x]\\). Then, the ideal \\(&lt;p(x)&gt;\\) is maximal if and only if \\(p(x)\\) is irreducible.</p>"},{"location":"math/abstract-algebra/18-integral-domains/","title":"Chapter 18 - Integral Domains","text":""},{"location":"math/abstract-algebra/18-integral-domains/#section-181-fields-of-fractions","title":"Section 18.1 - Fields of Fractions","text":"<p>Definition. Given an integral domain \\(D\\), we can construct a field \\(F\\) containing \\(D\\) by stating that any \\(p/q \\in F\\), and that any two elements \\(a/b = c/d\\) if and only if \\(ad = bc\\). We can consider this akin o a set of ordered pairs</p> \\[ S = \\{(a, b) : a, b \\in D \\text{ and } b \\neq 0 \\} \\] <p>Lemma. 18.1: The relation \\((a, b) ~ (c, d) \\text{ if } ad = bc\\) is an equivalence relation.</p> <p>Lemma. 18.2: The operations of addition and multiplication on \\(F\\) are well-defined.</p> <p>Lemma. 18.3: The set of equivalence classes of \\(S, F\\) under \\(~\\) form a field.</p> <p>Theorem. 18.4: Let \\(D\\) be an integral domain. Then, \\(D\\) can be embedded in a field of fractions \\(F_D\\) where any element in \\(F_D\\) can be expressed as the quotient of two elements in \\(D\\).</p> <p>Additionally, \\(F_D\\) is unique. That is, given field \\(E\\) such that \\(E \\supset D\\), there exists a map \\(\\psi: F_D \\rightarrow D\\) giving an isomorphism such that \\(\\psi(a) = a\\) for all \\(a \\in D\\).</p> <p>Corollary. 18.6: Let \\(F\\) be a field of characteristic \\(0\\). Then, \\(F\\) contains a subfield isomorphic to \\(\\mathbb{Q}\\).</p> <p>Corollary. 18.6: Let \\(F\\) be a field of characteristic \\(p\\). Then, \\(F\\) contains a subfield isomorphic to \\(\\mathbb{Z}_p\\).</p>"},{"location":"math/abstract-algebra/18-integral-domains/#section-182-factorization-in-integral-domains","title":"Section 18.2 - Factorization in Integral Domains","text":"<p>Definition. Let \\(R\\) be a commutative ring with identity, and \\(a, b \\in R\\). We say that \\(a\\) divides \\(b\\), that is, \\(a | b\\), if there exists some \\(c \\in R\\) such that \\(b = ac\\).</p> <p>Definition. A unit element is any element that has a multiplicative inverse.</p> <p>Definition. Two elements \\(a, b \\in R\\) are said to be associates if there exists some unit \\(u \\in R\\) such that \\(a = ub\\).</p> <p>Definition. Let \\(D\\) be an integral domain. A nonzero element \\(p \\in D\\) is said to be irreducible if when given \\(p = ab\\), either \\(a\\) or \\(b\\) is a unit.</p> <p>Definition. Let \\(D\\) be an integral domain. A nonzero element \\(p\\) is prime if when given \\(p = ab\\), either \\(p | a\\) or \\(p | b\\).</p> <p>Definition. Given integral domain \\(D\\), we say that \\(D\\) is a Unique Factorization Domain (UFD) if it satisfies the following criteria:</p> <ol> <li>Given \\(a \\in D, a \\neq 0\\), and \\(a\\) is not a unit, \\(a\\) can be written as a product of irreducible elements in \\(D\\).</li> <li>Let \\(a = p_1 \\ldots p_r = q_1 \\ldots q_s\\), where \\(p_i\\) and \\(q_i\\) are all irreducible. Then, \\(r = s\\), and there exists some function \\(\\pi \\in S_r\\) such that \\(p_i\\) and \\(q_{\\pi(j)}\\) are associates for \\(j = 1, \\ldots, r\\).</li> </ol> <p>Definition. A ring \\(R\\) is a principal ideal domain (PID) if every ideal of \\(R\\) is principal.</p> <p>Lemma. 18.11: Let \\(D\\) be an integral domain and \\(a, b \\in D\\). Then,</p> <ol> <li>\\(a | b\\) if and only if \\(\\langle b \\rangle \\subseteq \\langle a \\rangle\\)</li> <li>\\(a\\) and \\(b\\) are associates if and only if \\(\\langle b \\rangle = \\langle a \\rangle\\)</li> <li>\\(a\\) is a unit in \\(D\\) if and only if \\(\\langle a \\rangle = D\\).</li> </ol> <p>Theorem. 18.12: Let \\(D\\) be a PID, and let \\(\\langle p \\rangle\\) be a nonzero ideal in \\(D\\). Thus, \\(\\langle p \\rangle\\) is a maximal ideal if and only if \\(p\\) is irreducible.</p> <p>Corollary. 18.13: Let \\(D\\) be a PID. For any \\(p \\in D\\), if \\(p\\) is irreducible, then \\(p\\) is prime.</p> <p>Lemma. 18.14: Let \\(D\\) be a PID. Let \\(I_1 \\subseteq I_2 \\subseteq \\ldots\\). Then, there exists some integer \\(N\\) such that \\(I_n = I_N\\) for all \\(n &gt; N\\). That is, any chain of ideals converges.</p> <p>Definition. Any commutative ring that satisfies the above condition (the ascending chain condition), even if it's not a PID, is called a Noetherian ring.</p> <p>Theorem. 18.15: Every PID is a UFD. Note that the converse is not true.</p> <p>Corollary. 18.16: Let \\(F\\) be a field. Then, \\(F[x]\\) is a UFD.</p> <p>Definition. Any integral domain \\(D\\) is a Euclidean domain with a Euclidean function \\(nu: D \\\\ \\{0\\} \\rightarrow \\mathbb{N}\\) that satisfies the following:</p> <ol> <li>Given \\(a, b \\neq 0\\), then \\(\\nu(a) \\leq \\nu(ab)\\).</li> <li>Given, \\(a, b \\in D\\) and \\(b \\neq 0\\), there exists some \\(q, r \\in D\\) such that \\(a = bq + r\\) and either \\(r = 0\\) or \\(\\nu(r) &lt; \\nu(b)\\).</li> </ol> <p>Example. Absolute value on \\(\\mathbb{Z}\\) is a Euclidean validation.</p> <p>Example. Degree on \\(F[x]\\) is a Euclidean validation.</p> <p>Example. \\(\\nu(a + bi) = a^2 + b^2\\) is a Euclidean validation over \\(\\mathbb{Z}[i]\\).</p> <p>Theorem. 18.21: Every Euclidean domain is a PID.</p> <p>Corollary. Every Euclidean domain is a UFD.</p> <p>Definition. Given a polynomial \\(p(x) \\in D\\), with \\(D\\) being an integer domain, we say that the content of \\(p(x)\\) is the greatest common divisor of its coefficients. Additionally, if the content is \\(1\\), we say that \\(p(x)\\) is primitive.</p> <p>Theorem. 18.24: Let \\(D\\) be a UFD, and \\(f(x), g(x) \\in D[x]\\) be primitive. Then, \\(f(x)g(x)\\) is primitive.</p> <p>Lemma. 18.25: Given \\(D\\) is a UFD, and \\(p(x), q(x) \\in D[x]\\), the content of \\(p(x)q(x)\\) is equal to the product of the contents of the individual polynomials</p> <p>Lemma. 18.26: Let \\(D\\) be a UFD and \\(F = F_D\\) be its field of fractions. Given \\(p(x) \\in D[x]\\), and \\(p(x) = f(x)g(x)\\) with \\(f(x), g(x) \\in F_D\\), we can say that \\(p(x) = f_1(x)g_1(x)\\) with \\(f_1(x), g_1(x) \\in D\\). Additionally, \\(\\deg f_1(x) = \\deg f(x)\\) and \\(\\deg g_1(x) = \\deg g(x)\\).</p> <p>As a direct consequence, we see the following.</p> <p>Corollary. Let \\(D\\) be a UFD, and \\(F = F_D\\). Then, a primitive polynomial \\(p(x) \\in D[x]\\) is irreducible in \\(D[x]\\) if and only if it is irreducible in \\(F[x]\\).</p> <p>Corollary. Let \\(D\\) be a UDF, and \\(F = F_D\\). Then, if a monic polynomial \\(p(x) \\ in D[x]\\) can be written as \\(p(x) = f(x)g(x)\\) with \\(f(x), g(x) \\in F_D[x]\\), then \\(p(x)\\) can be written as \\(p(x) = f_1(x)g_1(x)\\), where \\(f_1(x), g_1(x) \\in D[x]\\).</p> <p>Theorem. If \\(D\\) is as UFD, then \\(D[x]\\) is a UFD.</p> <p>Corollary. This theorem has several corollaries:</p> <ol> <li>Given a field \\(F\\), since \\(F\\) is a PID, it is also a UFD. Thus, \\(F[x]\\) is a UFD.</li> <li>The ring of polynomials over integers, \\(\\mathbb{Z}[x]\\) is a UFD.</li> <li>Given \\(D\\) is a UFD, \\(D[x]\\) is a UFD. Thus, \\(D[x_1, x_2]\\) is a UFD, and by induction, \\(D[x_1, \\ldots, x_n]\\) is a UFD.</li> </ol>"},{"location":"math/abstract-algebra/DF-10-modules/","title":"Dummit &amp; Foote Chapter 10 - Modules","text":""},{"location":"math/abstract-algebra/DF-10-modules/#section-101-basic-definitions-and-examples","title":"Section 10.1 - Basic Definitions and Examples","text":"<p>Definition. Let \\(R\\) be a ring. A left \\(R\\)-module or a left module over \\(R\\) is a nonempty set \\(M\\) together with</p> <ol> <li>A binary operation \\(+\\) on \\(M\\) under which \\(M\\) is an abelian group</li> <li>An action \\(\\times\\) of \\(R\\) on \\(M\\), that is, a map or function \\(R \\times M \\rightarrow M\\), denoted \\(rm\\), that for all \\(r, s \\in R, m, n \\in M\\) satisfies<ul> <li>\\((r + s)m = rm + sm\\)</li> <li>\\((rs)m = r(sm)\\)</li> <li>\\(r(m + n) = rm + rn\\)</li> <li>If \\(R\\) has identity \\(1\\), then \\(1m = m\\)</li> </ul> </li> </ol> <p>Theorem. If \\(R\\) is commutative, any left-module is also a right-module.</p> <p>Remark. Modules over a field \\(F\\) and vector spaces over \\(F\\) are identical.</p> <p>Definition. An R-submodule is a subset\\(N \\subseteq M\\) which is closed under the action taken forall \\(r \\in R\\). That is, given \\(r \\in R, n \\in N\\), then \\(rn \\in N\\). Every module has at least two submodules: itself and the trivial (empty) submodule.</p> <p>Remark. If \\(F\\) is a field, submodules are equivalent to subspaces.</p> <p>Example. Let \\(F\\) be a field and \\(F[x]\\) a polynomial ring. Then, let \\(V\\) be a vector space of \\(F\\), and \\(T\\) be a linear transformation from \\(V\\) to itself. That is, \\(V: T \\rightarrow T\\). We know that \\(V\\) is an \\(F\\)-module. We will want to show that \\(V\\) can be written as an \\(F[x]\\)-module for some choice of \\(T\\). That is, we want an action \\(F[x] \\times V \\rightarrow V\\).</p> <p>Now, for a given linear transformation \\(T\\), consider some polynomial \\(p(x) = a_n x^n + \\ldots + a_0\\) and some \\(v \\in V\\). We define \\(p(x) \\times v\\) by$</p> \\[ p(x) \\times v = a_n T^n(v) + a_{n-1} T^{n-1}(v) + \\ldots + a_0 v \\] <p>with \\(T^n\\) being defined as applying \\(T\\) a total of \\(n\\) times.</p> <p>Proposition. Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. Then, a subset \\(N\\) of \\(M\\) is a submodule of \\(M\\) if and only if</p> <ol> <li>\\(N \\neq \\emptyset\\)</li> <li>For all \\(r \\in R\\), \\(x, y \\in N\\), then \\(rx - y \\in N\\)</li> </ol> <p>Definition. Let \\(R\\) be a commutative ring with identity. An \\(R\\)-algebra is a ring \\(A\\) together with a ring homomorphism \\(f: R \\rightarrow A\\) such that \\(\\varphi(1_R) = 1_A\\). Thus, the subring \\(f(R) \\subseteq A\\) is contained in the center of \\(A\\).</p> <p>Recall. The center of a ring \\(A\\) is the subring \\(A'\\) such that for all \\(x, y \\in R'\\), then \\(xy = yx\\). In other words, it is the commutative subring of \\(A\\).</p> <p>Definition. Given two \\(R\\)-algebras \\(A, B\\), an *\\(R\\)-algebra homomorphism$ is a ring homomorphism \\(\\varphi: A \\rightarrow B\\) that maps \\(1_A \\rightarrow 1_B\\) such that \\(\\varphi(ra) = r\\varphi(a)\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-102-quotient-modules-and-module-homomorphisms","title":"Section 10.2 - Quotient Modules and Module Homomorphisms","text":"<p>Definition. Let \\(R\\) be a ring and \\(M, N\\) be \\(R\\)-modules. then a ring homomorphism \\(\\varphi: M \\rightarrow N\\) is an \\(R\\)-module homomorphism if for all \\(r \\in R\\), \\(\\varphi(rx) = r\\varphi(x)\\).</p> <p>Theorem. An \\(R\\)-module homomorphism is an isomorphism if it is 1-1 and onto, and said modules are isomorphic.</p> <p>Definition. Let \\(M, N\\) be \\(R\\)-modules. The set \\(\\text{Hom}_R(M, N)\\) is the set of all homomorphisms from \\(M\\) to \\(N\\).</p> <p>Proposition. Let \\(M\\), \\(N\\), and \\(L\\) be \\(R\\)-modules. Then,</p> <ol> <li>A function \\(\\varphi: M \\rightarrow N\\) is an \\(R\\)-module homomorphism if and only if \\(\\varphi(rx + y) = r\\varphi(x) + \\varphi(y)\\) for all \\(x, y \\in M\\) and \\(r \\in R\\).</li> <li>Let \\(\\varphi, \\psi \\in \\text{Hom}_R(M, N)\\). Then, define \\(\\varphi + \\psi\\) as</li> </ol> \\[ (\\varphi + \\psi)(m) = \\varphi(m) + \\psi(m) \\] <p>Then, \\(\\varphi + \\psi \\in \\text{Hom}_R(M, N)\\). Additionally, if \\(R\\) is commutative, with \\((r\\varphi)(m) = r(\\varphi(m))\\), then \\(r\\varphi \\in \\text{Hom}_R(M,N)\\) 3. If \\(\\varphi \\in \\text{Hom}_R(L, M)\\) and \\(\\psi \\in \\text{Hom}_R(M, N)\\), then \\(\\psi \\circ \\varphi \\in \\text{Hom}_R(L, N)\\) 4. \\(\\text{Hom}_R(M, M)\\) is a ring with identity. With \\(R\\) being commutative, \\(\\text{Hom}_R(M, M)\\) is an \\(R\\)-algebra.</p> <p>Proposition. Let \\(R\\) be a ring, \\(M\\) an \\(R\\)-module, and \\(N \\subseteq M\\) an \\(R\\)-submodule. then, \\(M/N\\) can be made into an \\(R\\)-module by defining addition. With \\(r \\in R\\) and \\(x + N \\in M/N\\),</p> \\[ r(x + N) = (rx) + N \\] <p>That is,</p> \\[ r \\overline{x} = \\overline{rx} \\] <p>Definition. Let \\(A, B\\) be submodules  of the \\(R\\)-module \\(M\\). Then, the sum of \\(A\\) and \\(B\\) is defined as</p> \\[ A + B = {a + b | a \\in A, b \\in B} \\] <p>This is the smallest submodule that contains both \\(A\\) and \\(B\\).</p> <p>Theorem. First Isomorphism Theorem. Let \\(M, N\\) be \\(R\\)-modules, and \\(\\varphi: M \\rightarrow N\\) be an \\(R\\)-module homomorphism. Then, \\(\\ker \\varphi\\) is a submodule of \\(M\\), and \\(M / \\ker \\varphi \\cong \\varphi(M)\\).</p> <p>Theorem. Second Isomorphism Theorem. Let \\(A, B\\) be submodules of the \\(R\\)-module \\(M\\). Then, \\((A + B)/B \\cong A/(A \\cap B)\\).</p> <p>Theorem. Third Isomorphism Theorem. Let \\(M\\) be an \\(R\\)-module, and \\(A \\subseteq B\\) be submodules of \\(M\\). Then, \\(\\frac{M/A}{B/A} \\cong M/B\\).</p> <p>Theorem. Lattice Isomorphism Theorem. Let \\(N\\) be a submodule of the \\(R\\)-module \\(M\\). Then, there is a bijection between submodules of \\(M\\) containing \\(N\\) and submodules of \\(M/N\\). This is given by \\(A \\leftrightarrow A/N\\), for \\(A \\supseteq N\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-103-generation-of-modules-direct-sums-and-free-modules","title":"Section 10.3 - Generation of Modules, Direct Sums, and Free Modules","text":"<p>Definition. Let \\(M\\) be an \\(R\\)-module and \\(N_1, \\ldots, N_n\\) be submodules of \\(M\\).</p> <ol> <li>The sum of \\(N_1, \\ldots, N_n\\) is the set of all finite sums of elements from the sets \\(N_i\\). That is, \\(N_1, \\ldots, N_n := \\{a_1 + a_2 + \\ldots + a_n | a_i \\in N_i\\}\\)</li> <li>For any subset \\(A\\) of \\(M\\), let \\(RA = \\{r_1 a_1 + r_2 a_2 + \\ldots + r_m a_m | r_i \\in R, a_i \\in A\\}\\). If \\(N\\) is a submodule of \\(M\\) such that \\(N = RA\\), then \\(A\\) is called the generating set for \\(N\\).</li> <li>A submodule \\(N\\) of \\(M\\) is finitely generated if there is some finite subset \\(A\\) of \\(M\\) such that \\(N = RA\\). That is, \\(N\\) is generated by some finite subset.</li> <li>A submodule of \\(M\\) (up to equality) is \\(cyclic\\) if there exists some element \\(a \\in M\\) such that \\(N = Ra = \\{ra | r \\in R\\}\\).</li> </ol> <p>Definition. Let \\(M_1, \\ldots, M_k\\) be a collection of \\(R\\)-modules. Then, the direct product is defined as</p> \\[ M_1 \\otimes \\ldots M_k = (m_1, \\ldots, m_k), m_i \\in M_i \\] <p>This direct product is in itself an \\(R\\)-module.</p> <p>Proposition. Let \\(N_1, \\ldots, N_n\\) be submodules of the \\(R\\)-module \\(M\\). Then, the following are equivalent:</p> <ol> <li>The map \\(\\pi: N_1 \\otimes \\ldots \\otimes N_k \\rightarrow N_1 + \\ldots + N_k\\) defined by \\(\\pi(a_1, \\ldots, a_n) = a_1 + \\ldots + a_n\\) is an isomorphism</li> <li>\\(N_j \\cup (N+1 + \\ldots + N_{j-1} + N{j+1} + \\ldots + N_n) = 0\\) for all \\(j \\in \\{1, 2, \\ldots, k\\}\\)</li> <li>Every \\(x \\in N_1 + \\ldots + N_n\\) can be written uniquely in the form \\(a_1 + \\ldots + a_n\\), with \\(a_i \\in N_i\\)</li> </ol> <p>Definition. An \\(R\\)-module \\(F\\) is said to be free on the subset \\(A\\) of \\(F\\) if for every nonzero \\(x \\in F\\), there exists nonzero elements \\(r_1, \\ldots, r_n\\) of \\(R\\) and unique \\(a_1, \\ldots, a_n\\) such that \\(x = r_1 a_1 + \\ldots + r_n a_n\\) for some \\(n \\in \\mathbb{Z}^+\\). That is, \\(A\\) is a basis or set of free generators of \\(F\\).</p> <p>Theorem. For any set \\(A\\), there is a free \\(R\\)-module \\(F(A)\\) on \\(A\\) such that \\(F(A)\\) satisfies the universal property: if \\(M\\) is any \\(R\\)-module, and \\(\\varphi: A \\rightarrow M\\) is a map of sets, there exists a unique \\(R\\)-module homomorphism: \\(\\Phi: F(A) \\rightarrow M\\) such that \\(\\Phi(a) = \\varphi(a)\\) for all \\(a \\in A\\).</p> <p>Corollary. If \\(F_1\\) and \\(F_2\\) are free modules on \\(A\\), then there is a unique isomorphism between \\(F_1\\) and \\(F_2\\), which is the identity map on A.</p> <p>Corollary. If \\(F\\) is a free \\(R\\)-module with basis \\(A\\), then \\(F \\cong F(A)\\).</p> <p>Definition. For a free module \\(F\\) with basis \\(A\\), if \\(R\\) is commutative, then the rank of \\(F\\) is the cardinality of \\(A\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-104-tensor-products-of-modules","title":"Section 10.4 - Tensor Products of Modules","text":"<p>Skipped</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-105-exact-sequences-projective-injective-and-flat-modules","title":"Section 10.5 - Exact Sequences - Projective, Injective, and Flat Modules","text":"<p>Skipped</p>"},{"location":"math/abstract-algebra/DF-12-modules-pids/","title":"Dummit &amp; Foote Chapter 12 - Modules over Principal Ideal Domains","text":""},{"location":"math/abstract-algebra/DF-12-modules-pids/#section-121-the-basic-theory","title":"Section 12.1 The Basic Theory","text":"<p>Definition. The left \\(R\\)-module \\(M\\) is said to be a Noetherian \\(R\\)-module if there are no infinitely increasing chains of submodules. That is, given</p> \\[ M_1 \\subseteq M_2 \\subseteq \\ldots \\] <p>there exists some \\(k \\in \\mathbb{N}\\) such that given any \\(n \\in \\mathbb{N}\\) with \\(n \\geq k\\), then \\(M_n = M_k\\).</p> <p>Definition. A ring \\(R\\) is Noetherian if it is Noetherian when viewed as a left \\(R\\)-module over itself.</p> <p>Theorem. Let \\(R\\) be a ring and \\(M\\) a left \\(R\\)-module. Then, the following are equivalent:</p> <ol> <li>\\(M\\) is Noetherian</li> <li>Every nonempty set of submodules of \\(M\\) contains a maximal element under inclusion</li> <li>Every submodule of \\(M\\) is finitely-generated</li> </ol> <p>Corollary. If \\(R\\) is a principal ideal domain (PID), then all nonempty set of ideals of \\(R\\) has a maximal element. Additionally, \\(R\\) is as Noetherian ring.</p> <p>Proposition. Let \\(R\\) be an integral domain, and \\(M\\) be a free \\(R\\)-module of rank \\(n &lt; \\infty\\). Then, given \\(S\\) is subset \\(M\\) with \\(|S| &gt; n\\), the elements of \\(S\\) are \\(R\\)-linearly dependent.</p> <p>Definition. Given \\(R\\) an integral domain and \\(M\\) an \\(R\\)-module,</p> \\[ \\text{Tor}(M) = \\{ x \\in M | rx = 0 \\text{ for any } r \\neq 0 \\} \\] <p>This is the torsion submodule of \\(M\\). If \\(\\text{Tor}(M)\\) is empty, then \\(M\\) is torsion-free.</p> <p>Definition. Let \\(R\\) be an integral domain and \\(M\\) be an \\(R\\)-module. Then, given a submodule \\(N\\),</p> \\[ \\text{Ann}_R(N) = \\{r \\in R | rn = 0 \\text{ for all } n \\in N \\} \\] <p>This ideal of \\(R\\) is the annihilator of \\(N\\). That is, \\(\\text{Ann}(N)\\) is the set of elements of \\(R\\) such that \\((r)N = \\{ 0 \\}\\).</p> <p>Note that if \\(N\\) is not a torsion submodule of \\(M\\), then \\(\\text{Ann}(N) = (0)R\\). Additionally, given \\(N, L\\) are submodules of \\(M\\) with \\(N \\subseteq L\\), then \\(\\text{Ann}(N) \\subseteq \\text{Ann}(L)\\).</p> <p>Additionally, if \\(R\\) is a PID, as \\(\\text{Ann}_R(N)\\) is an ideal, \\(\\text{Ann}(N) = (n)R\\) and \\(\\text{Ann}(L) = (l)R\\) for some \\(n, l \\in R\\) such that \\(n | l\\).</p> <p>Definition. Given any integral domain \\(R\\), the rank of an \\(R\\)-module \\(M\\) is the maximum number of \\(R\\)-linearly independent elements of M.</p> <p>Corollary. The rank of a free module is the number of generating elements.</p> <p>Theorem. Let \\(R\\) be a principal ideal domain, and \\(M\\) be a free \\(R\\)-module of finite rank \\(m\\), and \\(N\\) be a submodule of \\(M\\). Then,</p> <ol> <li>\\(N\\) is a free submodule with rank \\(n \\leq m\\).</li> <li>There exists a basis \\(y_1, y_2, \\ldots, y_m\\) of \\(M\\) so that \\(r_1 y_1, r_2 y_2, \\ldots, r_m y_n\\) is a basis of \\(N\\) for some \\(r_i \\in R\\) and \\(r_1 | r_2 | \\ldots | r_n\\)</li> </ol> <p>Theorem. Fundamental Theorem, Existence: Invariant Form. Let \\(R\\) be a PID and \\(M\\) be a finitely generated \\(R\\)-module. THen,</p> <ul> <li>\\(M\\) is isomorphic for some \\(r \\in \\mathbb{N}\\cup{0}\\), \\(a_1, \\ldots, \\a_m \\neq 0 \\in R\\) such that \\(a_1 | a_2 | \\ldots | a_m\\), with</li> </ul> \\[ M \\cong R^{\\oplus r} \\oplus \\frac{R}{(a_1)R} \\oplus \\frac{R}{(a_2)R} \\oplus \\ldots \\oplus \\frac{R}{(a_m)R} \\] <ul> <li> <p>\\(M\\) is torsion-free if and only if \\(M\\) is free</p> </li> <li> <p>Note that</p> </li> </ul> \\[ \\text{Tor}{M} \\cong \\frac{R}{(a_1)R} \\oplus \\frac{R}{(a_2)R} \\oplus \\ldots \\oplus \\frac{R}{(a_m)R} \\] <p>As a consequence, \\(M\\) is a torsion module if and only if \\(r = 0\\).</p> <p>Definition. In the above, \\(r\\) is the free rank of \\(M\\), and \\(a_1, \\ldots, a_m\\) are the invariant factors of \\(M\\).</p> <p>Theorem. Fundamental Theorem, Existence: Elementary Divisor Form. The sum above can be written as</p> \\[ M \\cong R^{\\oplus r} \\oplus \\frac{R}{(p_1^{\\alpha_1})R} \\oplus \\frac{R}{(p_2^{\\alpha_2})R} \\oplus \\ldots \\oplus \\frac{R}{(p_t^{\\alpha_t})R} \\] <p>with \\(p_t\\) non-unique primes and \\(\\alpha_t\\) non-unique, but with \\((p_t^{\\alpha_t})\\) unique. These are called the elementary divisors of \\(M\\).</p> <p>TODO: Incomplete for Now</p>"},{"location":"math/abstract-algebra/DF-13-fields/","title":"Dummit &amp; Foote Chapter 13 - Field Theory","text":""},{"location":"math/abstract-algebra/DF-13-fields/#section-131-basic-theory-of-field-extensions","title":"Section 13.1 Basic Theory of Field Extensions","text":"<p>Definition. The characteristic of a field \\(F\\) is the smallest positive integer \\(p\\) such that \\(1_F * p = 0\\). It follows that \\(p\\) is \\(0\\) or prime, and \\(p \\alpha = 0\\) for any \\(\\alpha \\in F\\).</p> <p>Definition. If \\(K, F\\) are fields such that \\(F \\subseteq K\\), then \\(K\\) is an extension field or extension of \\(F\\), denoted \\(K / F\\).</p> <p>Definition. The degree (or relative degree or index) of \\(K/F\\), denoted \\([K:F]\\), is the dimension of \\(K\\) as a \\(F\\)-vector space.</p> <p>Theorem. Let \\(F\\) be a field, \\(p(x) \\in F[x]\\). Then, there exists a \\(K\\) such that \\(p(x)\\) has a root in \\(K\\).</p> <p>Theorem. Let \\(F\\) be a field, \\(p(x) \\in F[x]\\). Then, \\(K = \\frac{F[x]}{(p(x))}\\) and \\(\\theta = x \\amod{p(x)}\\), \\(K\\) has a basis of \\(1, \\theta, \\ldots, \\theta^{n-1}\\) where \\(n = \\deg(p)\\).</p> <p>Theorem. Let \\(K/F\\) and \\(\\alpha, \\beta, \\ldots \\in K\\). Then, the smallest subfield of \\(K\\) containing \\(F\\) and \\(\\alpha, \\beta, \\ldots\\) is \\(F(\\alpha, \\beta, \\ldots)\\), which is the field generated by \\(\\alpha, \\beta, \\ldots\\) over \\(F\\).</p> <p>Definition. If \\(K\\) is generated by \\(F(\\alpha)\\), then \\(K\\) is a simple extension of \\(F\\).</p> <p>Theorem. Let \\(F\\) be a field, \\(p(x) \\in F[x]\\) be irreducible. Then, if \\(\\alpha\\) is a root of \\(p(x)\\) and \\(K\\) is an extension of \\(F\\) containing \\(\\alpha\\), then \\(F(\\alpha) \\cong \\frac{F[x]}{(p(x))}\\).</p> <p>TODO</p>"},{"location":"math/diffeq/1-intro/","title":"Section 1 - Basic Concepts","text":""},{"location":"math/diffeq/1-intro/#section-11-definitions","title":"Section 1.1 - Definitions","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. A differential equation is an equation that describes a function in terms of its derivatives. Examples of differential equations include Newton's Laws, among others.</p> <p>Definition. The order of a differential equation is the largest derivative present in the equation with a non-zero constant.</p> <p>Definition. A differential equation that only involves derivatives with respect to one variable is called an ordinary differential equation (ODE).</p> <p>Definition. A differential equation that describes a function in terms of derivatives with respect to more than one linearly-independent variable is called a partial equation.</p> <p>Definition. A linear differential equation is any differential equation that cn be written in the following form:</p> \\[ a_n(t)y^{(n)}(t) + a_{n-1}(t)+y^{n-1}(t) + \\ldots + a_1(t)y'(t) + a_0(t)y(t) = g(t) \\] <p>Note that \\(a_n(t)\\) does not depend on any derivative of \\(y\\), so the presence of terms such as \\(e^y\\) or \\(\\sqrt{y'}\\) signal that the equation is nonlinear.</p> <p>Definition. The solution(s) to a differential equation over an interval \\(\\alpha &lt; t &lt; \\beta\\) are any function(s) \\(y(t)\\) that satisfy the differential equation.</p> <p>Definition. The initial conditions are a condition or set of conditions that constrain the possible solution sets.</p> <p>Definition. An Initial Value Problem is a differential equation along with the appropriate boundary or initial conditions.</p> <p>Definition. The integral of validity for a solution to a differential equation is the largest possible interval containing the initial conditions for which the solution is valid.</p> <p>Definition. The general solution to a differential equation is the most general form a solution to a differential equation can take without requiring the initial conditions.</p> <p>Definition. The actual solution to a differential equation is the specific solution that satisfies the differential equation and the boundary conditions.</p> <p>Definition. A solution is said to be explicit if it can be written in the form \\(y = y(t)\\). Otherwise, it is said to be implicit.</p>"},{"location":"math/diffeq/1-intro/#section-12-directional-fields","title":"Section 1.2 - Directional Fields","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. A directional field is the graph of a \\(t\\) vs. \\(y(t)\\), with vectors drawn at each point with a slope corresponding to \\(y'(t)\\). Notably, each arrow will be pointed right (towards increasing \\(t\\)).</p>"},{"location":"math/diffeq/2-1st-order/","title":"Section 2 - First Order Differential Equations","text":""},{"location":"math/diffeq/2-1st-order/#section-21-linear-differential-equations","title":"Section 2.1 - Linear Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the following first-order linear differential equation be given, with \\(p(t)\\) and \\(g(t)\\) continuos.</p> \\[ \\frac{dy}{dt} + p(t)y = g(t) \\]"},{"location":"math/diffeq/2-1st-order/#deriving-the-solution","title":"Deriving the Solution","text":"<p>Next, we let \\(\\mu(t)\\) be our integrating factor. Multiply both sides of the equation through by \\(\\mu(t)\\).</p> \\[ \\mu(t)\\frac{dy}{dt} + \\mu(t)p(t)y = \\mu(t)g(t) \\] <p>Now, define \\(\\mu(t)\\) so that \\(\\mu(t)p(t) = \\mu'(t)\\). Thus, we can state the following:</p> \\[ \\mu(t)\\frac{dy}{dt} + \\mu'(t)y = \\mu(t)g(t) \\] <p>The left of the preceding equation is simply the product rule, so we can write \\((\\mu(t)y(t))' = \\mu(t)g(t)\\). Take the integral of both sides.</p> \\[\\begin{align}     \\int (\\mu(t)y(t))' dt &amp;= \\int \\mu(t)g(t) \\\\     \\mu(t)y(t) + C &amp;= \\int \\mu(t)g(t) dt \\\\     y(t) &amp;= \\frac{\\int \\mu(t)g(t) dt - C}{\\mu(t)} \\end{align}\\] <p>Let \\(C\\) absorb the negative sig, and we see the following.</p> \\[ y(t) = \\frac{\\int \\mu(t)g(t) dt + C}{\\mu(t)} \\] <p>This is the general solution to the differential equation. However, it is incomplete, as we do not know \\(\\mu(t)\\)</p> <p>To derive the function, recall that we defined \\(\\mu(t)p(t) = \\mu'(t)\\). Thus, we can rewrite this equation.</p> \\[\\begin{align}     \\frac{\\mu'(t)}{\\mu(t)} &amp;= p(t) \\\\     (\\ln \\mu(t))' &amp;= p(t) \\\\ \\end{align}\\] <p>Integrate both sides.</p> \\[\\begin{align}     \\ln \\mu(t) + k = \\int p(t) dt \\\\     \\mu(t) &amp;= e^{\\int p(t) dt + k} \\\\     &amp;= e^k e^{\\int p(t) dt} \\end{align}\\] <p>As \\(k\\) is an unknown constant, rewrite this as \\(\\mu(t) = k \\exp(\\int p(t) dt)\\).</p>"},{"location":"math/diffeq/2-1st-order/#summary","title":"Summary","text":"<p>The following differential equation is given.</p> \\[ \\frac{dy}{dt} + p(t)y = g(t) \\] <p>To find a solution to this differential equation, construct the integrating factor. \\(\\mu(t)\\).</p> \\[\\mu(t) = k \\exp(\\int p(t) dt)\\] <p>Thus, the solution to the differential equation can be written as the following.</p> \\[ y(t) = \\frac{\\int \\mu(t)g(t) dt + C}{\\mu(t)} \\]"},{"location":"math/diffeq/2-1st-order/#section-22-separable-differential-equations","title":"Section 2.2 - Separable Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the following differential equation of the following forms be given.</p> <p>\\begin{align}     N(y) \\frac{dy}{dx} &amp;= M(x)     \\frac{dy}{dx} &amp;= \\frac{M(x)}{N(y)} \\     \\frac{dy}{dx} &amp;= \\frac{N(y)}{M(x)} \\     \\frac{dy}{dx} &amp;= N(y)M(x) \\ \\end{align}.</p> <p>For the sake of simplicity, select the following form:</p> \\[ N(y) \\frac{dy}{dx} = M(x) \\] <p>Thus, integrate both sides with respect to \\(x\\).</p> \\[ \\int N(y) \\frac{dy}{dx} dx = \\int M(x) dx \\] <p>Since \\(y\\) is really \\(y(x)\\), we can make the following substitution:</p> \\[ u = y(x) \\text{ and } du = y'(x)dx = \\frac{dy}{dx} dx \\] <p>This reduces the integral to the following:</p> \\[ \\int N(u) du = \\int M(x) dx \\] <p>This is solvable from here.</p>"},{"location":"math/diffeq/2-1st-order/#section-24-bernoulli-equations","title":"Section 2.4 - Bernoulli Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let a differential equation of the following form be given, with \\(n \\in \\mathbb{N}; n \\geq 2\\)</p> \\[ y' + p(x)y = q(x)y^n \\] <p>This is a Bernoulli equation.</p> <p>Divide by \\(y^n\\).</p> \\[ y^{-n}y' + p(x)y^{1-n} = q(x) \\] <p>Now, make the substitution \\(v = y^{1-n}\\). Thus, the derivative is as follows.</p> \\[ v' = (1-n)y^{-n}y' \\] <p>Substituting into the first equation yields the following.</p> \\[ \\frac{1}{1-n}v' + p(x)v = q(x) \\] <p>After solving, be sure to rewrite in terms of \\(y\\).</p>"},{"location":"math/diffeq/3-2nd-order/","title":"Section 3 - Second Order Differential Equations","text":""},{"location":"math/diffeq/3-2nd-order/#section-31-basic-concepts","title":"Section 3.1 - Basic Concepts","text":"<p>This section is from Paul's Online Math Notes.</p> <p>All second-order differential equations can be written in the following form:</p> \\[ p(t) y'' + q(t) y' + r(t) y = g(t) \\] <p>In the case where \\(p(t)\\), \\(q(t)\\), and \\(r(t)\\) are constants, we write the equation as the following:</p> \\[ ay'' + by' + cy = g(t) \\] <p>This is a second-order differential equation with constant coefficients.</p> <p>Definition. In the event that \\(g(t) = 0\\), we say the equation is homogenous. Otherwise, the equation is nonhomogeneous.</p> <p>Definition. Principal of Superposition. Let \\(y_1(t)\\) and \\(y_2(t)\\) be solutions to a linear, homogenous differential equation. Then, any linear combination of said solutions is also a solution to the differential equation. In other words, with \\(c_1, c_2 \\in \\mathbb{R}\\), the following is a solution to a differential equation.</p> \\[ y(t) = c_1 y_1(t) + c_2 y_2(t) \\] <p>Given a second-order homogenous differential equation with constant coefficients, we assume solutions of the following form:</p> \\[ y(t) = e^{rt} \\] <p>Substituting this equation into the differential equation, we see the following:</p> \\[ e^{rt}(ar^2 + br + c) = 0 \\] <p>Thus, we allow the characteristic equation of the differential equation to be as follows:</p> \\[ ar^2 + br + c = 0 \\]"},{"location":"math/diffeq/3-2nd-order/#section-32-real-distinct-roots","title":"Section 3.2 - Real &amp; Distinct Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>When the two roots to the characteristic equation are discrete roots in the real numbers, we see the following solutions.</p> \\[ y_1(t) = e^{r_1 t} \\] \\[ y_2(t) = e^{r_2 t} \\] <p>Thus,</p> \\[ y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t} \\]"},{"location":"math/diffeq/3-2nd-order/#section-33-complex-roots","title":"Section 3.3 - Complex Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the solutions to the characteristic equation be of the following form:</p> \\[ r_{1,2} = \\lambda \\pm \\mu i \\] <p>Thus, our two solutions are</p> \\[ y_1(t) = e^{(\\lambda + \\mu i)t} \\] \\[ y_2(t) = e^{(\\lambda - \\mu i)t} \\] <p>Recall Euler's Formula:</p> \\[ e^{i \\theta} = \\cos \\theta + i \\sin \\theta \\] <p>A corollary of Euler's formula is the following:</p> \\[ e^{-i \\theta} = \\cos(-\\theta) + i \\sin(-\\theta) = \\cos \\theta - i \\sin \\theta \\] <p>Thus, we can write our solutions as the following:</p> \\[\\begin{align}     y_1(t) &amp;= e^{(\\lambda + \\mu i)t} &amp;= e^{\\lambda t} e^{i \\mu t} &amp;= e^{\\lambda t}(\\cos(\\mu t) + i \\sin(\\mu t)) \\\\     y_2(t) &amp;= e^{(\\lambda - \\mu i)t} &amp;= e^{\\lambda t} e^{-i \\mu t} &amp;= e^{\\lambda t}(\\cos(\\mu t) - i \\sin(\\mu t)) \\end{align}\\] <p>A linear combination of the two solutions can be written as the following:</p> \\[ y(t) = c_1 e^{\\lambda t} \\cos(\\mu t) + c_2 e^{\\lambda t} \\sin(\\mu t) \\]"},{"location":"math/diffeq/3-2nd-order/#section-34-repeated-roots","title":"Section 3.4 - Repeated Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume the solutions to the characteristic equations are \\(r = r_1 = r_2\\). Thus, the two equations \\(y_t(t)\\) and \\(y_2(t)\\) are not linearly independent.</p> <p>After a lot of algebra, we see that</p> \\[y_1(t) = e^{rt}\\] \\[y_2(t) = t e^{rt}\\]"},{"location":"math/diffeq/3-2nd-order/#section-35-reduction-of-order","title":"Section 3.5 - Reduction of Order","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Skipped.</p>"},{"location":"math/diffeq/3-2nd-order/#section-36-fundamental-set-of-solutions-wronskian","title":"Section 3.6 - Fundamental Set of Solutions, Wronskian","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. Given two functions \\(f(t)\\), \\(g(t)\\), the Wronskian is defined as</p> \\[ W(f,g) = \\det \\begin{vmatrix}   f(t) &amp; g(t) \\\\   f'(t) &amp; g'(t) \\end{vmatrix} \\] <p>Definition. If \\(W(f, g) \\neq 0\\), then \\(f(t)\\) and \\(g(t)\\) are said to form a fundamental set of solutions, and can be superimposed to form the general solution.</p>"},{"location":"math/diffeq/3-2nd-order/#section-38-nonhomogeneous-differential-equations","title":"Section 3.8 - Nonhomogeneous Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume we have the differential equation as follows:</p> \\[ y'' + p(t) y' + q(t) y = g(t) \\] <p>The equivalent homogenous differential equation is</p> \\[ y'' + p(t) y' + q(t) y = 0 \\] <p>Theorem. Assume \\(Y_1(t)\\), \\(Y_2(t)\\) are solutions to the nonhomogeneous differential equations. Then, \\(Y_1(t) - Y_2(t)\\) is a solution to the homogenous differential equation. This can be proved by substitution.</p> <p>Thus, with \\(y_h(t)\\) the solution to the homogenous problem, and \\(y_p(t)\\) the solution to this particular problem, we can say that the general form of the solution to this differential equation is</p> \\[ y(t) = y_h(t) + y_p(t) \\]"},{"location":"math/diffeq/3-2nd-order/#section-39-undetermined-coefficients","title":"Section 3.9 - Undetermined Coefficients","text":"<p>This section is from Paul's Online Math Notes.</p> <p>We know the following guesses for functions.</p> \\(g(t)\\) \\(y_p\\) guess \\(\\alpha e^{\\beta t}\\) \\(A e^{\\beta t}\\) \\(a \\cos(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) \\(b \\sin(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) \\(a \\cos(\\beta t) + \\sin(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) n-th degree polynomial \\(A_nt^n + A_{n-1}t^{n-1} + A_1 t + A_0\\) <p>Combine this with the following:</p> <p>Theorem. Given \\(y_{p_1}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_1(t)\\) and \\(y_{p_2}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_2(t)\\), then the function \\(y_{p_1}(t) + y_{p_2}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_1(t) + g_2(t)\\)</p>"},{"location":"math/diffeq/3-2nd-order/#section-310-variation-of-parameters","title":"Section 3.10 - Variation of Parameters","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume we have the differential equation as follows:</p> \\[ y'' + p(t) y' + q(t) y = g(t) \\] <p>The equivalent homogenous differential equation is</p> \\[ y'' + p(t) y' + q(t) y = 0 \\] <p>For this method, we must have \\(y_1(t)\\) and \\(y_2(t)\\) known. Through a lot of math, we see that</p> \\[ y_p = -y_1 \\int \\frac{y_2(t)g(t)}{W(y_1, y_2)} dt + y_2 \\int \\frac{y_1(t)g(t)}{W(y_1, y_2)} dt \\]"},{"location":"math/diffeq/4-laplace/","title":"Section 4 - Laplace Transformations","text":""},{"location":"math/diffeq/4-laplace/#section-41-definition","title":"Section 4.1 - Definition","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. The Laplace transform of a function is given by the following:</p> \\[ \\mathcal{L} \\{f(t)\\}(s) = F(s) = \\int_0^{\\infty} e^{-st}f(t) dt \\]"},{"location":"math/diffeq/4-laplace/#section-42-properties","title":"Section 4.2 - Properties","text":"<p>The Laplace Transformation is a linear transformation over functions in \\(\\mathbb{R}[t]\\). That is, given \\(a, b \\in \\mathbb{R}, f(t), g(t) \\in \\mathbb{R}[t]\\), we know that</p> \\[ \\mathcal{L} \\{a f(t)\\ + b g(t) \\}(s) = a F(s) + b G(s) \\]"},{"location":"math/diffeq/4-laplace/#section-43-inverse-laplace-transformation","title":"Section 4.3 - Inverse Laplace Transformation","text":"<p>Given \\(F(s)\\), we define the Inverse Laplace Transformation as the following;</p> \\[ f(t) = \\mathcal{L}^{-1} \\{F(s)\\} \\]"},{"location":"math/diffeq/4-laplace/#section-44-step-function","title":"Section 4.4 - Step Function","text":"<p>The step/Heaviside function \\(u_c(t)\\) is defined as 0 if \\(t &lt; c\\), and 1 if \\(t &gt; c\\).</p> <p>Alternatively, \\(u(t - c) = H(t - c)\\) is 0 if \\(t &lt; c\\), and 1 if \\(t &gt; c\\).</p> <p>Applying this to the Laplace transform,</p> \\[ \\begin{align}     \\mathcal{L} \\{ u_c(t) f(t-c) \\} &amp;= \\int_0^{\\infty} e^{-st}u_c(t)f(t) dt \\\\     &amp;= \\int_c^{\\infty} e^{-st}f(t) dt \\end{align} \\] <p>If we let \\(u = t - c\\),</p> \\[ \\begin{align}     \\mathcal{L} \\{ u_c(t) f(t-c) \\} &amp;= \\int_0^{\\infty} e^{-s(u+c)}f(u) du \\\\     &amp;= \\int_0^{\\infty} e^{-su}e^{-cs}f(u) du \\\\     &amp;= e^{-cs} \\int_0^{\\infty} e^{-su}f(u) du \\\\     &amp;= e^{-cs} F(s) \\end{align} \\]"},{"location":"math/diffeq/4-laplace/#section-45-laplace-transformation-applied-to-ivps","title":"Section 4.5 - Laplace Transformation applied to IVPs","text":"<p>Theorem. Given a function \\(f(t)\\) with \\(C^n\\) continuity, then</p> \\[ \\mathcal{L} \\{ f^{(n)} (t) \\} = s^n F(s) - s^{n-1} f(0) - s^{n-2} f'(0) - \\ldots - s f^{(n-2)} (0) - f^{(n-1)} (0) \\] <p>For \\(n=1, 2\\) we see that</p> \\[ \\begin{align}     \\mathcal{L} \\{ y' \\} &amp;= sY(s) - y(0) \\\\     \\mathcal{L} \\{ y'' \\} &amp;= s^2 Y(s) - s y(0) - y'(0) \\end{align} \\] <p>We can take the Laplace transformation of an IVP, solve for \\(Y(s)\\), then take the inverse to find the solution.</p>"},{"location":"math/diffeq/4-laplace/#section-46-non-constant-coefficient-ivps","title":"Section 4.6 - Non-constant Coefficient IVPs","text":"<p>If \\(f(t)\\) is piecewise continuous on \\([0, \\infty)\\), then \\(\\lim_{s \\rightarrow \\infty} F(s) = 0\\).</p> <p>Definition. A function \\(f(t)\\) is said to be of exponential order \\(\\alpha\\) if there exists positive constants \\(T, M\\) such that for all \\(t \\geq T\\), \\(|f(t)| \\leq Me^{\\alpha t}\\).</p> <p>To check this, simply compute \\(\\lim_{t \\rightarrow \\infty} \\frac{|f(t)|}{e^{\\alpha t}}\\). If this is finite for some \\(\\alpha\\), then the function is of exponential order \\(\\alpha\\).</p>"},{"location":"math/diffeq/4-laplace/#section-47-ivps-with-step-functions","title":"Section 4.7 - IVPs with Step Functions","text":"<p>Recall that \\(\\mathcal{L} \\{u_c(t)f(t-c)\\} = e^{-cs}F(s)\\). Then, we can solve IVPs involving step functions.</p>"},{"location":"math/diffeq/4-laplace/#section-48-dirac-delta-function","title":"Section 4.8 - Dirac Delta Function","text":"<p>The Dirac Delta function has several properties. First, \\(\\delta(t - a) = 0\\) when \\(t \\neq a\\). Notably, though,</p> \\[\\int_{\\mathbb{R}} f(t) \\delta(t - a) dt = f(a)\\] <p>Note that this is not an actual function, buy instead a generalized function or distribution, as several functions can express this property using infinite limits.</p> <p>Then, we can see that \\(\\mathcal{L} \\{\\delta(t-a)\\} = \\int_0^\\infty e^{-st} \\delta(t-a) dt\\) by definition. Then, applying the properties of the Delta function, \\(\\mathcal{L} \\{\\delta(t-a)\\} = e^{-as}\\), given \\(a &gt; 0\\).</p>"},{"location":"math/diffeq/4-laplace/#section-49-convolution-integrals","title":"Section 4.9 - Convolution Integrals","text":"<p>Consider two functions \\(F(s)\\) and \\(G(s)\\) such that \\(F(s) G(s) = H(s)\\), of which we want to find an inverse Laplace transform.</p> <p>We define a convolution integral \\((f*g)(t)\\) as</p> \\[(f*g)(t) = \\int_0^t f(t - \\tau)(g - \\tau) d\\tau\\] <p>A unique property of this integral is that \\((f*g) = (g*f)\\).</p> <p>With this, we see that \\(\\mathcal{L} \\{f * g\\} = F(s)G(s)\\), or that \\(\\mathcal{L}^{-1} \\{F(s)G(s)\\} = (f * g)(t)\\).</p>"},{"location":"math/diffeq/5-systems/","title":"Section 5 - Systems of Differential Equations","text":"<p>Sections 5.1-5.3 are review.</p>"},{"location":"math/diffeq/5-systems/#section-54-systems-of-differential-equations","title":"Section 5.4 - Systems of Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p>"},{"location":"math/real-analysis/11-metric-spaces/","title":"Chapter 11 - Metric Spaces","text":""},{"location":"math/real-analysis/11-metric-spaces/#section-114-netric-spaces","title":"Section 11.4 - Netric Spaces","text":"<p>Definition. A metric on set \\(S\\) is a function \\(d: S \\otimes S \\rightarrow \\mathbb{R}\\) that satifies the following properties for all \\(x, y, z \\in S\\),</p> <ul> <li>\\(d(x, y) \\geq 0\\)</li> <li>\\(d(x, y) = 0 \\; \\text{ if and only if } x = y\\)</li> <li>\\(d(x, y) = d(y, x)\\)</li> <li>\\(d(x, y) \\leq d(x, z) + d(z, y)\\)</li> </ul> <p>Definition. A metric space \\((S, d)\\) is a set \\(S\\), with elements called points, together with a metric \\(d\\).</p> <p>Definition. With metric space \\((S, d)\\), if \\(A \\subset S\\), then \\((A, d)\\) is a subspace of \\((S, d)\\).</p> <p>Definition. The discrete metric is provided by</p> \\[ d(x, y) = \\begin{cases}   0 \\; \\text{ if } x = y \\\\   1 \\; \\text{ if } x \\neq y \\end{cases} \\] <p>Definition. Let \\((S, d)\\) be a metric space. Then, for each \\(\\varepsilon &gt; 0\\), the \\(\\varepsilon\\)-neighborhood or \\(\\varepsilon\\)-ball of a point \\(a \\in S\\) is the set</p> \\[ V_\\varepsilon(a) = {x \\in S | d(a, x) &lt; \\varepsilon} \\] <p>Definition. Let \\((S, d)\\) be a metric space. Then, a subset \\(G \\subseteq S\\) is open if for each \\(x \\in G\\), there exists some \\(\\varepsilon &gt; 0\\) so that \\(V_\\varepsilon(x) \\subseteq G\\).</p> <p>Definition. Let \\((S, d)\\) be a metric space. Then, a subset \\(G \\subseteq S\\) is closed if its complement \\(C(G) = S - G = S \\ F\\) is closed.</p> <p>Definition. Let \\((S, d)\\) be a metric space. A point \\(c \\in S\\) is a *cluster point$ of a set \\(A \\subseteq S\\) if every \\(\\varepsilon\\)-neighborhood of \\(c\\) contains some point \\(a \\in A\\) such that \\(a \\neq c\\).</p> <p>Theorem. Every \\(\\varepsilon\\)-neighborhood of a point is an open set.</p> <p>Theorem. The union of an arbitrary collection of open sets is open.</p> <p>Theorem. The intersection of a finite collection of open sets is open.</p> <p>Theorem. The union of finitely many closed sets is closed.</p> <p>Theorem. The intersection of infinitely many closed sets is closed.</p> <p>Theorem. A subset of a metric space is closed if and only if it contains all of its cluster points.</p> <p>Definition. A sequence \\((x_n)\\) in a metric space \\((S, d)\\) converges to a point \\(x \\in S\\) if given any \\(\\varepsilon &gt; 0\\), there exists a \\(K \\in \\mathbb{N}\\) such that given \\(n \\in \\mathbb{N}\\),</p> \\[ n \\geq K \\Rightarrow d(x_n, x) \\leq \\varepsilon \\] <p>Theorem. Let \\((x_n)\\) be a sequence in metric space \\((S, d)\\). Then,</p> <ul> <li>\\((x_n)\\) converges to \\(x\\) if and only if every \\(\\varepsilon\\)-neighborhood of \\(x\\) contains all but finitely many terms of \\((x_n)\\).</li> <li>If \\((x_n) \\rightarrow x\\) and \\((x_n) \\rightarrow x'\\), then \\(x = x'\\).</li> <li>If \\((x_n)\\) converges, then \\((x_n)\\) is bound.</li> </ul> <p>Definition. A sequence \\((x_n)\\) in metric space \\((S, d)\\) is a Cauchy sequence if for every \\(\\varepsilon &gt; 0\\), there exists some \\(H \\in \\mathbb{N}\\) such that for any \\(m, n \\in \\mathbb{N}\\),</p> \\[ m, n \\geq H \\Rightarrow d(x_n, x_m) &lt; \\varepsilon \\] <p>Definition. A metric space in which every Cauchy sequence converges is said to be complete.</p> <p>Remark. \\(\\mathbb{R}\\) is complete, but \\(\\mathbb{Q}\\) is not.</p> <p>Definition. Let \\(A\\) be a subset of metric space \\((S, d)\\). Then, an open cover of \\(A\\) is some collection of subsets \\(\\mathcal{G} = \\{G_\\alpha\\}_{\\alpha \\in I}\\), such that \\(G_\\alpha \\subseteq S\\) and \\(A \\subseteq \\cup_{\\alpha \\in I} G_\\alpha\\). That is, \\(A\\) is contained within the union of all open subsets in \\(\\mathcal{G}\\).</p> <p>Definition. If \\(\\mathcal{G}' \\subseteq \\mathcal{G}\\) is an open cover of \\(A\\), then \\(\\mathcal{G}'\\) is a subcover of \\(\\mathcal{G}\\).</p> <p>Definition. Given \\(K\\) is a subset of metric space \\((S, d)\\), K is compact if every cover of \\(K\\) contains a finite subcover.</p> <p>Theorem. If \\(K\\) is a compact subset of a metric space, then \\(K\\) is closed and bounded.</p> <p>Theorem. Heine-Borel Theorem. In \\(\\mathbb{R}\\), the convese is true. That is, if \\(K \\subseteq \\mathbb{R}\\) is closed and bounded, then it is compact.</p> <p>Theorem. If \\(K\\) is a compact subset of a metric space, then every infinite subset of \\(K\\) has a cluster point.</p> <p>Corollary. Bolzano-Weirstrass Theorem. Every bounded infinite subset of \\(\\mathbb{R}\\) has a cluster point in \\(\\mathbb{R}\\).</p> <p>Definition. Let \\((S, d)\\) be a metric space, and \\(A_1, A_2 \\in S\\) be subsets. Then, \\(A_1, A_2\\) are said to be separated if there exist disjoint open subsets \\(U_1, U_2\\) such that \\(A_1 \\subseteq U_1\\) and \\(A_2 \\subseteq U_2\\).</p> <p>Definition. A subset \\(C\\) of metric space \\((S, d)\\) is said to be connected if it is not the union of nonempty separated subsets.</p> <p>Theorem. A subset of \\(\\mathbb{R}\\) is connected if and only if it is an interval.</p>"},{"location":"math/real-analysis/2-reals/","title":"Chapter 2 - The Real Number Line","text":""},{"location":"math/real-analysis/2-reals/#section-21-the-algebraic-and-order-properties-of-real-numbers","title":"Section 2.1 - The Algebraic and Order Properties of Real Numbers","text":"<p>Proposition. 2.1.1: \\(\\mathbb{R}\\) is a field, with zero element \\(0\\) and identity \\(1\\).</p> <p>Definition. The rational numbers \\(\\mathbb{Q}\\) is the field of fractions of the natural numbers \\(\\mathbb{N}\\).</p> <p>Theorem. 2.1.4: There does not exist a rational number \\(r\\) such that \\(r^2 = 2\\).</p> <p>Definition. An ordered field is a field \\(F\\) together with subset \\(F^+\\) such that</p> <ol> <li>\\(F+\\) is closed under addition and multiplication</li> <li>If \\(a \\in F\\), then exclusively \\(a \\in F^+\\), \\(a = 0\\), or \\(-a \\in F^+\\).</li> </ol> <p>Theorem. In any ordered field \\(F\\), the following hold</p> <ol> <li>\\(1 \\in F^+\\)</li> <li>\\(\\mathbb{N} \\subseteq F^+\\)</li> <li>If \\(a \\in F^+\\), then \\(\\frac{1}{a} \\in F^+\\)</li> </ol> <p>Definition. The order relation \\(a &gt; b\\) and \\(b &lt; a\\) is defined by \\(a - b \\in F^+\\).</p> <p>Theorem. If \\(a, b, c \\in F\\), then</p> <ol> <li>One of \\(a &gt; b\\), \\(a = b\\), or \\(a &lt; b\\) hold (trichotomy)</li> <li>If \\(a &gt; b\\) and \\(b &gt; c\\), then \\(a &gt; c\\) (transitivity)</li> <li>If \\(a &gt; b\\), then \\(-a &lt; -b\\)</li> <li>If \\(a &gt; b\\) and \\(c &gt; 0\\), then \\(ac &gt; bc\\)</li> <li>If \\(a &gt; b\\) and \\(c &lt; 0\\), then \\(ac &lt; bc\\)</li> <li>If \\(a &gt; b &gt; 0\\), then \\(\\frac{1}{b} &gt; \\frac{1}{a} &gt; 0\\)</li> </ol> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded above if there exists some \\(u \\in F\\) such that \\(s \\leq u\\) for all \\(s \\in S\\). Then, said element \\(u\\) is an upper bound of \\(S\\).</p> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded below if there exists some \\(u \\in F\\) such that \\(s \\geq u\\) for all \\(s \\in S\\). Then, said element \\(u\\) is a lower bound of \\(S\\).</p> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded if it is bounded both above and below.</p> <p>Definition. Given field \\(F\\) and nonempty subset \\(S \\subseteq F\\), an element \\(u \\in F\\) is a supremum or least upper bound of \\(S\\) if \\(u\\) is an upper bound of \\(S\\), and given any other upper bound \\(v\\), then \\(u &lt; v\\)</p> <p>Definition. Given field \\(F\\) and nonempty subset \\(S \\subseteq F\\), an element \\(u \\in F\\) is an infimum or greatest lower bound of \\(S\\) if \\(u\\) is a lower bound of \\(S\\), and given any other lower bound \\(v\\), then \\(u &gt; v\\)</p> <p>Definition. Given an ordered field \\(F\\), the field has the supremum/infimum property if given any nonempty subset \\(S\\), if \\(S\\) is bounded above/below, \\(S\\) has a supremum/infimum.</p>"},{"location":"math/real-analysis/2-reals/#section-22-absolute-value-and-the-real-line","title":"Section 2.2 - Absolute Value and the Real Line","text":"<p>Definition. Absolute value is defined as normal (piecewise). Multiline function in LaTeX are hard.</p> <p>Theorem. Given any \\(a, b \\in \\mathbb{R}\\), we know that</p> <ol> <li>\\(|a| &gt; 0\\) for \\(a \\neq 0\\)</li> <li>\\(|ab| = |a||b|\\)</li> <li>\\(|a + b| \\leq |a| + |b|\\)</li> </ol> <p>Corollary. Given \\(a, b \\in \\mathbb{R}\\), then \\(||a| - |b|| \\leq |a - b|\\).</p> <p>Remark. Every field has at least one absolute value function.</p> <p>Theorem. In an ordered field \\(F\\), for any \\(r &gt; 0\\), we know that</p> <ol> <li>\\(|x = r\\) if and only if \\(x = r\\) or \\(x = -r\\)</li> <li>\\(|x &lt; r\\) if and only if \\(-r &lt; x &lt; r\\)</li> <li>\\(|x &gt; r\\) if either \\(x &gt; r\\) or \\(x &lt; -r\\)</li> </ol> <p>Definition. The standard distance function or metric on the real numbers \\(\\mathbb{R}\\) given \\(a, b\\) is \\(|a - b|\\).</p> <p>Theorem. For any real numbers \\(a, b, c\\),</p> <ol> <li>\\(|a - b| &gt; 0\\) if and only if \\(a \\neq b\\) and \\(|a - b| = 0\\) if and only if \\(a = b\\)</li> <li>\\(|a - b| = |b - a|\\)</li> <li>\\(|a - c| \\leq |a - b| + |b + c|\\)</li> </ol> <p>Definition. A set together with a function satisfying these three properties is known as a metric space.</p> <p>Definition. The \\(\\varepsilon\\)-neighborhood of \\(a \\in \\mathbb{R}\\), denoted \\(V_\\varepsilon(a)\\) is the set of all real numbers \\(x \\in \\mathbb{R}\\) such that \\(|x - a| &lt; \\varepsilon\\). That is,</p> \\[ V_\\varepsilon(a) = (a - \\varepsilon, a + \\varepsilon) \\] <p>Decimals. Let \\(x \\in \\mathbb{R}\\) such that \\(x &gt; 0\\). By the archimedean property, there exists some \\(b_0 \\in \\mathbb{N} \\cup {0}\\) such that \\(b_0 &lt; x &lt; b_0 + 1\\). We can repeat this to see</p> \\[ x = b_0 + \\frac{b_1}{10} + \\frac{b_2}{100} + \\ldots + \\frac{b_n}{100^n} + \\ldots \\] <p>Definition. The decimal expansion of \\(x\\) is denoted \\(b_0.b_1 b_2 b_3 \\ldots\\).</p>"},{"location":"math/real-analysis/2-reals/#section-25-intervals","title":"Section 2.5 - Intervals","text":"<p>Definition. A subset \\(I\\) is an interval if and only if, given \\(a, b \\in I\\), then \\([a, b] \\subseteq I\\).</p> <p>Definition. Intervals \\(I_1, I_2, \\ldots, I_n, \\ldots\\) are nested if and only if \\(I_1 \\subseteq I_2 \\subseteq \\ldots \\subseteq I_n \\subseteq \\ldots\\).</p> <p>Theorem. Nested Intervals Property. If \\(I_n = [a_n, b_n]\\) is a set of nested intervals that are closed and bound, then there exists some number \\(z \\in \\mathbb{R}\\) such that \\(z \\in I_n\\) for all \\(n\\).</p> <p>Theorem. If \\(a &lt; b\\), then the interval \\([a, b]\\) is an uncountable set.</p> <p>Corollary. \\(\\mathbb{R}\\) is uncountable.</p>"},{"location":"math/real-analysis/3-sequences-series/","title":"Chapter 3 - Sequences and Series","text":""},{"location":"math/real-analysis/3-sequences-series/#section-31-sequences-and-their-limits","title":"Section 3.1 - Sequences and their Limits","text":"<p>Definition. A sequence in \\(\\mathbb{R}\\) is a function \\(X: \\mathbb{N} \\rightarrow \\mathbb{R}\\), typically notated as \\(X\\) or \\((x_n)\\), with \\(x_n\\) being referred to as the terms of the sequence. The set \\({x_n | n \\in \\mathbb{N}}\\) is the range of this sequence.</p> <p>Definition. The sequence is bounded if its range is a bounded subset of \\(\\mathbb{R}\\).</p> <p>Example. The constant sequence \\(C = (c) = (c, c, c, \\ldots)\\).</p> <p>Example. The harmonic sequence \\(\\frac{1}{n} = (1, \\frac{1}{2}, \\frac{1}{3}, \\ldots)\\)</p> <p>Example. The geometric sequence, given base \\(a \\in \\mathbb{R}\\) and ratio \\(r \\in \\mathbb{R}\\)</p> \\[ (x_n) = (a, ar,  ar^2, ar^3, \\ldots) \\] <p>Example. The arithmetic sequence, given base \\(a \\in \\mathbb{R}\\) and distance \\(d \\in \\mathbb{R}\\),</p> \\[ (x_n) = (a, a + d, a + 2d, a + 3d, \\ldots) \\] <p>Example. Decimal expansions are bounded sequences.</p> <p>Definition. A sequence \\(X = (x_n)\\) is said to converge to a number \\(x \\in \\mathbb{R}\\) if when given any \\(\\varepsilon &gt; 0\\), there exists some \\(K \\in \\mathbb{N}\\) such that for every \\(n \\in \\mathbb{N}\\) with \\(n \\geq K\\),</p> \\[ |x_n - x| &lt; \\varepsilon \\] <p>If this is the case, we say that \\(X\\) converges to  \\(x\\), and \\(x\\) is a limit of X. This can be written as</p> \\[ \\lim X = x \\text{ or } \\text \\lim(x_n) = x \\text{ or }  x_n \\rightarrow x \\] <p>Definition. If a sequence does not have a limit, it is divergent.</p> <p>Theorem. A sequence can have at most one limit. That is, if a limit exists, it is unique.</p> <p>Theorem. If a limit is convergent, then it is bounded.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-32-limit-theorems","title":"Section 3.2 - Limit Theorems","text":"<p>Theorem. Suppose there exists some \\(X\\) such that \\((x_n) \\rightarrow x\\) and \\(Y\\) such that \\((y_n) \\rightarrow y\\). Then,</p> <ol> <li>\\(x_n + y_n \\rightarrow x + y\\)</li> <li>\\(x_n \\cdot y_n \\rightarrow xy\\)</li> <li>If \\(x_n \\neq 0\\) for all \\(n\\), then \\(\\frac{1}{x_n} \\rightarrow \\frac{1}{x}\\)</li> </ol> <p>Theorem. Suppose \\((x_n)\\) and \\((y_n)\\) are convergent sequences and \\(N \\in \\mathbb{N}\\). Then,</p> <ol> <li>If \\(x_n \\leq y_n\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\leq \\lim(y_n)\\)</li> <li>If \\(x_n \\leq a\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\leq a\\)</li> <li>If \\(x_n \\geq a\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\geq a\\)</li> </ol> <p>Theorem. Squeeze Theorem. Suppose \\((x_n), (y_n), (z_n)\\) are all sequences of real numbers, and \\(\\lim(x_n) = \\lim(z_n) = a\\). Then, if for some \\(N in \\mathbb{N}\\),</p> \\[ \\text{If } x_n \\leq y_n \\leq z_n, \\text{ then } \\lim(y_n) = a \\] <p>Theorem. Suppose \\((x_n)\\) is a sequence if real numbers. Then,</p> <ol> <li>If \\(x_n \\rightarrow x\\), then \\(|x_n| \\rightarrow |x|\\)</li> <li>If \\(|x_n| \\rightarrow 0\\), then \\(x_n \\rightarrow 0\\)</li> <li>\\(x_n \\rightarrow x\\) if and only if \\(|x_n - n| \\rightarrow 0\\)</li> </ol> <p>Theorem. Suppose \\((x_n)\\) is a sequence if real numbers, with each \\(x_n \\geq 0\\). Then, given some \\(k \\in \\mathbb{N}\\), if \\(x_n \\rightarrow x\\), then \\(\\sqrt[k]{x_n} \\rightarrow \\sqrt[k]{x}\\).</p>"},{"location":"math/real-analysis/3-sequences-series/#section-33-monotonic-sequences","title":"Section 3.3 - Monotonic Sequences","text":"<p>Definition. A sequence \\((x_n)\\) is monotonically increasing if \\(x_{n+1} \\geq x_n\\) for all \\(n \\in \\mathbb{N}\\).</p> <p>Definition. A sequence \\((x_n)\\) is monotonically decreasing if \\(x_{n+1} \\leq x_n\\) for all \\(n \\in \\mathbb{N}\\).</p> <p>Definition. A sequence is monotonic if it is either monotonically increasing or decreasing.</p> <p>Theorem. A monotonic sequence is converging if and only if it is bound.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-34-subsequences","title":"Section 3.4 - Subsequences","text":"<p>Definition. Let \\(X = (x_n)\\) be a sequence in \\(\\mathbb{R}\\). Then, the sequence</p> \\[ X_{n_k} = (x_{n_1}, x_{n_2}, \\ldots) \\] <p>is a subsequence of \\(X\\),</p> <p>Theorem. If a sequence converges to \\(x\\), then every subsequence also converges to \\(x\\).</p> <p>Theorem. Every sequence of real numbers \\((x_n)\\) contains a monotonic subsequence \\((x_{n_k})\\).</p> <p>Corollary. Bolzano-Weierstrass Theorem. Every bounded sequence of real numbers has a convergent subsequence.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-35-the-cauchy-criterion","title":"Section 3.5 - The Cauchy Criterion","text":"<p>Definition. A sequence \\((x_n)\\) is said to be a Cauchy sequence such that for any given \\(\\varepsilon\\), there exists a natural number \\(H\\) such that all natural numbers \\(m, n \\geq H\\), then</p> \\[|x_m - x_n \\leq \\varepsilon\\] <p>Theorem. If \\((x_n)\\) is a Cauchy sequence, then \\((x_n)\\) is convergent.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-37-series","title":"Section 3.7 - Series","text":"<p>Definition. Let \\((x_n)\\) be a sequence in \\(\\mathbb{R}\\). Then, the infinite series generated by \\(X\\) is the sequence \\(S = (s_n)\\) with terms</p> \\[ s_1 = x_1; \\; s_{n+1} = s_n + x_{n+1} \\] <p>In other words, \\(s_n = \\sum_{i=1}^n x_i\\). We denote this series as \\(\\sum x_n\\).</p> <p>Definition. If this series is convergent to some number \\(s\\), we say that \\(s\\) is the sum of the series.</p> <p>For natural numbers \\(n &gt; m\\), note that</p> \\[ s_n - s_m = \\sum_{i=m + 1}^n x_i \\] <p>In particular, \\(s_n - s_{n - 1} = x^n\\). Thus, the Cauchy criteria takes the form</p> <p>Theorem. Cauchy Criteria for Series. The series \\(\\sum x_n\\) converges if and only if, for a given \\(\\varepsilon\\), there exists some natural number \\(H\\) such that when \\(m &gt; n &gt; H\\),</p> \\[ |s_m - s_n| = |\\sum_{i = m + 1}^n x_i| &lt; \\varepsilon \\] <p>Corollary. \\(n\\)-th Term Test. If \\(\\sum x_n\\) converges, then \\(x_n \\rightarrow 0\\).</p> <p>Corollary. Absolute Convergence Test. If \\(\\sum |x_n|\\) converges, then \\(\\sum x_n\\) converges.</p> <p>Theorem. A series with non-negative terms converges if and only if its sequence of partial sums is bounded.</p> <p>Theorem. \\(e = \\lim_{n \\rightarrow \\infty} (1+\\frac{1}{n})^n = \\sum_{n=0}^\\infty \\frac{1}{n!}\\)</p>"},{"location":"math/real-analysis/4-limits/","title":"Chapter 4 - Limits","text":""},{"location":"math/real-analysis/4-limits/#section-41-limits-of-functions","title":"Section 4.1 - Limits of Functions","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\). Then, a point \\(c \\in \\mathbb{R}\\) is a cluster point of \\(A\\) if for every \\(\\delta &gt; 0\\), the \\(\\delta\\)-neighborhood of \\(c\\) contains a point \\(a \\in A\\) such that \\(a \\neq c\\). That is, there exists some \\(a\\) such that \\(0 &lt; |a - c| &lt; \\delta\\).</p> <p>Theorem. A real number \\(c\\) is a cluster point for a set \\(A\\) if and only if there exists a sequence \\((a_n)\\) in \\(A\\\\ \\{c\\}\\) such that \\(a_n \\rightarrow c\\)</p> <p>Corollary. A real number \\(c\\) is a cluster point of a set \\(A\\) if and only if every \\(\\delta\\)-neighborhood contains infinitely many points of \\(A\\).</p> <p>Definition. The set of every cluster point of \\(A\\) is called the derived set of \\(A\\), and denoted \\(A'\\).</p> <p>Corollary. A set \\(A\\) is closed if and only if \\(A' \\subseteq A\\).</p> <p>Remark. If \\(A'\\) is the derived set of \\(A\\), then \\(A'' \\subseteq A'\\).</p> <p>Remark. Intervals involving infinity and square brackets for the constant are closed.</p> <p>Definition. Suppose \\(f: A \\rightarrow \\mathbb{R}\\) is a function with domain \\(A \\subseteq \\mathbb{R}\\), and let \\(c \\in A\\) be a cluster point of \\(A\\). then, a real number \\(L\\) is a limit of \\(f\\) at \\(c\\) if given any \\(\\varepsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such that</p> \\[ 0 &lt; |x-c| &lt; \\delta \\Rightarrow |f(x) - L| &lt; \\varepsilon \\] <p>Theorem. For a given function and cluster point, there can be at most one limit at said point.</p> <p>Theorem. Let \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightarrow \\mathbb{R}\\). Then, to show that \\(lim_{x \\rightarrow c} f(x) = L\\), it suffices to show that for every sequence \\((a_n)\\) in \\(A\\\\ \\{c\\}\\), the sequence \\((f(a_n))\\) converges tto \\(L\\).</p> <p>Definition. The extended real numbers are \\(\\hat{\\mathbb{R}} = \\mathbb{R} \\cup \\{ \\infty, -\\infty \\}\\) are a totally-ordered set with supremum and infimum. Note that this set is no longer a field.</p> <p>Definition. At any point \\(c\\), the limit of \\(f\\) at \\(c\\) is infinite if given some \\(\\alpha\\),  there exists some \\(V_\\delta(c)\\) such that for all \\(x \\in V_\\varepsilon(c)\\), then \\(f(x) \\in V_\\alpha(\\infty)\\).</p> <p>Definition. The limit of a function at infinity is defined if for a given \\(\\varepsilon\\), there exists some \\(\\alpha\\) so that there exists some \\(V_\\delta(c)\\) such that for all \\(x \\in A\\),</p> \\[ x &gt; \\alpha \\Rightarrow |f(x) - L| &lt; \\varepsilon \\]"},{"location":"math/real-analysis/4-limits/#section-42-limit-theorems","title":"Section 4.2 - Limit Theorems","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\) and \\(c \\in \\mathbb{R}\\) be a cluster point of \\(A\\). Then, a function \\(f: A \\rightarrow \\mathbb{R}\\) is bounded on a neighborhood of \\(c\\) if there exists some \\(\\delta\\)-neighborhood \\(V_\\delta(c)\\) of \\(c\\) and some constant \\(M &gt; 0\\) such that for all \\(x \\in A \\cap V_\\delta(c)\\), then \\(|f(x)| \\leq M\\).</p> <p>Theorem. If \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightarrow \\mathbb{R}\\) has a finite limit at \\(c \\in \\mathbb{R}\\), then \\(f\\) is bounded on some neighborhood of \\(c\\).</p> <p>Theorem. With \\(A \\subseteq \\mathbb{R}\\), and \\(f, g: A \\rightarrow \\mathbb{R}\\), with \\(c \\in \\mathbb{R}\\) a cluster point of \\(A\\), then if \\(\\lim_{x \\rightarrow c} f(x) = L\\) and \\(\\lim_{x \\rightarrow c} g(x) = M\\), then:</p> \\[\\lim_{x \\rightarrow c} (f(x) + g(x)) = L + M\\] \\[\\lim_{x \\rightarrow c} (f(x)g(x)) = LM\\] <p>Additionally, if \\(g(x) \\neq 0\\) for all \\(x \\in A\\), and \\(M \\neq 0\\), then</p> \\[ \\lim_{x \\rightarrow c} \\frac{f(x)}{g(x)} = \\frac{L}{M} \\] <p>Corollary. If \\(p, q \\in \\mathbb{R}[x]\\), and \\(q(c) \\neq 0\\) for some \\(c \\in \\mathbb{R}\\), then</p> \\[ \\lim_{x \\rightarrow c} p(x) = p(c) \\] \\[ \\lim_{x \\rightarrow c} \\frac{p(x)}{q(x)} = \\frac{p(c)}{q(c)} \\] <p>Theorem. Squeeze Theorem. Let \\(A \\subseteq \\mathbb{R}\\). Then, if \\(f, g, h: A \\rightarrow \\mathbb{R}\\) and with \\(c \\in \\mathbb{R}\\) being a cluster point of \\(A\\), then if both</p> \\[ \\lim_{x \\rightarrow c} f(x) = \\lim_{x \\rightarrow c} h(x) = L \\] \\[ f(x) \\leq g(x) \\leq h(x) \\; \\text{ for all } x \\in A, x \\neq c \\] <p>Then, \\(\\lim_{x \\rightarrow c} g(x) = L\\).</p>"},{"location":"math/real-analysis/5-continuity/","title":"Chapter 5 - Continuity","text":""},{"location":"math/real-analysis/5-continuity/#section-51-continuous-functions","title":"Section 5.1 - Continuous Functions","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\), and \\(f: A \\rightarrow \\mathbb{R}\\). Then, if \\(a \\in A\\), \\(f\\) is continuous at \\(a\\) if, given any \\(\\varepsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such that for all \\(x \\in A\\),</p> \\[ |x - a| &lt; \\delta \\Rightarrow |f(x) - f(a)| &lt; \\varepsilon \\] <p>Note that if \\(a\\) is an isolated point of \\(A\\), that is, not a cluster point, then \\(a\\) is automatically continuous.</p> <p>If \\(a\\) is a cluster point of \\(A\\), then this definition collapses to the definition of \\(\\lim_{x \\rightarrow a} f(x) = f(a)\\).</p> <p>Note that a function cannot be continuous at a point outside of its domain, even if the limit exists.</p> <p>Definition. \\(f\\) is continuous on \\(A\\) if it is continuous at every point \\(a \\in A\\).</p> <p>Theorem. \\(f\\) is continuous if and only if for every sequence \\((x_n)\\) in \\(A\\) that converges to \\(a\\), the sequence \\((f(x_n))\\) converges to \\(f(a)\\).</p> <p>Definition. Let \\((S, d_S)\\) and \\((T, d_T)\\) be metric spaces. A function \\(f: S \\rightarrow T\\) is continuous at a point \\(a \\in S\\) if given any \\(\\varepsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such that for all \\(x \\in S\\),</p> \\[ d_S(x, a) &lt; \\delta \\Rightarrow d_T(f(x),  f(a)) &lt; \\varepsilon \\] <p>Theorem. A function \\(f: S \\rightarrow T\\) is continuous at a point \\(a \\in A\\) if and only if given some neighborhood \\(V(f(a)) \\in B\\), there exists some \\(U(a) \\in A\\) such that \\(f(U) \\subseteq V\\).</p>"},{"location":"math/real-analysis/5-continuity/#section-52-combinations-of-continuous-functions","title":"Section 5.2 - Combinations of continuous Functions","text":"<p>Theorem. Let \\(f, g: A \\rightarrow \\mathbb{R}\\) be continuous at \\(a \\in A\\). Then,</p> <ul> <li>\\(f + g\\) and \\(fg\\) are continuous at \\(a\\)</li> <li>If \\(g(x) \\neq 0\\) for all \\(x \\in A\\), then \\(\\frac{f}{g}\\) is continuous at \\(a\\).</li> </ul> <p>As a consequence, every polynomial, rational, and basic trigonometric function are continuous on its domains.</p> <p>Theorem. Lett \\(A, B \\subseteq \\mathbb{R}\\), such that \\(f: A \\rightarrow B\\) and \\(g: B \\rightarrow \\mathbb{R}\\). Then, if \\(c\\) is a cluster point of \\(A\\) such that \\(\\lim_{x \\rightarrow c} f(x) = L \\in B\\) and \\(g\\) is continuous at \\(L\\), then</p> \\[ \\lim_{x \\rightarrow c} g(f(x)) = g(L) = g(\\lim_{x \\rightarrow c} f(x)) \\] <p>Corollary. let \\(A, B \\subseteq  \\mathbb{R}\\), with \\(f: A \\rightarrow B\\) and \\(g: B \\rightarrow \\mathbb{R}\\). If \\(f\\) is continuous at \\(a \\in A\\) and \\(g\\) is continuous at \\(f(a) \\in B\\), then \\(g(f(x))\\) is continuous at \\(a\\).</p>"},{"location":"math/real-analysis/5-continuity/#section-53-continuous-functions-on-intervals","title":"Section 5.3 - continuous functions on Intervals","text":"<p>Theorem. Let \\(S, T\\) be metric spaces with \\(A \\subseteq S\\) and \\(f: A \\rightarrow T\\). If \\(A\\) is a compact subset of \\(S\\), then \\(f(A)\\) is a compact subset of \\(T\\).</p> <p>Corollary. Let \\(f: A \\rightarrow \\mathbb{R}\\) be a continuous function, with \\(A\\) being a compact subset of metric space \\(S\\). Then, \\(f(A)\\) is closed and bounded. Moreover, there exists a \\(p, q \\in A\\) such that \\(f(p)\\) and \\(f(q)\\) are the supremum and infimum of \\(f(A)\\).</p> <p>Corollary. Maximum-Minimum Theorem. If \\(I = [a, b]\\) is a closed and bounded interval and \\(f: I \\rightarrow \\mathbb{R}\\) is continuous on \\(I\\), then \\(f\\) has an absolute minimum and maximum on \\(I\\).</p> <p>Theorem. Let \\(S, T\\) be metric spaces and \\(A \\subseteq S\\). Then, if \\(f: A \\rightarrow T\\) is continuous on \\(A\\), and \\(A\\) is a connected subset of \\(S\\), then \\(f(A)\\) is a connected subset of \\(T\\).</p> <p>Corollary. Suppose that \\(I\\) is an interval. Let \\(f: I \\rightarrow \\mathbb{R}\\) be continuous on \\(I\\). Then, \\(f(I)\\) is an interval.</p> <p>Theorem. (Bolzano's) Intermediate Value Theorem. Suppose \\(f: [a, b] \\rightarrow \\mathbb{R}\\) is continuous on \\([a, b]\\) with \\(a \\neq b\\). Then, given some \\(k\\) such that \\(f(a) &lt; k &lt; f(b)\\), there exists some \\(c \\in (a, b)\\) such that \\(k = f(c)\\).</p> <p>Definition. Let \\(A \\subseteq R\\). Then, a function \\(f: A \\rightarrow \\mathbb{R}\\) is uniformly continuous if given any \\(\\varepsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) depending only on \\(\\varepsilon\\) such that for any \\(x, y \\in A\\),</p> \\[ |x - y| &lt; \\delta \\Rightarrow |f(x) - f(y)| &lt; \\varepsilon \\] <p>Note that if \\(f\\) is uniformly continuous, it must be continuous on \\(A\\).</p> <p>Theorem. Let \\(I = [a, b]\\) be a closed and bounded interval. If \\(f: I \\rightarrow \\mathbb{R}\\) is continuous on \\(I\\), then \\(f\\) is uniformly continuous.</p> <p>Remark. If \\(S, T\\) are metric spaces, \\(K\\) is a compact subset of \\(S\\), and \\(f: K \\rightarrow T\\) is continuous on \\(K\\), then \\(f\\) is uniformly continuous.</p> <p>Theorem. Suppose \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightarrow \\mathbb{R}\\) is uniformly continuous. Then, if \\((x_n)\\) is a Cauchy sequence in \\(A\\), \\((f(x_n))\\) is a Cauchy sequence in \\(\\mathbb{R}\\).</p> <p>Remark. Suppose \\(S, T\\) are metric spaces and \\(f: S \\rightarrow T\\) is uniformly continuous. Then, if \\((x_n)\\) is a Cauchy sequence in \\(S\\), \\((f(x_n))\\) is a Cauchy sequence in \\(T\\).</p>"},{"location":"math/real-analysis/6-derivatives/","title":"Chapter 6 - Derivatives","text":""},{"location":"math/real-analysis/6-derivatives/#section-61-the-derivative","title":"Section 6.1 - The Derivative","text":"<p>Definition. Let \\(I \\subseteq \\mathbb{R}\\) be an interval \\(f: I \\rightarrow \\mathbb{R}\\) a function, and \\(c \\in I\\). Then, the derivative of \\(f\\) at \\(c\\) is</p> \\[ f'(c) = \\lim_{x \\rightarrow c} \\frac{f(x)-f(c)}{x-c} \\] <p>provided that the limit exists. Thus, we obtain \\(f'(x)\\), or the derivative of \\(f\\), with a domain of all points \\(c \\in I\\) where the limit exists.</p> <p>Theorem. If \\(f: I \\rightarrow \\mathbb{R}\\) is differentiable at point \\(c\\), it is continuous at point \\(c\\).</p> <p>Theorem. Let \\(f, g: I \\rightarrow \\mathbb{R}\\) be differentiable at \\(c \\in I\\). Then,</p> <ul> <li>\\((f+g)'(c) = f'(c) + g'(c)\\)</li> <li>\\((fg)'(c) = f'(c)g(c) + f(c)g'(c)\\)</li> <li>\\((\\frac{f}{g})'(c) = \\frac{f'(c)g(c) - f(c)g'(c)}{(g(c))^2}\\)</li> </ul> <p>Theorem. Let \\(f: I \\rightarrow \\mathbb{R}\\) and \\(g: J \\rightarrow \\mathbb{R}\\), with \\(f(I) \\subseteq J\\). Then, if \\(f\\) is differentiable at \\(c \\in I\\) and \\(g\\) is differentiable att \\(f(c) \\in J\\), then \\(f \\circ g\\) is differentiable at \\(c\\), and</p> \\[ (g \\circ f)'(c) = g'(f(c))f'(c) \\] <p>Theorem. Suppose \\(f: I \\rightarrow \\mathbb{R}\\) is a one-to-one function on some interval \\(I\\). Then, with \\(J = f(I)\\) and \\(f^{-1}: J \\rightarrow \\mathbb{R}\\), for all \\(x \\in I\\),</p> \\[ f^{-1}(f(x)) = x \\] <p>Lemma. \\(f\\) is continuous on some interval \\(I\\) if and only if it is monotonic on said interval.</p> <p>Theorem. Suppose \\(f: I \\rightarrow \\mathbb{R}\\) is a one-to-one function on some interval \\(I\\). Then, with \\(J = f(I)\\) and \\(f^{-1}: J \\rightarrow \\mathbb{R}\\), if \\(f\\) is continuous on \\(I\\) and \\(I\\) is an interval, then \\(f(I)\\) is an interval, and \\(f^{-1}\\) is continuous on \\(J\\).</p> <p>Theorem. Suppose \\(f: I \\rightarrow \\mathbb{R}\\) is differentiable at some \\(c \\in I\\). Then, with \\(J = f(I)\\) and \\(f^{-1}: J \\rightarrow \\mathbb{R}\\), if \\(f\\) is differentiable at \\(c\\) and \\(f'(c) \\neq 0\\), then \\(f^{-1}\\) is differentiable at \\(d = f(c)\\), and</p> \\[ (f^{-1})(d) = \\frac{1}{f'(c)} \\] <p>Definition. Let \\(I \\subseteq \\mathbb{R}\\) be an interval and let \\(f: I \\rightarrow \\mathbb{R}\\). Then, \\(f\\) has a relative maximum (or local maximum) at some point \\(c \\in I\\) if there exists some \\(\\delta\\)-neighborhood \\(V_\\delta(c)\\) such that for all \\(x \\in V_\\delta(C) \\cup I\\), then \\(f(x) \\leq f(c)\\). Relative minima are defined similarly.</p> <p>Theorem. Let \\(I\\) be an interval and \\(f: I \\rightarrow \\mathbb{R}\\). Then, if \\(f\\) has a relative extremum at an interior point \\(c \\in I\\), and if \\(f'(c)\\) exists, then \\(f'(c) = 0\\).</p> <p>Corollary. Suppose \\(f: [a, b] \\rightarrow \\mathbb{R}\\) and \\(g: [a, b] \\rightarrow \\mathbb{R}\\) are both continuous on \\([a, b]\\) and differentiable on \\((a, b)\\) with \\(a \\neq b\\). Then,</p> <ul> <li>Rolle's Theorem. If \\(f(a) = f(b)\\), then there exists at least one point \\(c \\in (a, b)\\) with \\(f'(c) = 0\\).</li> <li>If \\(f(a) = g(a)\\) and \\(f(b) = g(b)\\), then there exists aat least one point \\(c \\in (a, b)\\) such that \\(f'(c) = g'(c)\\).</li> </ul> <p>Theorem. Mean Value Theorem. Let \\(f\\) be continuous on \\([a, b]\\) and differentiable on \\((a, b)\\), with \\(a \\neq b\\). Then, there exists at least one \\(c \\in (a, b)\\) with</p> \\[ f(b) - f(a) = f'(c)(b - a) \\] <p>Theorem. Suppose \\(f: I \\rightarrow \\mathbb{R}\\) is differentiable on I. Then,</p> <ul> <li>If \\(f'(x) &gt; 0\\) for all $x \\in $I, then \\(f\\) is strictly increasing on \\(I\\).</li> <li>If \\(f'(x) = 0\\) for all $x \\in $I, then \\(f\\) is constant on \\(I\\).</li> <li>If \\(f'(x) &lt;&gt; 0\\) for all $x \\in $I, then \\(f\\) is strictly decreasing on \\(I\\).</li> </ul> <p>Theorem. Cauchy Mean Value Theorem. Let \\(f, g\\) be continuous on \\([a, b]\\) and differentiable on \\((a, b)\\). If \\(g'(x) \\neq 0\\) for all \\(x \\in (a, b)\\), and \\(a \\neq b\\), then there exists  at least one point \\(c \\in (a, b)\\) such that</p> \\[ \\frac{f(b)-f(a)}{g(b)-g(a)} = \\frac{f'(c)}{g'(c)} \\]"},{"location":"physics/electrodynamics/10-electromagnetic-waves/","title":"Chapter 10 - Electromagnetic Waves","text":""},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-101-time-dependent-electromagnetic-fields-in-a-vacuum-satisfy-the-wave-equation","title":"Section 10.1 - Time-Dependent Electromagnetic Fields in a Vacuum Satisfy the Wave Equation","text":"<p>Consider an empty space. Then, it is evident that</p> \\[\\begin{align}     \\nabla \\cdot \\mathbf{E} &amp;= 0 \\\\     \\nabla \\cdot \\mathbf{H} &amp;= 0 \\end{align}\\] \\[\\begin{align} \\nabla \\times \\mathbf{E} + \\frac{\\partial \\mathbf{B}}{\\partial t} &amp;= 0 \\\\ \\nabla \\times \\mathbf{H} - \\frac{\\partial \\mathbf{D}}{\\partial t} &amp;= 0 \\end{align}\\] <p>As \\(\\mathbf{B} = \\mu_0 \\mathbf{H}\\) and \\(\\mathbf{D} = \\varepsilon_0 \\mathbf{E}\\) in a vacuum, the third and fourth equations can be rewritten as</p> \\[\\begin{align} \\nabla \\times \\mathbf{E} + \\mu_0 \\frac{\\partial \\mathbf{H}}{\\partial t} &amp;= 0 \\\\ \\nabla \\times \\mathbf{H} - \\varepsilon_0 \\frac{\\partial \\mathbf{E}}{\\partial t} &amp;= 0 \\end{align}\\] <p>We can take the curl of both equations and then substitute to see that</p> \\[\\begin{align} \\nabla \\times \\nabla \\times \\mathbf{E} + \\mu_0 \\varepsilon_0 \\frac{\\partial^2 \\mathbf{E}}{\\partial t^2} &amp;= 0 \\\\ \\nabla \\times \\nabla \\times \\mathbf{H} + \\mu_0 \\varepsilon_0 \\frac{\\partial^2 \\mathbf{H}}{\\partial t^2} &amp;= 0 \\end{align}\\] <p>We can apply a vector identity to see</p> \\[\\begin{align} -\\nabla^2 \\mathbf{E} + \\mu_0 \\varepsilon_0 \\frac{\\partial^2 \\mathbf{E}}{\\partial t^2} &amp;= 0 \\\\ -\\nabla^2 \\mathbf{H} + \\mu_0 \\varepsilon_0 \\frac{\\partial^2 \\mathbf{H}}{\\partial t^2} &amp;= 0 \\end{align}\\]"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1011-the-wave-equation-and-plane-waves","title":"Section 10.1.1 - The Wave Equation and Plane Waves","text":"<p>Definition. The equation \\([\\frac{\\partial^2}{\\partial x^2} - \\frac{1}{v^2} \\frac{\\partial^2}{\\partial t^2}] f(x, t) = 0\\) is well-known to mathematicians (see Differential Equations), and is known as the wave equation. In physics, the speed of the wave is \\(v = c = \\frac{1}{\\sqrt{\\mu_0 \\varepsilon_0}}\\).</p> <p>Consider some function \\(f(s)\\). If \\(s = x - vt\\) or \\(x + vt\\), it is trivial to see that \\(f(x)\\) satisfies the wave equation.</p> <p>Definition. A plane wave is a solution to the Laplacian form of the last two Maxwell equations for empty space that also satisfy the one-dimensional wave equation. However, these solutions may not be valid electromagnetic waves as they are not guaranteed to satisfy the first two Maxwell equations.</p> <p>Notably, the functions for \\(\\mathbf{E} = \\mathbf{E}_0 f(s)\\) and \\(\\mathbf{H} = \\mathbf{H}_0 g(s)\\) do not have to be equal. However, \\(v = c\\).</p> <p>Definition. A plane electromagnetic wave is a plane wave which satisfies the first two Maxwell equations. The divergence equations restrict \\(\\mathbf{E}_0\\) and \\(\\mathbf{H}_0\\) to be in the plane normal to the direction of motion, as \\(\\hat{\\mathbf{K}} \\cdot \\mathbf{E}_0 = 0\\). That is, electomagnetic plane waves are transverse, not longitudinal.</p> <p>Additionally, the curl equations force \\(f(s) = g(s)\\), such that \\(H_0 = E_0 \\sqrt{\\frac{\\varepsilon_0}{\\mu_0}}\\).</p> <p>Definition. The quantity \\(Y_0 = \\sqrt{\\frac{\\varepsilon_0}{\\mu_0}}\\) is the vacuum admittance and its inverse, \\(Z_0 = \\sqrt{\\frac{\\mu_0}{\\varepsilon_0}}\\) is the vacuum impedance.</p> <p>If we assume the direction of propagation can be written as \\(\\hat{\\mathbf{k}}\\), we can write \\(f(s) = f(\\hat{\\mathbf{k}} \\cdot \\mathbf{r} - vt)\\), such that \\(\\mathbf{E}(\\mathbf{r}, t) = \\mathbf{E}_0 f(\\hat{\\mathbf{k}}\\cdot\\mathbf{r} - vt)\\), where \\(\\hat{\\mathbf{k}}\\cdot\\mathbf{E}_0 = 0\\).</p> <p>From this, we can see that \\(\\mathbf{H}(\\mathbf{r}, t) = \\sqrt{\\frac{\\varepsilon_0}{\\mu_0}} \\hat{\\mathbf{k}} \\times \\mathbf{E}_0 f(\\hat{\\mathbf{k}} \\cdot \\mathbf{r} - vt)\\). Similarly, \\(\\hat{\\mathbf{k}} \\cdot \\mathbf{H} = 0\\).</p> <p>Additionally, we can compute \\(\\mathbf{S} = \\mathbf{E} \\times \\mathbf{H} = c \\varepsilon_0 E_0^2 f^2(\\hat{\\mathbf{k}} \\cdot \\mathbf{r} - vt) \\hat{\\mathbf{k}}\\). We can also see that \\(\\varepsilon_0 E^2 = \\mu_0 H^2\\) at any given time.</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1012-monochromatic-plane-waves","title":"Section 10.1.2 - Monochromatic Plane Waves","text":"<p>In any simple material, we like to say that \\(\\mathbf{D} = \\varepsilon \\mathbf{E}\\) and \\(\\mathbf{B} = \\mu \\mathbf{H}\\). However, this only holds true at a fixed frequency \\(\\omega\\). For multiple frequencies, we see that \\(\\mathbf{D}(\\omega) = \\varepsilon(\\omega)\\mathbf{E}(\\omega)\\) and \\(\\mathbf{B}(\\omega) = \\mu(\\omega)\\mathbf{H}(\\omega)\\). This causes problems. As such, we will want to consider waves that are only composed of one frequency under Fourier decomposition.</p> <p>Definition. A monochromatic plane wave is a plane wave in which the full Fourier series of \\(f(x)\\) has only one term. That is, \\(f(x)\\) is \\(\\sin(x)\\) or \\(\\cos(x)\\). We furthermore define a wave vector \\(\\mathbf{k}\\) as \\(\\mathbf{k} = k \\hat{\\mathbf{k}}\\), so that \\(\\omega = kc\\). Then,</p> \\[\\begin{align} \\mathbf{E}(\\mathbf{r}, t) &amp;= \\mathbf{E_0} \\cos(\\mathbf{k} \\cdot \\mathbf{r} - \\omega t) \\\\ \\mathbf{H}(\\mathbf{r}, t) &amp;= \\sqrt{\\frac{\\varepsilon_0}{\\mu_0}} \\hat{\\mathbf{k}} \\times \\mathbf{E}_0 \\cos(\\mathbf{k} \\cdot \\mathbf{r} - \\omega t) \\end{align}\\] <p>Notably, the frequency, or number of cycles per second, is \\(f = \\frac{\\omega}{2\\pi}\\), and wavelength \\(\\lambda = \\frac{2\\pi}{k}\\).</p> <p>We can calculate the energy density \\(u\\), energy current density \\(\\mathbf{S}\\), momentum density \\(\\mathbf{g}\\), and momentum current density \\(-\\overleftrightarrow{\\mathbf{T}}\\)</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1013-monochromatic-plane-waves-in-a-linear-model","title":"Section 10.1.3 - Monochromatic Plane Waves in a Linear Model","text":"<p>Monochromatic plane waves with frequency \\(\\omega\\) in a simple linear material are similar to monochromatic plane waves in a vacuum, except when in a material, we know that the magnitude of the wave vector \\(k = \\frac{\\omega}{v}\\), and \\(v = \\frac{1}{\\sqrt{\\mu \\varepsilon}}\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1014-polarization-of-monochromatic-plane-waves","title":"Section 10.1.4 - Polarization of Monochromatic Plane Waves","text":"<p>Any plane wave described in such a way that \\(\\mathbf{E} = \\mathbf{E}_0 f(\\mathbf{k} \\cdot \\mathbf{r} - ct)\\) is linearly polarized in the direction of \\(\\mathbf{E}_0\\). That is, the direction of polarization is the direction of \\(\\mathbf{E}\\), and if that direction is unchanging, the wave is linearly polarized.</p> <p>Notably, an elliptically polarized wave can be described as follows:</p> \\[\\begin{align} \\mathbf{E}(\\mathbf{r}, t) &amp;= E_{x0} \\hat{\\mathbf{x}} \\cos(kz - \\omega t) + E_{y0} \\hat{\\mathbf{y}} \\sin(kz - \\omega t) \\\\ \\mathbf{H}(\\mathbf{r}, t) &amp;= \\sqrt{\\frac{\\varepsilon}{\\mu}} (E_{x0} \\hat{\\mathbf{y}} \\cos(kz - \\omega t) - E_{y0} \\hat{\\mathbf{x}} \\sin(kz - \\omega t)) \\end{align}\\] <p>If \\(E_{x0} = E_{y0}\\), the wave is said to be circularly polarized.</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-102-reflection-and-refraction-of-plane-electromagnetic-waves-at-a-a-planar-interface","title":"Section 10.2 - Reflection and Refraction of Plane Electromagnetic Waves at a a Planar Interface","text":"<p>This section will focus on plane monochromatic waves incident from material 1 onto material 2, where both materials are homogenous insulators and the surface between the two materials is smooth (on the scale of the wavelength).</p> <p>In this case, we must re-consider Maxwell's equations. We know from previous sections that \\(\\nabla \\cdot \\mathbf{E} = \\frac{\\mathbf{\\rho_e}}{\\varepsilon_0}\\) and \\(\\nabla \\cdot \\mathbf{H} = \\frac{\\mathbf{\\rho_m}}{\\mu_0}\\). We also know that \\(\\nabla \\cdot \\mathbf{D} = \\rho_{ef}\\) and \\(\\nabla \\cdot \\mathbf{B} = \\rho_{mf}\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1021-boundary-conditions-at-an-interface-between-two-materials","title":"Section 10.2.1 - Boundary Conditions at an Interface Between Two Materials","text":"<p>Consider the boundary between the two materials. If we consider \\(\\nabla \\cdot \\mathbf{D}\\), and take the integral over a Gaussian pillbox on the boundary, we can apply divergence theorem to see that \\(\\int_V \\nabla \\cdot \\mathbf{D} dV = \\int_{SofV} \\mathbf{D} \\cdot \\hat{\\mathbf{n}} dS = \\rho_{efree}\\). If we assume the materials are insulating, we do not expect to find any electrical charge, so \\(\\rho_{efree} = 0\\). Thus, we can say that \\(\\int_{SofV} D \\cdot \\hat{\\mathbf{n}} = 0\\), so \\(\\mathbf{D}_1 \\cdot \\hat{\\mathbf{n}} + \\mathbf{D}_2 \\cdot \\hat{\\mathbf{n}} = \\mathbf{D_1} \\cdot \\hat{\\mathbf{z}} + \\mathbf{D}_1 \\cdot (-\\hat{\\mathbf{n}}) = 0\\). Then, we can say that \\(\\mathbf{D}_1 \\cdot \\hat{\\mathbf{n}} = \\mathbf{D}_2 \\cdot \\hat{\\mathbf{n}}\\), or in simpler terms, \\(\\mathbf{D}_1^\\perp = \\mathbf{D}_2^\\perp\\).</p> <p>Applying the same logic to \\(\\mathbf{B}\\), we see that \\(\\mathbf{B}_1^\\perp = \\mathbf{B}_2^\\perp\\). Note that due to the existence of polarization and magnetization, we cannot say the same regarding \\(\\mathbf{E}\\) or \\(\\mathbf{H}\\).</p> <p>Now, consider a rectangular loop along the interface. If we then consider \\(\\int_S (\\nabla \\times \\mathbf{E}) \\cdot d\\mathbf{S} = \\int_{\\partial S} \\mathbf{E} \\cdot d\\mathbf{l} = -\\frac{\\partial \\Phi_B}{\\partial t}\\). If we let the width of the rectangle approach \\(0\\), then \\(\\int_{\\partial S} \\mathbf{E} \\cdot d\\mathbf{l} = \\mathbf{E}_1 \\cdot d\\mathbf{l} + \\mathbf{E}_2 \\cdot d\\mathbf{l} = 0\\), which implies that \\(\\mathbf{E}_1 \\cdot d\\mathbf{l} = \\mathbf{E}_2 \\cdot d\\mathbf{l}\\), or in other words, \\(\\mathbf{E}_1^\\parallel = \\mathbf{E}_2^\\parallel\\).</p> <p>Applying the same logic to the other Maxwell equation, we see that \\(\\mathbf{H}_1^\\parallel = \\mathbf{H}_2^\\parallel\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1022-normal-incidence","title":"Section 10.2.2 - Normal Incidence","text":"<p>Consider a monochromatic plane wave that is incident normally from material 1 onto material 2. That is, the wave vector \\(\\mathbf{k}\\) is normal to the interface. In this case, we will let the interface be at \\(z=0\\) on the \\(x-y\\) plane, and \\(\\hat{\\mathbf{k}} = \\hat{\\mathbf{z}}\\).</p> <p>Here, we can write the incident wave as \\(\\mathbf{E}_i (z, t) = E_i \\hat{\\mathbf{x}} \\cos{k_1 z - \\omega t}\\). Then, Maxwell's equations give us \\(\\mathbf{H}_i(z, t) = Y_1 E_i \\hat{\\mathbf{y}} \\cos(k_1 z - \\omega 2)\\). By symmetry, we expect the reflected wave to be in the \\(-z\\) direction, with some phase \\(\\phi_r\\) such that the wave is a function of \\(\\cos(-k_1 z - \\omega t + \\phi_r)\\). Additionally, we expect the transmitted wave to be of the form \\(\\cos(k_2 z - \\omega t + \\phi_t)\\). This gives for material 1</p> \\[\\begin{align} \\mathbf{E}_1(z, t) &amp;= \\mathbf{E}_i(z, t) + \\mathbf{E}_r(z, t) &amp;= E_i \\hat{\\mathbf{x}} \\cos(k_1 z - \\omega t) + \\mathbf{E}_r \\cos(k_1 z - \\omega t - \\phi_r) \\\\ \\mathbf{H}_1(z, t) &amp;= \\mathbf{H}_i(z, t) + \\mathbf{H}_r(z, t) &amp;= Y_1 E_i \\hat{\\mathbf{y}} \\cos(k_1 z - \\omega t) + \\mathbf{H}_r \\cos(k_1 z - \\omega t - \\phi_r) \\\\ \\end{align}\\] <p>In material 2, we see that</p> \\[\\begin{align} \\mathbf{E}_2(z, t) &amp;= \\mathbf{E}_t \\cos(k_2 z - \\omega t - \\phi_t) \\\\ \\mathbf{H}_2(z, t) &amp;= \\mathbf{H}_t \\cos(k_2 z - \\omega t - \\phi_t) \\\\ \\end{align}\\] <p>Now, break \\(\\mathbf{E}_r\\) into components such that \\(\\mathbf{E}_r = E_{rx} \\hat{\\mathbf{x}} + E_{ry} \\hat{\\mathbf{y}}\\). Then, we see that</p> \\[\\begin{align} \\mathbf{E}_r(z, t) &amp;= (E_{rx} \\hat{\\mathbf{x}} + E_{ry} \\hat{\\mathbf{y}}) \\cos(k_1 z - \\omega t - \\phi_r) \\\\ \\mathbf{H}_r(z, t) &amp;= -Y \\hat{\\mathbf{z}} \\times (E_{rx} \\hat{\\mathbf{x}} + E_{ry} \\hat{\\mathbf{y}}) \\cos(k_1 z - \\omega t - \\phi_r) \\\\ \\end{align}\\] <p>Evaluation at \\(z = 0\\), we see that</p> \\[\\begin{align} \\mathbf{E}_1 &amp;= \\hat{\\mathbf{x}} E_i \\cos(-\\omega t) + (E_{rx} \\hat{\\mathbf{x}} + E_{ry} \\hat{\\mathbf{y}}) \\cos(-\\omega t - \\phi_r) \\\\ \\mathbf{H}_1 &amp;= \\hat{\\mathbf{y}} Y_1 E_i \\cos(-\\omega t) + Y_1(-E_{rx} \\hat{\\mathbf{y}} + E_{ry} \\hat{\\mathbf{x}}) \\cos(-\\omega t - \\phi_r) \\\\ \\end{align}\\] <p>In material 2, we see that</p> \\[\\begin{align} \\mathbf{E}_2(z, t) &amp;= (E_{tx} \\hat{\\mathbf{x}} + E_{ty} \\hat{\\mathbf{y}}) \\cos(-\\omega t - \\phi_t) \\\\ \\mathbf{H}_2(z, t) &amp;= Y_2(E_{tx} \\hat{\\mathbf{y}} - E_{ty} \\hat{\\mathbf{x}}) \\cos(-\\omega t - \\phi_t) \\\\ \\end{align}\\] <p>In this case, applying boundary conditions is as simple as matching components to obtain two (out of four) equations:</p> \\[\\begin{align} \\mathbf{E}_y &amp;: E_{ry} \\cos(-\\omega t - \\phi_r) &amp;= E_{ty} \\cos(-\\omega t - \\phi_t) \\\\ \\mathbf{H}_x &amp;: Y_1 E_{ry} \\cos(-\\omega t - \\phi_r) &amp;= -Y_2 E_{ty} \\cos(-\\omega t - \\phi_t) \\end{align}\\] <p>However, this would imply that \\(Y_1 = -Y_2\\), which is not possible for ordinary materials. As such, we set \\(E_{ry} = 0\\), which then implies \\(E_{ty} = 0\\) (or vice-versa). Then, with \\(E_r = E_{rx}\\) and \\(E_t = E_{tx}\\), we see that the other two component-wise equations yield</p> \\[\\begin{align} E_i \\cos(\\omega t) + E_r \\cos(\\omega t - \\phi_r) &amp;= E_t \\cos(\\omega t - \\phi_t) \\\\ Y_1 E_i \\cos(\\omega t) - Y_1 E_r \\cos(\\omega t - \\phi_r) &amp;= Y_2 E_t \\cos(\\omega t - \\phi_t) \\\\ \\end{align}\\] <p>From this, we can apply the identity \\(\\cos(\\omega t - \\phi) = \\cos(\\omega t) \\cos \\phi + \\sin(\\omega t) \\sin \\phi\\) to split each equation into two equations that must hold for any \\(t\\). Then, comparing the \\(E \\sin \\omega t\\) and \\(H \\sin \\omega t\\) equations lead us to the conclusion that \\(\\sin \\phi_r = \\sin \\phi_t = 0\\). Thus, we see that \\(E_i + E_r = E_t\\) and \\(Y_1 (E_i - E_r) = Y_2 E_t\\). We can solve this system to see</p> \\[\\begin{align} E_t &amp;= \\frac{2Y_1}{Y_1 + Y_2} E_i \\\\ E_r = \\frac{Y_1 - Y_2}{Y_1 + Y_2} E_i \\end{align}\\] <p>If we assume \\(\\mu_1 = \\mu_2\\), we can rewrite the equations in terms of wave numbers \\(k_i\\), where \\(k_i = \\frac{\\omega}{v_i} = \\omega \\sqrt{\\mu_i \\varepsilon_i}\\)</p> \\[\\begin{align} E_t &amp;= \\frac{2k_1}{k_1 + k_2} E_i \\\\ E_r = \\frac{k_1 - k_2}{k_1 + k_2} E_i \\end{align}\\] <p>Definition. We can also define the index of refraction for a material \\(n\\) as \\(n_i = \\sqrt{\\frac{\\varepsilon_i \\mu_i}{\\varepsilon_0 \\mu_0}}\\).</p> <p>When \\(\\mu_1 = \\mu_2 = \\mu_0\\), we can write as \\(n_i = \\sqrt{\\frac{\\varepsilon_i}{\\varepsilon_0}}\\). Then, we can write the reflected and transmitted amplitudes as</p> \\[\\begin{align} E_t &amp;= \\frac{2n_1}{n_1 + n_2} E_i \\\\ E_r &amp;= \\frac{n_1 - n_2}{n_1 + n_2} E_i \\end{align}\\] <p>We can also define the average power incident on the interface as \\(I_(in) = \\langle \\mathbf{S}_i \\cdot \\hat{\\mathbf{n}} \\rangle = \\langle (\\mathbf{E}_i \\times \\mathbf{H}_i) \\cdot \\hat{\\mathbf{n}} \\rangle\\). We know that \\(\\mathbf{E}_i \\times \\mathbf{H}_i\\) is in the direction of \\(\\hat{\\mathbf{n}}\\), so \\(I_{in} = \\langle (\\mathbf{E}_i \\times (Y_1 \\hat{\\mathbf{k}} \\times \\mathbf{E}_i) \\cdot \\hat{\\mathbf{z}}) \\rangle = \\frac{1}{2} Y_1 E_i^2\\). We can further define \\(I_r = -\\frac{1}{2} Y_1 E_t^2\\) and \\(I_t = \\frac{1}{2}Y_2 E_t^2\\).</p> <p>We can then compute \\(R = \\frac{|I_r|}{I_{in}} = \\frac{E_r^2}{E_i^2} = (\\frac{Y_1 - Y_2}{Y_1 + Y_2})^2\\) to see the fraction of power reflected and \\(I_t = \\frac{I_t}{I_{in}} = \\frac{Y_2 E_t^2}{Y_1 E_i^2} = \\frac{4 Y_1 Y_2}{(Y_1 + Y_2)^2}\\). Note that mathematically, \\(R + T = 1\\). That is, all the power of the incident wave is either reflected or transmitted. Additionally, when \\(\\mu_1 \\approx \\mu_2\\), we can replace \\(Y\\) with \\(n\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1023-oblique-incidence","title":"Section 10.2.3 - Oblique Incidence","text":"<p>Now, we assume that the wave is incident to the interface at some angle \\(\\theta_i\\).</p> <p>Definition. The interfacial plane is the interface. The plane of incidence is the plane defined by the incident wave vector \\(\\mathbf{k}_i\\) and the normal vector of the interfacial plane \\(\\mathbf{z}\\)</p> <p>Theorem. Snell's Law. In the plane if incidence, the continuity of the electromagnetic field implies that for all times on the \\(z=0\\) plane, the argument of the \\(\\cos(\\mathbf{k}_{i,r,t} \\cdot \\mathbf{r} - \\omega t)\\) must be equal. That is, in our example, for \\(\\mathbf{r} = x \\hat{\\mathbf{x}} + y \\hat{\\mathbf{y}}\\), we see that \\(\\mathbf{k}_i \\cdot \\mathbf{r} = \\mathbf{k}_t \\cdot \\mathbf{r} + \\phi_r = \\mathbf{k}_t \\cdot \\mathbf{r} + \\phi_t\\). Then, \\(\\phi_r\\) and \\(\\phi_t\\) must vanish, and the wave vectors must satisfy \\(k_{ix}x + k_{iy}y = k_{rx}x + k_{ry}y = k_{tx}x + k_{ty}y\\). This implies that \\(k_{ix} = k_{rx} = k_{tx}\\) and \\(k_{iy} = k_{ry} + k_{ty}\\).</p> <p>More generally, the requirement of continuity on the interface implies that the three wave vectors are coplanar and that components parallel to the interface must be equal, leading to \\(k_i \\sin \\theta_i = k_r \\sin \\theta_r = k_t \\sin \\theta_t\\). Note that in this case, \\(k_i = k_r = \\frac{\\omega}{v}\\) as they describe propagation in the same media, so \\(\\theta_r = \\theta_i\\). As \\(n \\propto \\frac{1}{v}\\), we see that \\(n \\propto k\\), so we can rewrite Snell's Law as \\(n_i \\sin \\theta_i = n_t \\sin \\theta_t\\). This gives us the following wave vectors:</p> \\[\\begin{align} \\mathbf{k}_i &amp;= k_i(\\sin \\theta_i \\hat{\\mathbf{x}} + \\cos \\theta_i \\hat{\\mathbf{z}}) \\\\ \\mathbf{k}_r &amp;= k_i(\\sin \\theta_i \\hat{\\mathbf{x}} - \\cos \\theta_i \\hat{\\mathbf{z}}) \\\\ \\mathbf{k}_t &amp;=(k_t \\sin \\theta_t \\hat{\\mathbf{x}} + k_t \\cos \\theta_t \\hat{\\mathbf{z}}) = (k_i \\sin \\theta_i \\hat{\\mathbf{x}} + k_t \\cos \\theta_t \\hat{\\mathbf{z}}) \\end{align}\\] <p>Additionally, if we write Snell's Law as \\(\\sin \\theta_t = \\frac{n_i}{n_t} \\sin\\theta_i\\), we c an see that it predicts the angle of refraction. Note that if \\(\\frac{n_i}{n_t} \\sin\\theta_i \\geq 1\\), we see total internal reflection and there will be no transmitted waves.</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#reflected-and-transmitted-fields-for-oblique-incidence","title":"Reflected and transmitted Fields for Oblique Incidence","text":"<p>Any incident oblique wave can be written as a superposition of two waves, one with transverse magnetic and electric fields respectively. A transverse electric (TE) wave has its electric field perpendicular to the plane of incidence. Respectively, a transverse magnetic (TM) wave has its magnetic field perpendicular to the plane of incidence.</p> <p>Thus, if \\(\\hat{\\mathbf{k}}_i = \\sin \\theta_i \\hat{\\mathbf{x}} + \\cos \\theta_i \\hat{\\mathbf{z}}\\), we can say that an incident wave with arbitrary polarization can be written as \\(\\mathbf{E}(\\mathbf{r}, t) = (E_{TE}(-\\hat{\\mathbf{y}}) + E_{TM}\\hat{\\mathbf{k}} \\times (-\\hat{\\mathbf{y}})) \\cos(\\mathbf{k}_i \\cdot \\mathbf{r} - \\omega t)\\).</p> <p>In the case of a transverse electric field, \\(\\mathbf{E}\\) is perpendicular to the plane of incidence. We know that \\(\\mathbf{E}_r \\perp \\mathbf{k}_r\\) and \\(\\mathbf{E}_t \\perp \\mathbf{k}_r\\). Knowing our definition of \\(\\mathbf{r}\\), we can say that \\(E_{rz} = \\tan \\theta_i E_{rx}\\) and \\(E_{tz} = -\\tan \\theta_t E_{tx}\\). Continuity of \\(\\mathbf{D}^\\perp\\) requires that \\(\\varepsilon_1 E_{rz} = \\varepsilon_2 E_{tz}\\), and continuity of \\(\\mathbf{E}^\\parallel\\) requires that \\(E_{rx} = E_{tx}\\). We can then see that \\(\\varepsilon_1 \\tan \\theta_i = -\\varepsilon_2 \\tan \\theta_t E_{tx}\\), which we can substitute \\(E_{rx} = E_{tx}\\) to see that \\(\\varepsilon_1 \\tan \\theta_i = \\varepsilon_2 \\tan \\theta_t\\). However, this gives us an equation for \\(\\tan \\theta_t\\) which contradicts with an equation that is not derived in the textbook. This implies that the \\(x\\) and \\(z\\) components of the electric field vanish.</p> <p>All that is left is the \\(y\\)-direction, for which continuity requires that \\(E_i + E_r = E_t\\). The continuity of the \\(x\\)-component of the magnetic fields thus requires that \\([Y_1 (\\hat{\\mathbf{k}}_i \\times E_i \\hat{\\mathbf{y}} + \\hat{\\mathbf{k}}_r \\times E_r \\hat{\\mathbf{y}}) - Y_2 (\\hat{\\mathbf{k}}_t \\times E_t \\hat{\\mathbf{y}})] \\cdot \\hat{\\mathbf{x}} = 0\\). We can use the scalar triple product identity to simplify this to \\([Y_1 (\\hat{\\mathbf{k}}_i E_i + \\hat{\\mathbf{k}}_r E_r) - Y_2(\\hat{\\mathbf{k}}_t E_t)] \\cdot \\hat{\\mathbf{z}} = 0\\). This can be simplified to tell us that \\(Y_i (E_i - E_r) \\cos(\\theta_i) = Y_2 E_t \\cos \\theta_t\\). These can be solved to find that \\(E_t = \\frac{2Y_1 \\cos \\theta_i}{Y_1 \\cos \\theta_i + Y_2 \\cos \\theta_t} E_i\\) and \\(E_r = \\frac{Y_1 \\cos \\theta_i - Y_2 \\cos \\theta_t}{Y_1 \\cos \\theta_i + Y_2 \\cos \\theta_t}E_i\\).</p> <p>We can again calculate \\(R = \\frac{|I_r|}{I_{in}} = \\frac{E_r^2}{E_i^2} = (\\frac{Y_1 \\cos \\theta_i - Y_2 \\cos \\theta_t}{Y_1 \\cos \\theta_i + Y_2 \\cos \\theta_t})^2\\). Similarly, we see that \\(T = \\frac{4Y_1 \\cos(\\theta_i) \\cdot Y_2 \\cos(\\theta_t)}{(Y_1 \\cos \\theta_i + Y_2 \\cos \\theta_t)^2}\\). Once again, \\(I_r + I_t = I_{in}\\) and \\(R + T = 1\\).</p> <p>Now, consider the case of a transverse magnetic field. In this case, we know that \\(\\mathbf{H_{i, r, t}} = H_{i, r, t}\\hat{\\mathbf{y}}\\), and thus \\(\\mathbf{E} = Z \\mathbf{H} \\times \\hat{\\mathbf{k}}\\), where \\(Z = \\sqrt{\\frac{\\mu}{\\varepsilon}} = Y^{-1}\\).</p> <p>Thus, we can say that</p> \\[\\begin{align} \\mathbf{E}_i &amp;= Z_1 H_i \\hat{\\mathbf{y}} \\times \\hat{\\mathbf{k}}_i &amp;= Z_1 H_i (\\cos \\theta_i \\hat{\\mathbf{x}} - \\sin \\theta_i \\hat{\\mathbf{z}}) \\\\ \\mathbf{E}_r &amp;= Z_1 H_r \\hat{\\mathbf{y}} \\times \\hat{\\mathbf{k}}_r &amp;= -Z_1 H_r (\\cos \\theta_i \\hat{\\mathbf{x}} + \\sin \\theta_i \\hat{\\mathbf{z}}) \\\\ \\mathbf{E}_t &amp;= Z_2 H_t \\hat{\\mathbf{y}} \\times \\hat{\\mathbf{k}}_t &amp;= Z_2 H_t (\\cos \\theta_t \\hat{\\mathbf{x}} - \\sin \\theta_t \\hat{\\mathbf{z}}) \\\\ \\end{align}\\] <p>Then, continuity of \\(\\mathbf{H}^\\parallel\\) implies that \\(H_i + H_r = H_t\\), and continuity of the electric fields implies that \\(Z_1 (H_i - H_r) \\cos \\theta_i = Z_2 H_t \\cos \\theta_t\\).</p> <p>From this, we can solve for \\(H_r\\) and \\(H_t\\) to see that</p> \\[\\begin{align} H_r &amp;= \\frac{Z_1 \\cos \\theta_i - Z_2 \\cos \\theta_t}{Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t} H_i \\\\ H_t &amp;= \\frac{2Z_1 \\cos \\theta_i}{Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t} H_t \\end{align}\\] <p>Alongside \\(E_i = Z_1 H_i\\) and \\(H_t = Z_2 H_t\\), we can see that</p> \\[\\begin{align} E_r &amp;= \\frac{Z_2 \\cos \\theta_t - Z_1 \\cos \\theta_i}{Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t} H_i \\\\ E_t &amp;= \\frac{2Z_2 \\cos \\theta_i}{Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t} H_t \\end{align}\\] <p>The relative signs of \\(E_r\\) and \\(H_r\\) was chosen to agree with their relationship for normal incidence.</p> <p>Again, we can define \\(R\\) and \\(T\\) as</p> \\[\\begin{align} R &amp;= (\\frac{Z_2 \\cos \\theta_t - Z_1 \\cos \\theta_i}{Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t})^2 \\\\ T &amp;= \\frac{4Z_1 Z_2 \\cos \\theta_i \\cos \\theta_t}{(Z_1 \\cos \\theta_i + Z_2 \\cos \\theta_t)^2} \\end{align}\\] <p>Interestingly, if \\(Z_2 \\cos \\theta_t = Z_1 \\cos \\theta_i\\) for a wave with any polarization, the reflection's transverse magnetic component vanishes. If \\(\\mu_1 \\approx \\mu_2\\), we see that this condition becomes \\(n_1 \\cos \\theta_t = n_2 \\cos \\theta_i\\), which using Snell's law, becomes</p> \\[\\sqrt{1-(\\frac{n_1}{n_2})^2 \\sin^2 \\theta_i} = \\frac{n_2}{n_1} \\cos \\theta_i\\] <p>Definition. This is satisfied when \\(\\frac{n_2}{n_1} = \\tan \\theta_i\\). This angle, for which there is no reflected TM wave, is called Brewster's Angle.</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1024-reflection-from-conductors","title":"Section 10.2.4 - Reflection from Conductors","text":"<p>We know that in a conductor, with current density \\(\\sigma\\), we can state that \\(\\mathbf{J}_e = \\sigma \\mathbf{E}\\). This then reintroduces the current term in Maxwell's equation, so that \\(\\nabla \\times \\mathbf{H} = \\sigma \\mathbf{E} + \\varepsilon \\frac{\\partial \\mathbf{E}}{\\partial t}\\).</p> <p>Then, if we curl the curls, we see that</p> \\[\\begin{align} \\nabla \\times \\nabla \\times \\mathbf{E} &amp;= -\\mu \\frac{\\partial}{\\partial t}(\\nabla \\times \\mathbf{H}) &amp;= -\\mu \\frac{\\partial}{\\partial t}(\\sigma \\mathbf{E} + \\varepsilon \\frac{\\partial \\mathbf{E}}{\\partial t}) \\\\ \\nabla \\times \\nabla \\times \\mathbf{H} &amp;= \\sigma \\nabla \\times \\mathbf{E} + \\varepsilon \\frac{\\partial}{\\partial t}(\\nabla \\times \\mathbf{E}) &amp;= -\\mu\\sigma \\frac{\\partial}{\\partial t}\\mathbf{H} + \\varepsilon \\frac{\\partial}{\\partial t}(-\\mu \\frac{\\partial}{\\partial t}\\mathbf{H}) \\\\ \\end{align}\\] <p>Applying our identity for the curl of a curl and knowing the divergence of the electric and magnetic fields vanishes, we see that</p> \\[\\begin{align} \\nabla^2 \\mathbf{E} &amp;= -\\mu \\sigma \\frac{\\partial \\mathbf{E}}{\\partial t} - \\mu \\varepsilon \\frac{\\partial^2 \\mathbf{E}}{\\partial t^2} \\\\ \\nabla^2 \\mathbf{H} &amp;= -\\mu \\sigma \\frac{\\partial \\mathbf{H}}{\\partial t} - \\mu \\varepsilon \\frac{\\partial^2 \\mathbf{H}}{\\partial t^2} \\\\ \\end{align}\\] <p>We see that if \\(\\sigma = 0\\), we recover the equations for propagation in a linear material.</p> <p>We can solve these equations manually, but that would make us sad. Instead, we assume solutions of the form \\(\\mathbf{E} = E_0 \\hat{\\mathbf{x}} \\cos(kz - \\omega t) e^{-\\kappa z}\\).</p> <p>Through calculation, we see that</p> \\[\\begin{align} \\nabla^2 \\mathbf{E} &amp;= -k^2 E_0 \\hat{\\mathbf{x}} \\cos(kz - \\omega t) e^{-\\kappa z} + 2 k \\kappa E_0 \\hat{\\mathbf{x}} \\sin(kz - \\omega t) e^{-\\kappa z} + \\kappa^2 E_0 \\hat{\\mathbf{x}} \\cos(kz - \\omega t) e^{-\\kappa z} \\\\ \\mu \\sigma \\frac{\\partial \\mathbf{E}}{\\partial t} &amp;= \\mu \\sigma \\omega E_0 \\hat{\\mathbf{x}} \\sin(kz - \\omega t)^{-\\kappa z} \\\\ \\mu \\sigma \\frac{\\partial^2 \\mathbf{E}}{\\partial^2 t} &amp;= -\\mu \\sigma \\omega^2 E_0 \\hat{\\mathbf{x}} \\cos(kz - \\omega t)^{-\\kappa z} \\end{align}\\] <p>We can substitute this into the wave equation. For the wave equation to hold true at all times, by matching terms of \\(\\sin\\) and \\(\\cos\\) we see that \\(-k^2 + \\kappa^2 + \\mu \\varepsilon \\omega^2 = 0\\) and \\(\\mu \\sigma \\varepsilon - 2k \\kappa = 0\\). We can solve the second for \\(\\kappa\\) to see that \\(\\kappa = \\frac{\\mu \\sigma \\omega}{2k}\\). This then allows us to solve the first equation for \\(k^2\\), where we see</p> \\[k^2 = \\frac{\\mu \\varepsilon \\omega^2}{2}(1+\\sqrt{1+(\\frac{\\sigma}{\\varepsilon \\omega})^2})\\] <p>This then lets us solve the first equation (again) for \\(\\kappa^2\\), where we see that</p> \\[\\kappa^2 = \\frac{\\mu \\varepsilon \\omega^2}{2}(\\sqrt{1+(\\frac{\\sigma}{\\varepsilon \\omega})^2} - 1)\\] <p>Our final equations them become</p> \\[\\begin{align} \\kappa = \\omega \\sqrt{\\mu\\varepsilon} \\sqrt{\\frac{\\sqrt{1 + (\\frac{\\sigma}{\\varepsilon \\mu})^2} + 1}{2}} \\\\ \\kappa = \\omega \\sqrt{\\mu\\varepsilon} \\sqrt{\\frac{\\sqrt{1 + (\\frac{\\sigma}{\\varepsilon \\mu})^2} - 1}{2}} \\\\ \\end{align}\\] <p>Note that when \\(\\sigma \\ll \\varepsilon \\omega\\), the wave number collapses to \\(k = \\frac{\\omega}{v}\\), and we recover propagation in a vacuum. However, when \\(\\sigma \\gg \\varepsilon \\omega\\), we see that \\(k = \\kappa = \\sqrt{\\frac{\\mu \\omega \\sigma}{2}}\\). Here, the distance the wave propagates before decreasing by a factor of \\(\\frac{1}{e}\\) is known as the skin depth \\(d\\), where \\(d = \\frac{1}{\\kappa} = \\sqrt{\\frac{2}{\\mu \\omega \\sigma}}\\). This is significantly less than the wavelength \\(\\lambda = \\frac{2\\pi}{k}\\). Note that in this limit, the electromagnetic wave is heavily damped.</p> <p>The magnetic field is simpler to solve. We know that \\(\\frac{\\partial\\mathbf{H}}{\\partial t} = -\\frac{1}{\\mu}\\nabla \\times \\mathbf{E} = \\hat{\\mathbf{y}} \\frac{E_0 e^{-\\kappa z}}{\\mu}[k \\sin(kz - \\omega t) + \\kappa \\cos(kz - \\omega t)]\\)</p> <p>Integrating, we see that \\(\\mathbf{H} = \\hat{\\mathbf{y}} = \\frac{E_0 e^{-\\kappa z}}{\\mu \\omega}[k \\cos(kz - \\omega t) - \\kappa \\sin(kz - \\omega t)]\\).</p> <p>We can combine this to see \\(\\mathbf{H} = \\hat{\\mathbf{y}} e^{-\\kappa z} \\frac{\\sqrt{k^2 + \\kappa^2}}{\\mu \\omega} \\cos(kz - \\omega t + \\phi)\\) where \\(\\cos \\phi = \\frac{k}{\\sqrt{k^2 + \\kappa ^2}}\\) and \\(\\sim \\phi = \\frac{\\kappa}{\\sqrt{k^2 + \\kappa^2}}\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#reflection-of-em-waves-incident-from-an-insulator-onto-a-conductor","title":"Reflection of EM waves Incident from an Insulator onto a Conductor","text":"<p>In material \\(1\\) (\\(z &lt; 0\\)), we know that</p> \\[\\begin{align} \\mathbf{E}_1 &amp;= \\hat{\\mathbf{x}} E_i \\cos (k_1 z - \\omega t) + \\hat{\\mathbf{x}} E_r \\cos (-k_1 z - \\omega t + \\phi_r) \\\\ \\mathbf{H}_1 &amp;= Y_1\\hat{\\mathbf{y}} E_i \\cos (k_1 z - \\omega t) - Y_1 \\hat{\\mathbf{y}} E_r \\cos (-k_1 z - \\omega t + \\phi_r) \\end{align}\\] <p>We know that in material \\(2\\) (\\(x &gt; 0\\)),</p> \\[\\begin{align} \\mathbf{E}_1 &amp;= \\hat{\\mathbf{x}} E_t \\cos (k_2 z - \\omega t + \\phi_t) e^{-\\kappa z} \\\\ \\mathbf{H}_1 &amp;= \\hat{\\mathbf{y}} frac{\\sqrt{k_2^2 + \\kappa^2}}{\\mu_2 \\omega} \\cos (k_2 z - \\omega t + \\phi + \\phi_t) e^{-\\kappa z} \\end{align}\\] <p>Here, \\(\\phi\\) = \\(\\tan^{-1} (\\frac{\\kappa}{k_2})\\) os a phase shift intrinsic to the conductor.</p> <p>The continuity of \\(\\mathbf{E}^\\parallel\\) and \\(\\mathbf{H}^\\parallel\\) gives us two equations. We can then substitute in \\(\\omega t = 0\\):</p> \\[\\begin{align} E_i + E_r \\cos \\phi_r &amp;= E_t \\cos \\phi_t \\\\ E_i - E_r \\cos \\phi_r &amp;= \\frac{\\mu_1 v_1 \\sqrt{k_2^2 + \\kappa^2}}{\\mu_2 \\omega} E_t \\cos(\\phi_t + \\phi) \\\\ \\end{align}\\] <p>Now, substitute \\(\\omega t = \\frac{\\pi}{2}\\):</p> \\[\\begin{align} E_r \\sin \\phi_r &amp;= E_t \\sin \\phi_t \\\\ -E_r \\sin \\phi_r &amp;= \\frac{\\mu_1 v_1 \\sqrt{k_2^2 + \\kappa^2}}{\\mu_2 \\omega} E_t \\sin(\\phi_t + \\phi) \\\\ \\end{align}\\] <p>We can then assume \\(\\mu_1 \\approx \\mu_2\\). This modifies the second equations, leaving us with</p> \\[\\begin{align} E_i - E_r \\cos \\phi_r &amp;= \\frac{\\sqrt{k_2^2 + \\kappa^2}}{k_1} E_t \\cos(\\phi_t + \\phi) \\\\ -E_r \\sin \\phi_r &amp;= \\frac{\\sqrt{k_2^2 + \\kappa^2}}{k_1} E_t \\sin(\\phi_t + \\phi) \\\\ \\end{align}\\] <p>Now, apply the sine and cosine additive identities and the definitions for \\(\\sin \\phi\\) and \\(\\cos \\phi\\), alongside the definitions of \\(\\sin \\phi_t\\) and \\(\\cos \\phi_t\\).</p> <p>$$\\begin{align} E_i - E_r \\cos \\phi_r &amp;= E_t(\\cos(\\phi_t \\frac{k_2}{k_1}) - \\sin(\\phi_t \\frac{\\kappa}{k_1})) \\ -E_r &amp;= E_t (\\sin(\\phi_t \\frac{k_2}{k_1}) + \\cos(\\phi_t \\frac{\\kappa}{k_2})) \\end{align}</p> <p>Adding these equations lets us see that \\(\\tan(\\phi_t) = -\\frac{\\kappa}{k_1 + k_2}\\). Applying the \\(\\sigma \\rightarrow 0\\), we see \\(\\phi_t \\rightarrow 0\\). Applying \\(\\sigma \\gg \\varepsilon \\omega\\), we see that \\(\\phi_t \\rightarrow -\\frac{\\pi}{4}\\).</p> <p>Adding the first equations, we also wee that</p> \\[E_t = \\frac{2k_1 E_i}{\\sqrt{(k_1 + k_2)^2 + \\kappa^2}}\\] <p>This also has the correct limits.</p> <p>Furthermore, we can also obtain formulae for \\(\\tan \\phi_r = \\frac{-2k_1 \\kappa}{k_1^2 - k_2^2 - \\kappa^2}\\), and \\(E_r = \\frac{\\sqrt(k_1^2 - k_2^2 - \\kappa^2)^2 + (2 k_1 \\kappa)^2}{(k_1 + k_2)^2 + \\kappa^2} E_i\\). These collapse to the reflection at normal incidence values when \\(\\sigma \\rightarrow 0\\). If \\(\\sigma \\gg \\varepsilon \\mu\\), we see that \\(\\kappa \\approx k_2 \\gg k_1\\), as well as \\(\\tan \\phi_r \\rightarrow 0\\) and \\(\\frac{E_r}{E_i} \\rightarrow 1\\) (complete reflection).</p> <p>We can also compute energy currents. That is, \\(I_i = \\frac{1}{2} Y_1 E_i^2 = \\frac{1}{2} v_1 \\varepsilon_1 E_i^2\\), and \\(I_r = \\frac{1}{2}v_1 \\varepsilon_1 E_r^2\\).</p> <p>The transmitted wave is a bit more complicated. We see \\(I_t = \\langle \\mathbf{S}_t \\cdot \\hat{\\mathbf{z}} \\rangle = \\frac{\\sqrt{k_2^2 + \\kappa^2}}{k_1} Y_1 E_t^2 \\langle \\cos(\\omega t + \\phi_t) \\cos(\\omega t + \\phi_t + \\phi) \\rangle e^{-2\\kappa z} = \\frac{1}{2} Y_1 E_t^2 e^{-2\\kappa z}\\).</p>"},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-103","title":"Section 10.3","text":""},{"location":"physics/electrodynamics/10-electromagnetic-waves/#section-1031-response-functions-and-fourier-transforms","title":"Section 10.3.1 - Response Functions and Fourier Transforms","text":""},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/","title":"Chapter 8 - Quasi-static Electrodynamics and Alternating Current Circuits","text":""},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-81-the-quasi-static-regime-of-electrodynamics","title":"Section 8.1 - The Quasi-static Regime of Electrodynamics","text":""},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-811-rc-circuit-time-dependent-circuits-with-resistor-and-capacitor-in-series-transient-currents","title":"Section 8.1.1 - RC Circuit - Time-dependent Circuits with Resistor and Capacitor in Series Transient Currents","text":"<p>Here, we have a DC voltage source followed by a switch, capacitor with capacitance \\(C\\), and a resistor with resistance \\(R\\).</p> <p>By Kirchoff's Voltage Law, we know that \\(V_{cell} = V_{capacitor} + V_{resistor}\\), so \\(V_{cell} = IR + \\frac{Q}{C}\\). As current is simply charge over time,</p> \\[V_{cell} = R\\frac{dQ}{dt} + \\frac{Q}{C}\\] <p>Assuming the capacitor is uncharged at \\(t = 0\\), we obtain the initial condition \\(Q(0) = 0\\). Solving the differential equation, we see that</p> \\[Q(t) = C V_{cell} (1 - e^{-\\frac{t}{RC}})\\] <p>Then, taking the derivative, we see that</p> \\[I = \\frac{dQ}{dt} = \\frac{V_{cell}}{R} e^{-\\frac{t}{RC}}\\] <p>If instead we assume an AC voltage source, we know that \\(V_{cell} = V_0 \\cos \\omega t\\). Then, by KVL,</p> \\[{V_0 \\cos \\omega t = I(t)R + \\frac{Q(t)}{C}}\\] <p>Now, assume that \\(I(t) = I_0 \\cos(\\omega t + \\phi)\\), as the current may be out-of-phase with voltage. Additionally, we assume that charge cannot accumulate outside of the capacitor. Then, with \\(\\frac{d}{dt} Q(t) = I(t)\\), we see that</p> \\[Q(t) = Q_0 + \\frac{I_0}{\\omega} \\sin(\\omega t + \\phi)\\] <p>Substituting into the formula for voltage, we see that</p> \\[V_0 \\cos \\omega t = I_0 (R \\cos (\\omega t + \\phi) + \\frac{1}{\\omega C} \\sin(\\omega t + \\phi)) + \\frac{Q_0}{C}\\] <p>With trigonometric identities, this becomes</p> \\[[V_0 - I_0 \\cos \\phi - \\frac{I_0}{wC} \\sin \\phi]\\cos(\\omega t) + [I_0 R \\sin \\phi - \\frac{I_0}{\\omega C} \\cos \\phi]\\sin(\\omega t) + \\frac{Q_0}{C} = 0\\] <p>Now, assume let the \\(\\frac{Q_0}{C}\\) vanishes, as there is no constant charge on the capacitor (which would imply an additional constant voltage). Then, for this equation to hold true at all times, the coefficients of \\(\\sin \\omega t\\) and \\(\\cos \\omega t\\) must equal zero. This will allow us to see that</p> \\[\\cos \\phi = \\frac{R}{\\sqrt{R^2 + X_C^2}}\\] <p>Definition. Here, the term \\(X_C = (wC)^{-1}\\) is the capacitive reactance of the circuit.</p> <p>Definition. We also can solve for current to see that \\(I_0 = \\frac{V_0}{Z}\\), where \\(Z = \\sqrt{R^2 + X_C^2}\\), where \\(Z\\) is the impedence of the circuit.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-812-quasi-static-error-for-a-parallel-plate-capacitor","title":"Section 8.1.2 - Quasi-Static Error for a Parallel Plate Capacitor","text":"<p>Consider a parallel-plate capacitor. We know that within the capacitor, the electric flux is \\(\\mathbf{D}(t) = \\varepsilon_0 \\mathbf{E}(t)\\). With the charge on a plate given by \\(Q(t)\\), we can say that \\(\\mathbf{D}(t) = \\frac{Q(t)}{\\pi R^2}\\).</p> <p>We also know by Ampere's law that \\(\\nabla \\times \\mathbf{H} = \\frac{\\partial}{\\partial t} \\mathbf{D}\\). From this, given circular platFrom this, applying Stokes to Ampere's Law, we see that</p> \\[\\mathbf{H} = \\frac{\\partial Q}{\\partial t} \\frac{s}{2 \\pi R^2} \\hat{\\mathbf{\\varphi}}\\] <p>This continues on in this manner, however, I've opted to skip most of the math.</p> <p>The correction term \\(\\delta E\\) can be written as \\(\\delta E = (\\frac{\\pi s}{Tc})^2 E_0\\), where \\(T\\) is the period of the sinusoidal current oscillations (that is, \\(\\omega = 2\\pi/T\\), or \\(T = 2\\pi / \\omega\\)). If this is much smaller than the size of the device, the correction can be neglected.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-813-inductance","title":"Section 8.1.3 - Inductance","text":"<p>For an inductor, the voltage drop across an inductor is directly proportional to the change in current. That is,</p> \\[\\Delta V_L = L \\frac{d^2Q}{dt^2} = L\\frac{dI}{dt}\\] <p>Consider a circular current loop with a voltage source in the \\(x-y\\) plane. Then, applying Faraday's law, \\(\\int_{circle} = (\\nabla \\times \\mathbf{E}) \\cdot \\hat{\\mathbf{z}} dS = -\\frac{\\partial}{\\partial t} \\int_{circle} \\mathbf{B} \\cdot \\hat{\\mathbf{z}} dS\\).</p> <p>Apply Stokes' law to the left hand side to see that \\(\\int_{circumference} \\mathbf{E} \\cdot d\\mathbf{l} = -\\frac{\\partial \\Phi_B}{\\partial t}\\).</p> <p>With \\(\\Phi_B = LI\\), we see the \"back EMF\" opposing the increasing current will be \\(\\mathbf{\\mathcal{E}} = -L \\frac{\\partial I}{\\partial t}\\).</p> <p>Consider a circuit with a voltage source, a switch, an inductor, and a resistor in series. Then, by KVL, \\(V_{cell} = L \\frac{dI}{dt} + IR\\).</p> <p>This is a differential equation. Assuming \\(I(0) = 0\\), we can see that</p> \\[I(t) = \\frac{V_{cell}}{R}(1 - e^{-\\frac{R}{L}t})\\] <p>Now, consider an alternating current voltage source. Then, by KVL, \\(V_0 \\cos \\omega t = R I(t) + L \\frac{dI(t)}{dt}\\). If we assume that current is also sinusoidal, we can say that \\(I(t) = I_0 \\cos(\\omega t + \\phi)\\). Substituting into the voltage expression,</p> \\[V_0 \\cos \\omega t = R I_0 \\cos(\\omega t + \\phi) + L \\omega I_0 \\sin(\\omega t + \\phi)\\] <p>After expanding using trigonometric identities and setting coefficients to zero, we see that</p> \\[\\sin \\phi = -\\frac{X_L}{Z}; \\cos \\phi = \\frac{R}{Z}; I_0 = \\frac{V_0}{Z}\\] <p>Definition. Here, \\(X_Z = \\omega L\\) is the impedence of the circuit.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-814-calculation-of-inductance","title":"Section 8.1.4 - Calculation of Inductance","text":"<p>Recall that the back EMF \\(\\mathbf{\\mathcal{E}} = -L \\frac{\\partial I}{\\partial T}\\). We can calculate the work done by this force as follows.</p> \\[\\frac{dW}{dt} = I(\\mathbf{\\mathcal{E}}) = IL \\frac{dI}{dt} = \\frac{1}{2} L \\frac{d}{dt}(I^2)\\] <p>Then, integrating both sides, we see that</p> \\[W = \\frac{1}{2}L\\int_0^{I^2(t)} I^2(t') dt' = \\frac{1}{2}LI^2\\] <p>We also know from Section 3.2 that the work needed to create a magnetic field is as follows.</p> \\[W = \\frac{1}{2} \\mu_0 \\int_V H^2 dV\\] <p>Now, consider a long air-filled solenoid with \\(n\\) turns per unit length and cross-sectional area \\(A\\). Then, we know the flux through a cross-section of the solenoid will be \\(\\Phi = BA = \\mu_0 n I A\\). Then, the back-EMF for one loop of the solenoid can be given by \\(\\mathbf{\\mathcal{E}}_{1 loop} = - \\frac{d\\Phi}{dt} = -\\mu_0 n A \\frac{dI}{dt}\\). Then, the total induced EMF will be \\(\\mathbf{\\mathcal{E}} = nl\\mathbf{\\mathcal{E}}_{1 loop}\\), where \\(l\\) is the length of the solenoid. Thus, by the definition of back-EMF,</p> \\[\\mathbf{\\mathcal{E}} = -nl \\mu_0 nA \\frac{dI}{dt} = -L\\frac{dI}{dt}\\] <p>So, \\(L = \\mu_0 n^2 Al\\)</p> <p>We can also compute thsi by energy. We know that \\(W = \\frac{1}{2} \\mu_0 \\int_V H^2 dV = \\frac{1}{2} \\mu_0 n^2 I^2 Al = \\frac{1}{2}LI^2\\). This simplifies immediately.</p> <p>Now, consider a coaxial cable. That is, consider a solid cylinder of radius \\(a\\) that conducts current in the \\(+z\\) direction. The circuit is completed by a thin cylindical shell outside of the conductor yet still with radius \\(a\\). We also assume that current density is uniform within the cylindrical conductor.</p> <p>Recall that from Ampere's Law, for \\(s \\in (0, a)\\), we have \\(\\mathbf{H}(s) = \\frac{I_enc}{2\\pi s}\\hat{\\mathbf{\\varphi}}\\), with \\(I_enc = \\frac{I\\pi s^2}{\\pi a^2}\\). Thus, \\(\\mathbf{H} = \\frac{Is}{2\\pi a^2}\\hat{\\mathbf{\\varphi}}\\). Then,</p> \\[W = \\frac{1}{2} \\mu_0 \\int_V H^2 dV = \\frac{1}{2} \\mu_0 l \\int_0^a 2\\pi s ds (\\frac{Is}{2\\pi a^2})^2 = \\frac{I^2}{2} \\frac{\\mu_0 l}{2\\pi} \\frac{1}{4}\\] <p>This implies that \\(L = \\frac{\\mu_0 l}{8\\pi}\\).</p> <p>We can also solve this via flux. We know that \\(\\mathbf{B}(s) = \\frac{u_0 Is}{2\\pi a^2}\\hat{\\mathbf{\\varphi}}\\). Then,</p> \\[\\Phi = \\int_0^a \\mathbf{B} \\cdot \\hat{\\mathbf{n}} dS = \\int_0^a \\frac{\\mu_0 Is}{2\\pi a^2} l ds = \\frac{\\mu_0 I l}{2\\pi} \\frac{1}{2}\\] <p>This is off by a factor of \\(2\\). Instead, multiply by a fator of \\(f(s) = \\frac{s^2}{a^2}\\) to see</p> \\[\\Lambda = LI = \\int_0^a \\frac{\\mu_0 I s^3}{2\\pi a^4} l ds = \\frac{\\mu_0 I l}{2\\pi} \\frac{1}{4}\\] <p>Note that due to the Skin Effect, at high frequencies, back EMF must be considered even inside the conductor.</p> <p>This becomes complicated, and is thus omitted.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-815-quasi-static-error-for-a-solenoidal-inductor","title":"Section 8.1.5 - Quasi-static Error for a Solenoidal Inductor","text":"<p>In section 5.2, we learned that in a long solenoid, \\(\\mathbf{H} = nI \\mathbf{z}\\), where \\(I\\) is the current and \\(n\\) the number of turns per uniut length. This, however, was dependent of the current being constant. Now, let current be represented as \\(I(t) = I_0 \\cos (\\omega t + \\phi)\\). Now, \\(\\mathbf{H}(t) = nI(t) \\hat{\\mathbf{z}}\\).</p> <p>This implies magnetic flux with a density of \\(\\mathbf{B} = \\mu_0 n I(t) \\hat{\\mathbf{z}}\\). Given the inductor with radius \\(s\\), this will cause changing flux \\(\\Phi_B(t) = \\mathbf{B} * A = \\mu_0 n I(t) \\pi s^2\\).</p> <p>By Faraday's Law,</p> \\[E_{induced} = -N \\frac{\\partial \\Phi_B}{\\partial{t}}\\] <p>This can be rearranged to see</p> \\[2\\pi s \\mathbf{E}_{induced}(t) = -\\frac{\\partial \\Phi_b(t)}{\\partial t} = -\\mu_0 n \\pi s^2 \\frac{\\partial}{\\partial t} I(t)\\] <p>This can be used to find \\(\\mathbf{E} = -\\mu_0 n \\frac{s}{2} \\frac{\\partial}{\\partial t}I(t) \\hat{\\mathbf{\\varphi}}\\), equivalent to a flux density \\(\\mathbf{D}(t) = -\\varepsilon_0 \\mu_0 n \\frac{s}{2} \\frac{\\partial}{\\partial t}I(t) \\hat{\\mathbf{\\varphi}}\\). We can then apply Ampere's law to see that</p> \\[\\nabla \\times \\mathbf{H} = \\frac{\\partial \\mathbf{D}(t)}{\\partial t} = -\\varepsilon_0 \\mu_0 n \\frac{s}{2} \\frac{\\partial^2}{\\partial t^2}I(t) \\hat{\\mathbf{\\varphi}}\\] <p>We can work backwards to find that \\(\\mathbf{H}(s, t) = (1 - \\frac{\\mu_0 \\varepsilon_0 s^2 \\omega^2}{4}) H_{z0}(t)\\hat{\\mathbf{z}}\\). Then, the same conditions should apply as in 8.1.2. That is, \\(\\omega\\) should be low enough or the d evice small enough that light can easily propagate across the device during one period of oscillation.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-82-circuits-with-resistance-capacitance-and-inductance-and-a-sinusoidal-emf","title":"Section 8.2 - Circuits with Resistance, Capacitance and Inductance and a Sinusoidal EMF","text":""},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-821-rlc-circuits","title":"Section 8.2.1 - RLC Circuits","text":"<p>Consider an AC voltage source, a resistor with resistance \\(R\\), inductor with inductance \\(L\\), and capacitor with capacitance \\(C\\) all in series. Then, by KVL,</p> \\[V(t) = RI(t) + \\frac{Q(t)}{C} + L \\frac{dI}{dt}\\] <p>Then, with \\(I(t) = I_0 \\cos(\\omega t + \\phi)\\), we can see that with inductive reactance \\(X_L = \\omega L\\) and capactive reactance \\(X_C = (\\omega C)^{-1}\\),</p> \\[I_0 = \\frac{V_0}{Z}; \\sin \\phi = \\frac{X_C - X_L}{Z}; \\cos \\phi = \\frac{R}{Z}; Z = \\sqrt{R^2 + (X_C - X_L)^2}\\] <p>Definition. The quantity \\(Z\\) is the impedence of the circuit.</p> <p>Definition. The resonance frequency is the frequency at which \\(X_C = X_L\\). This occurs only at \\(\\omega_0 = 1 / \\sqrt{LC}\\). Notably, the impedence equals the resistance (\\(Z = R\\)) and the current is in-phase with voltage (\\(\\phi = 0\\)).</p> <p>The instantaneous power delivered by the circuit is</p> \\[P(t) = V(t)I(t) = V_0 \\cos(\\omega t) \\frac{V_0}{Z(\\omega)}\\cos(\\omega t + \\phi)\\] <p>Averaged over one cycle (\\(T = 2\\pi / \\omega\\)),</p> \\[\\langle P(t) \\rangle = \\frac{V_0^2}{Z(\\omega)} \\frac{1}{T} \\int_0^T \\cos(\\omega t) [ \\cos(\\omega t)\\cos\\phi - \\sin(\\omega t)\\sin\\phi] dt = \\frac{V_0^2}{2Z(\\omega)}\\cos\\phi\\] <p>Note that \\(\\cos\\phi\\) = \\(R/Z\\), so</p> \\[\\langle P(t) \\rangle = \\frac{1}{2} \\frac{V_0^2R}{Z^2(\\omega)}\\] <p>Recall that the charactaristic times for a capacitor and inductor to charge are \\(T_C = RC\\) and \\(T_L = L/R\\). Then,</p> \\[\\langle P(t) \\rangle = \\frac{1}{2} \\frac{V_0^2}{r} \\frac{1}{1 + (\\omega T_l - (\\omega T_c)^{-1})^2}\\] <p>This has a clear maximum of \\(\\frac{1}{2} \\frac{V_0^2}{R}\\) for the frequency \\(\\omega T_L = \\frac{1}{\\omega T_C}\\), when \\(\\omega_0 = 1/\\sqrt{LC}\\).</p> <p>The width of the resonance, or the range of angular frequencies over which \\(\\frac{R^2}{Z^2(\\omega)}\\) is greater than half its peak value, is given by \\(\\Delta \\omega = \\frac{1}{T_L} = \\frac{L}{R} = \\omega_0^2 T_C\\).</p> <p>Note that all power dissipation happens over the resistor, not the inductor or capacitor.</p> <p>Now, consider a circuit in which the resistor, inductor, and capacitor are all in parallel. By KVL,</p> \\[V_0 \\cos(\\omega t) = RI_R(t) = L \\frac{dI_L(t)}{dt} = \\frac{Q_C(t)}{C}\\] <p>With currents given as \\(I_R(t) = I_{R0} \\cos(\\omega t + \\phi_R)\\), \\(I_L(t) = I_{L0} \\cos(\\omega t + \\phi_L)\\), and \\(I_C(t) = I_{C0} \\cos(\\omega t + \\phi_C)\\), we obtain</p> \\[ \\begin{array}{ccc}     I_{R0} = \\frac{V_0}{R} &amp; I_{L0} = \\frac{V_0}{\\omega L} &amp; I_{C0} = V_0 \\omega C \\\\     \\phi_R = 0 &amp; \\phi_L = -\\pi / 2 &amp; \\phi_C = \\pi/2 \\end{array} \\] <p>The total current them becomes \\(I(t) = \\frac{V_0}{R} \\cos(\\omega t) + V_0(\\frac{1}{X_L} - \\frac{1}{X_C})\\sin(\\omega t)\\). Additionally, \\(\\langle P(t) \\rangle = \\frac{1}{2} \\frac{V_0^2}{R}\\).</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-822-applications","title":"Section 8.2.2 - Applications","text":"<p>The voltage across the capacitor is higher than the voltage across the capacitor for frequencies less than the resonance frequency. The inverse holds for frequencies greater than the resonance frequency. This can be proved by computing \\(V_C / V_0 = R / Z(\\omega)\\) and \\(V_L / V_0 = \\omega L / Z(\\omega)\\).</p> <p>To eliminate high frequencies, take the voltage across the capacitor. Otherwise, take the voltage across the resistor.</p> <p>These circuits can also be used in tuners by taking the voltage across the resistorr.</p>"},{"location":"physics/electrodynamics/8-quasi-static-electrodynamics-ac-circuitts/#section-823-energy-in-inductors-and-capacitors","title":"Section 8.2.3 - Energy in Inductors and Capacitors","text":"<p>We know the energy in an electromagnetic field is</p> \\[u = \\frac{1}{2}(\\varepsilon_0 e^2 + \\mu_u H^2)\\] <p>In a capacitor, we cannot directly state the energy. However, we can state its rate of change:</p> \\[\\frac{du_E}{dt} = \\mathbf{E} \\cdot \\frac{d\\mathbf{D}}{dt}\\] <p>In the case where \\(\\mathbf{D} = \\varepsilon \\mathbf{E}\\), that is, in the presence of a simple dielectric,</p> \\[u_E = \\frac{1}{2} \\varepsilon E^2 = \\frac{1}{2}\\mathbf{E} \\cdot \\mathbf{D}\\] <p>This is mirrored in solenoids / inductors, where</p> \\[\\frac{du_M}{dt} = \\mathbf{H} \\cdot \\frac{d\\mathbf{B}}{dt}\\] <p>In the case where \\(\\mathbf{B} = \\mu \\mathbf{H}\\), that is, in the presence of a simple magnetic material,</p> \\[u_M = \\frac{1}{2}\\mathbf{H} \\cdot \\mathbf{B}\\] <p>Lastly, this continues for electromagnetic fields.</p> \\[\\frac{du_{EM}}{dt} = \\mathbf{E} \\cdot \\frac{d\\mathbf{D}}{dt} + \\mathbf{H} \\cdot \\frac{d\\mathbf{B}}{dt}\\] <p>In simple electromagnetic materials, we see that</p> \\[u_{EM} = \\frac{1}{2}(\\mathbf{E} \\cdot \\mathbf{D} + \\mathbf{H} \\cdot \\mathbf{B}) = \\frac{1}{2}(\\varepsilon E^2 + \\mu H^2)\\]"},{"location":"physics/electrodynamics/9-conservation-laws/","title":"Chapter 9 - Conservation Laws","text":""},{"location":"physics/electrodynamics/9-conservation-laws/#section-91-continuity-equation-as-a-model-conservation-law","title":"Section 9.1 - Continuity Equation as a Model Conservation Law","text":"<p>The laws of physics exhibit temporal, spatial, and angular symmetry. That is, the laws of physics do not change dependent on the time, position, and rotation of the observer. These symmetries lead to conservation of energy, momentum, and angular momentum respectively.</p> <p>The conservation of chargee is another symmetry-based conservation law, derived from \"gauge invariance\". In this section, this is conservation law is assumed to be valid.</p> <p>Recall the continuity equation, that is, \\(\\frac{\\partial \\rho(\\mathbf{r})}{\\partial t} = - \\nabla \\cdot \\mathbf{J}(\\mathbf{r})\\). That is, the charge density at any point in space is equal to to the divergence of the current. From this, we can integrate to find \\(Q(t) = \\int_V \\rho(\\mathbf{r}, t) dV\\), and \\(\\frac{dQ}{dt} = -\\int_{\\partial V} \\mathbf{J}(\\mathbf{r}, t) \\cdot \\hat{\\mathbf{n}} dS\\). This is a local conservation law, because it does not address situations in which charge decreases in one region and increases in another without the flow of current.</p>"},{"location":"physics/electrodynamics/9-conservation-laws/#section-92-conservation-of-electomagnetic-energy","title":"Section 9.2 - Conservation of Electomagnetic Energy","text":"<p>Consider a volume \\(V\\) with surface \\(\\partial V\\), that encloses some magnetic and electric point charges. Then, for any electric charge \\(q_{ei}\\) or magnetic charge \\(q_{mj}\\), the force on each charge is</p> \\[\\begin{align}     \\mathbf{F}_i &amp;= q_{ei} (\\mathbf{E}_i + \\mathbf{v}_i \\times \\mathbf{B}_i) \\\\     \\mathbf{F}_j &amp;= q_{mi} (\\mathbf{H}_j - \\mathbf{v}_j \\times \\mathbf{D}_j) \\end{align}\\] <p>We also know that the rate at which energy changes due to changing fields is \\(\\frac{dw_i}{dt} = \\mathbf{F}_i \\cdot \\mathbf{v}_i\\) and \\(\\frac{dw_m}{dt} = \\mathbf{F}_j \\cdot \\mathbf{v}_j\\). This allows us to conclude that at any point \\(\\mathbf{r}\\) inside the volume, the rate at which the mechanical energy density changes is</p> \\[\\frac{du_{mech}}{dt} = \\sum_i \\delta(\\mathbf{r} - \\mathbf{r}_i) \\mathbf{F}_i \\cdot \\mathbf{v}_i + \\delta(\\mathbf{r} - \\mathbf{r}_j) \\mathbf{F}_j \\cdot \\mathbf{v}_j\\] <p>Since \\((\\mathbf{v}_i \\times \\mathbf{B}_i) \\cdot \\mathbf{v}_i = 0\\) and \\((\\mathbf{v}_j \\times \\mathbf{D}_j) \\cdot \\mathbf{v}_j = 0\\), and the current densities are given as \\(\\mathbf{J}_e(\\mathbf{r}) = \\sum_i \\mathbf{v}_i q_{ei}\\delta(\\mathbf{r} - \\mathbf{r}_i)\\) and \\(\\mathbf{J}_m(\\mathbf{r}) = \\sum_j \\mathbf{v}_j q_{mj}\\delta(\\mathbf{r} - \\mathbf{r}_j)\\), we can rewrite this as</p> \\[\\frac{du_{mech}}{dt} = \\mathbf{J}_e(\\mathbf{r}) \\cdot \\mathbf{E}(\\mathbf{r}) + \\mathbf{J}_m(\\mathbf{r}) \\cdot \\mathbf{H}(\\mathbf{r})\\] <p>Combine this with the Maxwell equations to remove current densities, we see that</p> \\[\\frac{du_{mech}}{dt} = (\\nabla \\times \\mathbf{H} - \\frac{\\partial \\mathbf{H}}{\\partial t}) \\cdot \\mathbf{E} + (-\\nabla \\times \\mathbf{E} - \\frac{\\partial \\mathbf{B}}{\\partial t}) \\cdot \\mathbf{H}\\] <p>With a vector identity, this simplifies to</p> \\[\\frac{du_{mech}}{dt} = -\\nabla \\times (\\mathbf{E} \\times \\mathbf{H})\\] <p>Definition. We call \\(\\mathbf{S} = \\mathbf{E} \\times \\mathbf{H}\\) the Poynting vector.</p> <p>With this vector, we can define</p> \\[\\frac{du_{mech}}{dt} = -\\nabla \\times \\mathbf{S}\\]"},{"location":"physics/electrodynamics/9-conservation-laws/#section-922-energy-density-for-linear-materials","title":"Section 9.2.2 - Energy Density for Linear Materials","text":"<p>For a simple material, that is, one in which \\(\\mathbf{D} = \\varepsilon \\mathbf{E}\\) and \\(\\mathbf{B} = \\mu \\mathbf{H}\\), we can express the rate of change of electomagnetic energy \\(\\frac{\\partial u_{em}}{\\partial t}\\) as \\(\\frac{\\partial u_{em}}{\\partial t} = \\frac{1}{2} \\frac{\\partial}{\\partial t} (\\mathbf{E} \\cdot \\mathbf{D} + \\mathbf{H} \\cdot \\mathbf{B})\\). After integration, we can see that \\(u_{em} = \\frac{1}{2}(\\varepsilon E^2 + \\mu H^2)\\).</p> <p>Definition. A material is said to be an anisotropic linear material if \\(\\mathbf{D} = \\overleftrightarrow{\\mathbf{\\varepsilon}} \\cdot \\mathbf{E}\\) and \\(\\mathbf{B} = \\overleftrightarrow{\\mathbf{\\mu}} \\cdot \\mathbf{H}\\). Then, the internal electromagnetic energy becomes</p> \\[u_{em} = \\frac{1}{2}(\\mathbf{E} \\cdot \\overleftrightarrow{\\mathbf{\\varepsilon}} \\cdot \\mathbf{E}+ \\mathbf{H} + \\overleftrightarrow{\\mathbf{\\mu}} \\cdot \\mathbf{H}) = \\frac{1}{2}\\sum_{i,j}(\\varepsilon_{ij}E_iE_j + \\mu_{ij}H_iH_j)\\] <p>If the dyadics are symmetric, the energy functions uniquely specify the energy in terms of fields. In general, the polarization or magnetization of a material may depend on its past history and on time, and as such, the energy density for such materials cannot be expressed entirely in terms of the fields.</p>"},{"location":"physics/electrodynamics/9-conservation-laws/#section-923-poynting-vector-examples","title":"Section 9.2.3 - Poynting Vector Examples","text":"<p>Consider a long coaxial cable, bridged by a constant voltage \\(V\\) on one side and a resistor \\(R\\) on the other. Then, we know that between the conductors, \\(\\mathbf{H} = \\frac{R}{2\\pi s R}\\mathbf{\\varphi}\\). Additionally, we know that</p> \\[V=\\int_a^b \\mathbf{E} \\cdot d\\mathbf{l} = \\frac{Q}{2\\pi \\ell \\varepsilon_0} \\ln(\\frac{b}{a}) \\Rightarrow \\frac{Q}{\\ell} = \\frac{2\\pi\\varepsilon_0}{\\ln(\\frac{b}{a})} V\\] <p>This then implies that \\(\\mathbf{E} = \\frac{V}{\\ln(\\frac{b}{a})s} \\hat{\\mathbf{s}}\\). We can then solve for both the energy density and Poynting vector, as well as \\(\\mathbf{v} = \\mathbf{S} / u\\), the speed at which energy moves through the cable. With the impedance for a coaxial cable \\(Z_{C0} = \\sqrt{\\frac{\\mu_0}{\\varepsilon_0}} \\frac{\\ln(\\frac{b}{a})}{2\\pi}\\), we see that</p> \\[\\mathbf{v} = \\frac{2c \\hat{\\mathbf{z}}}{\\frac{R}{Z_{C0}} + \\frac{Z_{C0}}{R}}\\] <p>Consider a long cylindrical ohmic wire of radius \\(a\\), length \\(L\\), and resistivity \\(\\rho\\) along the \\(z\\)-axis. If this wire is carrying a constant current \\(I\\), we know that inside the wire, \\(E_z = \\rho J = \\rho \\frac{I}{\\pi a^2}\\). From Ampere's Law, \\(H_\\phi = \\frac{s}{2\\pi a^2} I\\). Then, inside the wire,</p> \\[\\mathbf{S} = -s \\frac{\\rho I^2}{2 \\pi^2 a^4} \\hat{\\mathbf{s}}\\] <p>Outside the wire, we know that \\(\\mathbf{E} = \\frac{\\rho I}{\\pi a^2} \\hat{\\mathbf{z}}\\) and \\(\\mathbf{H} = \\frac{I}{2\\pi s}\\hat{\\mathbf{\\varphi}}\\), so</p> \\[\\mathbf{S} = -\\frac{\\rho I}{\\pi a^2} \\frac{I}{2\\pi s} \\hat{\\mathbf{s}}\\] <p>Notably, inside the wire, \\(\\nabla \\cdot \\mathbf{S} = \\frac{\\rho I^2}{\\pi^2 a^4}\\), but is equal to \\(0\\) outside of the wire.</p> <p>Further examples are present but omitted.</p>"},{"location":"physics/electrodynamics/9-conservation-laws/#section-93-conservation-of-momentum-and-maxwell-stress-tensor","title":"Section 9.3 - Conservation of Momentum and Maxwell Stress Tensor","text":"<p>We know that the rate of change of momentum for any given particle is simply the force acting on it. To calculate this, recall the force density:</p> \\[\\mathbf{f} = \\sum_i \\mathbf{F}_i\\delta(\\mathbf{r}-\\mathbf{r}_i) + \\sum_j \\mathbf{F}_j\\delta(\\mathbf{r}-\\mathbf{r}_j) = \\sum_i q_{ei}\\delta(\\mathbf{r}-\\mathbf{r}_i)(\\mathbf{E} + \\mathbf{v}_i \\times \\mathbf{B}) + \\sum_j q_{ej} \\delta(\\mathbf{r}-\\mathbf{r}_i) (\\mathbf{H} - \\mathbf{v}_j \\times \\mathbf{D})\\] <p>Converting to currents, we see that</p> \\[\\mathbf{f}(\\mathbf{r}) = \\rho_e(\\mathbf{r})\\mathbf{E}(\\mathbf{r}) + \\mathbf{J}_e(\\mathbf{r}) \\times \\mathbf{B}(\\mathbf{r}) + \\rho_m(\\mathbf{r}) + \\mathbf{H}(\\mathbf{r}) - \\mathbf{J}_m(\\mathbf{r}) \\times \\mathbf{D}(\\mathbf{r})\\] <p>Substituting in Maxwell's Equations, we see that</p> \\[\\mathbf{f}(\\mathbf{r}) + \\frac{\\partial}{\\partial t}(\\mathbf{D} \\times \\mathbf{B}) = \\varepsilon_0 (\\nabla \\cdot \\mathbf{E})\\mathbf{E} + (\\nabla \\times \\mathbf{E})\\times\\mathbf{D} + \\mu_0(\\nabla \\cdot \\mathbf{H})\\mathbf{H} + (\\nabla \\times \\mathbf{H})\\times\\mathbf{B}\\] <p>Now, we claim that the right-hand side is the divergence of some tensor \\(\\overleftrightarrow{\\mathbf{T}}\\), so that</p> \\[\\mathbf{f}(\\mathbf{r}) + \\frac{\\partial}{\\partial t}(\\mathbf{D} \\times \\mathbf{B}) = \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}\\] <p>This tensor is the Maxwell Stress Tensor. We claim that the divergence of this tensor is composed of both an electric and magnetic part, so that \\(\\nabla \\cdot \\overleftrightarrow{\\mathbf{T}} = \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}_e + \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}_m\\). Then, we  can state</p> \\[\\begin{align} \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}_e &amp;= \\varepsilon_0 [(\\nabla \\cdot \\mathbf{E})\\mathbf{E} + (\\nabla \\times \\mathbf{E})\\times \\mathbf{E}] \\\\ \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}_m &amp;= \\varepsilon_0 [(\\nabla \\cdot \\mathbf{H})\\mathbf{H} + (\\nabla \\times \\mathbf{H})\\times \\mathbf{H}] \\end{align}\\] <p>We know that \\(\\nabla \\cdot(\\mathbf{EE}) = (\\nabla \\cdot \\mathbf{E})\\mathbf{E} + (\\mathbf{E} \\cdot \\nabla)\\mathbf{E}\\) and \\(\\nabla \\cdot(\\overleftrightarrow{\\mathbf{I}}f) = \\nabla f\\). If we let \\(f = \\frac{1}{2}\\mathbf{E} \\cdot \\mathbf{E}\\), we see that \\(\\nabla(\\frac{1}{2}\\mathbf{E} \\cdot \\mathbf{E}) = (\\mathbf{E} \\cdot \\nabla)\\mathbf{E} + (\\nabla \\times \\mathbf{E})\\mathbf{E}\\). Then, we see that</p> \\[\\begin{align} \\overleftrightarrow{\\mathbf{T}}_e &amp;= \\varepsilon_0 \\mathbf{EE} - \\frac{\\varepsilon_0}{2} \\overleftrightarrow{\\mathbf{I}}(\\mathbf{E} \\cdot \\mathbf{E}) \\\\ \\overleftrightarrow{\\mathbf{T}}_m &amp;= \\mu_0 \\mathbf{HH} - \\frac{\\mu_0}{2} \\overleftrightarrow{\\mathbf{I}}(\\mathbf{H} \\cdot \\mathbf{H}) \\end{align}\\] <p>Knowing that \\(\\overleftrightarrow{\\mathbf{T}} = \\overleftrightarrow{\\mathbf{T}}_e + \\overleftrightarrow{\\mathbf{T}}_m\\), and that \\(u = \\frac{1}{2}(\\varepsilon_0 E^2 + \\mu_0 H^2)\\) is the energy density of the electromagnetic fields in a vacuum,</p> \\[\\overleftrightarrow{\\mathbf{T}} = \\varepsilon_0 \\mathbf{EE} + \\mu_0 \\mathbf{HH} - \\overleftrightarrow{\\mathbf{I}}u\\] <p>Additionally, we denote the time rate of change of the momentum density of the electromagnetic fields as \\(\\mathbf{g}(\\mathbf{r}) = \\mathbf{D}(\\mathbf{r}) \\times \\mathbf{B}(\\mathbf{r})\\). Thus,</p> \\[\\mathbf{f}(\\mathbf{r}) = \\frac{\\partial}{\\partial t}\\mathbf{g} = \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}\\] <p>The Divergence Theorem states that \\(\\int_V(\\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}) dV = \\int_{SofV} dS \\hat{\\mathbf{n}} \\cdot \\overleftrightarrow{\\mathbf{T}}\\). We can prove this by expanding the left-hand side over a cube. Note that as \\(\\overleftrightarrow{\\mathbf{T}}\\) is symmetric, \\(\\hat{\\mathbf{n}} \\cdot \\overleftrightarrow{\\mathbf{T}} = \\overleftrightarrow{\\mathbf{T}} \\cdot \\hat{\\mathbf{n}}\\).</p> <p>Given a static field, the momentum does not change in time. That is,</p> \\[\\frac{\\partial}{\\partial t} \\mathbf{g} = \\frac{\\partial}{\\partial t}(\\mathbf{D} \\times \\mathbf{B}) = 0\\] <p>Then, we can see that \\(\\mathbf{f}(\\mathbf{r}) = \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}}\\). We can thus integrate over the volume to find force on an object.</p> \\[\\mathbf{F} = \\int_V \\mathbf{f}(\\mathbf{r}) dV = \\int_V \\nabla \\cdot \\overleftrightarrow{\\mathbf{T}} dV = \\int_{\\partial V} dS \\hat{\\mathbf{n}} \\cdot \\overleftrightarrow{\\mathbf{T}} = \\int_{\\partial V} \\overleftrightarrow{\\mathbf{T}} \\cdot \\hat{\\mathbf{n}} dS\\]"},{"location":"physics/electrostatics/1-math/","title":"Chapter 1 - Mathematics","text":""},{"location":"physics/electrostatics/1-math/#15-dyads-and-tensors","title":"1.5 - Dyads and Tensors","text":"<p>Definition. A dyadic is a representation of two-ish vectors.</p> \\[ \\stackrel{\\leftrightarrow}{\\mathbf{D}} = \\begin{matrix}     D_{xx} \\hat{\\mathbf{x}}\\hat{\\mathbf{x}} &amp;+ D_{xy} \\hat{\\mathbf{x}}\\hat{\\mathbf{y}} &amp;+ D{xz} \\hat{\\mathbf{x}}\\hat{\\mathbf{z}} \\\\     + D_{yx} \\hat{\\mathbf{y}}\\hat{\\mathbf{x}} &amp;+ D_{yy} \\hat{\\mathbf{y}}\\hat{\\mathbf{y}} &amp;+ D{yz} \\hat{\\mathbf{y}}\\hat{\\mathbf{z}} \\\\     + D_{zx} \\hat{\\mathbf{z}}\\hat{\\mathbf{x}} &amp;+ D_{zy} \\hat{\\mathbf{z}}\\hat{\\mathbf{y}} &amp;+ D{zz} \\hat{\\mathbf{z}}\\hat{\\mathbf{z}} \\end{matrix} \\] <p>Definition. If a dyadic can be written as a composition of two vectors \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), it is called a dyad.</p> \\[ \\mathbf{AB} = \\begin{matrix}     A_x B_x \\hat{\\mathbf{x}}\\hat{\\mathbf{x}} &amp;+ A_x B_y \\hat{\\mathbf{x}}\\hat{\\mathbf{y}} &amp;+ A_x B_z \\hat{\\mathbf{x}}\\hat{\\mathbf{z}} \\\\     + A_y B_x \\hat{\\mathbf{y}}\\hat{\\mathbf{x}} &amp;+ A_y B_y \\hat{\\mathbf{y}}\\hat{\\mathbf{y}} &amp;+ A_y B_z \\hat{\\mathbf{y}}\\hat{\\mathbf{z}} \\\\     + A_z B_x \\hat{\\mathbf{z}}\\hat{\\mathbf{x}} &amp;+ A_z B_y \\hat{\\mathbf{z}}\\hat{\\mathbf{y}} &amp;+ A_z B_z \\hat{\\mathbf{z}}\\hat{\\mathbf{z}} \\end{matrix} \\] <p>The dot product of a dyad \\(\\stackrel{\\leftrightarrow}{\\mathbf{D}} = \\mathbf{AB}\\) and vector \\(\\mathbf{v}\\) can be written as follows:</p> \\[ (\\mathbf{AB}) \\cdot \\mathbf{v} = \\mathbf{A} (\\mathbf{B} \\cdot \\mathbf{v}) \\] <p>Definition. A symmetric/antisymmetric dyadic is defined the same way that a matrix is.</p> <p>Definition. The identity dyadic is \\(\\stackrel{\\leftrightarrow}{\\mathbf{I}} = \\hat{\\mathbf{x}}\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}\\hat{\\mathbf{y}} + \\hat{\\mathbf{z}}\\hat{\\mathbf{z}}\\).</p> <p>Definition. FOr a tensor, with coordinates \\(u^i\\), we have two sets of basis vectors:</p> \\[ \\mathbf{e}_i = \\pdv{\\mathbf{r}}{u^i} \\] \\[ \\mathbf{e}^i = \\nabla{u^i} \\]"},{"location":"physics/electrostatics/1-math/#19-helmholtz-theorem","title":"1.9 - Helmholtz Theorem","text":"<p>Given an arbitrary vector field \\(\\mathbf{F}(\\mathbf(r))\\), we can write said field as a composition of a curl-free component \\(\\mathbf{\\Phi}(\\mathbf{r})\\) and a divergence-free component \\(\\mathbf{A}(\\mathbf{r})\\) as follows:</p> \\[ \\mathbf{F}(\\mathbf{r}) = - \\nabla{\\mathbf{\\Phi}(\\mathbf{r})} + \\nabla \\times{\\mathbf{A}(\\mathbf{r})} \\] <p>Definition. Here, the gradient of the scalar potential is \\(\\nabla{\\mathbf{\\Phi}(\\mathbf{r})}\\) and the curl of the vector potential is \\(\\nabla \\times{\\mathbf{A}(\\mathbf{r})}\\). Thus, the scalar potential is \\(\\mathbf{\\Phi}(\\mathbf{r})\\) and the vector potential is \\(\\mathbf{A}(\\mathbf{r})\\).</p> <p>Letting said field be over bounded volume \\(V\\) with closed surface \\(\\partial V\\), and the functions \\(\\mathbf{C}(\\mathbf{r}) = \\nabla \\times{\\mathbf{F}(\\mathbf{r})}\\) and \\(\\mathbf{D}(\\mathbf{r}) = \\nabla \\cdot \\mathbf{F}(\\mathbf{r})\\) are known, we can say that</p> \\[ \\mathbf{\\Phi}(\\mathbf{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{D(\\mathbf{r}')}{|{\\mathbf{r}-\\mathbf{r}'}|} d{V'} - \\frac{1}{4 \\pi} \\int_{\\partial V} \\frac{\\mathbf{F}(\\mathbf{r}') \\cdot \\mathbf{n}'}{|{\\mathbf{r}-\\mathbf{r}'}|} d{S'} \\] \\[ \\mathbf{A}(\\mathbf{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{C(\\mathbf{r}')}{|{\\mathbf{r}-\\mathbf{r}'}|} d{V'} - \\frac{1}{4 \\pi} \\int_{\\partial V} \\mathbf{n}' \\times \\frac{\\mathbf{F}(\\mathbf{r}')}{|{\\mathbf{r}-\\mathbf{r}'}|} d{S'} \\] <p>Now, assume that \\(\\lim(\\frac{\\mathbf{F}(\\mathbf{r})}{\\mathbf{r}}) = 0\\) as \\(\\mathbf{r} \\rightarrow \\infty\\), with a large enough volume, we see that the second terms vanish.</p> \\[ \\mathbf{\\Phi}(\\mathbf{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{D(\\mathbf{r}')}{|{\\mathbf{r}-\\mathbf{r}'}|} d{V'} \\] \\[ \\mathbf{A}(\\mathbf{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{C(\\mathbf{r}')}{|{\\mathbf{r}-\\mathbf{r}'}|} d{V'} \\]"},{"location":"physics/electrostatics/2-coulomb/","title":"Chapter 2 - Coulomb's Laws, Electric and Magnetic Fields","text":""},{"location":"physics/electrostatics/2-coulomb/#section-22-parallel-treatment-of-electric-and-magnetic-fields","title":"Section 2.2 - Parallel Treatment of Electric and Magnetic Fields","text":"<p>Consider two point charges, \\(q\\) and \\(Q\\), with the latter being at the origin of the coordinate system. Let \\(q\\) be located at point \\(\\mathbf{r}\\) relative to the origin.</p> <p>Thus, according to Coulomb's Law,</p> \\[ \\begin{align}     F^e_{qQ}(\\mathbf{r}) &amp;= \\frac{q_e Q_e}{4 \\pi \\varepsilon_0} \\frac{\\hat{\\mathbf{r}}}{|\\mathbf{r}|^2} \\\\     F^m_{qQ}(\\mathbf{r}) &amp;= \\frac{q_m Q_m}{4 \\pi \\mu_0} \\frac{\\hat{\\mathbf{r}}}{|\\mathbf{r}|^2} \\end{align} \\] <p>Divide by the charge \\(q\\) to obtain the electric or magnetic field at point \\(\\mathbf{r}\\).</p> \\[ \\begin{align}     E(\\mathbf{r}) &amp;= \\frac{Q_e}{4 \\pi \\varepsilon_0} \\frac{\\hat{\\mathbf{r}}}{|\\mathbf{r}|^2} \\\\     H(\\mathbf{r}) &amp;= \\frac{Q_m}{4 \\pi \\mu_0} \\frac{\\hat{\\mathbf{r}}}{|\\mathbf{r}|^2} \\end{align} \\] <p>Now, let \\(Q\\) be at point \\(\\mathbf{r'}\\). Then, the unit vector becomes \\(\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}}|\\), and we see the following.</p> \\[ \\begin{align}     E(\\mathbf{r}) &amp;= \\frac{Q_e}{4 \\pi \\varepsilon_0} \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\\\     H(\\mathbf{r}) &amp;= \\frac{Q_m}{4 \\pi \\mu_0} \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\end{align} \\] <p>With multiple charges, we can apply the superposition principal to see the following:</p> \\[ \\begin{align}     E(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\varepsilon_0} \\sum_{i=1}^N Q_e \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\\\     H(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\mu_0} \\sum_{i=1}^N Q_m \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\end{align} \\] <p>We can convert this to an integral as \\(N\\) goes to infinity.</p> \\[\\begin{align} E(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\varepsilon_0} \\int_V \\rho_e(\\mathbf{r'}) \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} d V' \\\\ H(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\mathbf{r'}) \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} d V' \\end{align}\\]"},{"location":"physics/electrostatics/2-coulomb/#section-23-divergence-and-curl-of-the-electrostatic-or-magnetostatic-field","title":"Section 2.3 - Divergence and Curl of the Electrostatic or Magnetostatic Field","text":"<p>From a lot of advanced math, we know that</p> \\[\\nabla \\cdot \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} = 4 \\pi \\delta(\\mathbf{r}-\\mathbf{r'})\\] <p>Now, apply the divergence operator over \\(\\mathbf{r}\\) to the electrostatic and magnetostatic fields.</p> \\[\\begin{align} \\nabla \\cdot E(\\mathbf{r}) &amp;= \\nabla \\cdot (\\frac{1}{4 \\pi \\varepsilon_0} \\int_V \\rho_e(\\mathbf{r'}) \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} d V') \\\\ \\nabla \\cdot H(\\mathbf{r}) &amp;= \\nabla \\cdot (\\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\mathbf{r'}) \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} d V') \\end{align}\\] <p>As the divergence operator does not operate on \\(\\mathbf{r'}\\), we see that</p> \\[\\begin{align} \\nabla \\cdot E(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\varepsilon_0} \\int_V \\rho_e(\\mathbf{r'}) \\nabla \\cdot (\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3}) d V' \\\\     &amp;= \\frac{1}{4 \\pi \\varepsilon_0} 4 \\pi \\int_V \\rho_e(\\mathbf{r'}) \\delta(\\mathbf{r}-\\mathbf{r'}) d V' \\\\     &amp;= \\frac{\\rho_e(\\mathbf{r})}{\\varepsilon_0} \\\\ \\nabla \\cdot H(\\mathbf{r}) &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\mathbf{r'}) \\nabla \\cdot (\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3}) d V' \\\\     &amp;= \\frac{1}{4 \\pi \\mu_0} 4 \\pi \\int_V \\rho_m(\\mathbf{r'}) \\delta(\\mathbf{r}-\\mathbf{r'}) d V' \\\\     &amp;= \\frac{\\rho_m(\\mathbf{r})}{\\mu_0} \\end{align}\\] <p>The curl of an electrostatic or magnetostatic is relatively simple.</p> \\[ \\begin{align}     \\nabla \\times{E(\\mathbf{r})} &amp;= \\frac{1}{4 \\pi \\varepsilon_0} \\int_V \\rho_e(\\mathbf{r'}) \\nabla \\times{(\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3})} d V' \\\\     \\nabla \\times{H(\\mathbf{r})} &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\mathbf{r'}) \\nabla \\times{(\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3})} d V' \\\\ \\end{align} \\] <p>Additionally, we know \\(\\nabla \\times{f\\mathbf{A}} = f \\nabla \\times{\\mathbf{A}} + \\nabla{f}\\times\\mathbf{A}\\). Thus,</p> \\[ \\begin{align}     \\nabla \\times{(\\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3})} &amp;= \\frac{1}{|\\mathbf{r}-\\mathbf{r'}|^3} \\nabla \\times{(\\mathbf{r}-\\mathbf{r'})} + (\\nabla \\times{\\frac{1}{|\\mathbf{r}-\\mathbf{r'}|^3}}) \\times (\\mathbf{r}-\\mathbf{r'}) \\\\ \\end{align} \\] <p>We can verify that \\(\\nabla \\times{(\\mathbf{r}-\\mathbf{r'})} = 0\\), cancelling the first term. Additionally, \\(\\nabla \\times{\\frac{1}{|\\mathbf{r}-\\mathbf{r'}|^3}} = -3 \\frac{\\mathbf{r}-\\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^5}\\), which when crossed with \\(\\mathbf{r}-\\mathbf{r'}\\), will cancel. Thus, all terms in the curl cancel, so for a static field, the curl is zero.</p>"},{"location":"physics/electrostatics/2-coulomb/#section-24-electric-and-magnetic-flux-densities","title":"Section 2.4 - Electric and Magnetic Flux Densities","text":"<p>The electric and magnetic flux density vectors are given by \\(\\varepsilon_0 \\mathbf{E}\\) and \\(\\mu_0 \\mathbf{H}\\).</p> <p>Now, given \\(S\\) is a surface enclosing \\(Q_e\\) or \\(Q_m\\) total charge, we denote flux as following:</p> \\[ \\Phi_e = \\varepsilon_0 \\int_S \\mathbf{E} \\cdot \\hat{\\mathbf{n}} d = Q_e S \\text{ or } \\Phi_m = \\mu_0 \\int_S \\mathbf{H} \\cdot \\hat{\\mathbf{n}} d S = Q_m \\] <p>Thus, applying divergence theorem,</p> \\[ Q_e = \\Phi_e = \\varepsilon_0 \\int_S \\mathbf{E} \\cdot \\hat{\\mathbf{n}} d = \\varepsilon_0 \\int_V \\nabla \\cdot \\mathbf{E} d V \\] \\[ Q_m = \\Phi_m = \\mu_0 \\int_S \\mathbf{H} \\cdot \\hat{\\mathbf{n}} d = \\varepsilon_0 \\int_V \\nabla \\cdot \\mathbf{H} d V \\] <p>Since \\(Q_e = \\int_V \\rho_e d V\\) and \\(Q_m = \\int_V \\rho_m d V\\), we see that</p> \\[ \\begin{align}     \\int_V \\rho_e d V &amp;= \\varepsilon_0 \\int_V \\nabla \\cdot \\mathbf{E} d V \\\\     \\rho_e &amp;= \\varepsilon_0 \\int_V \\nabla \\cdot \\mathbf{E} d V \\\\     \\int_V \\rho_m d V &amp;= \\mu_0 \\int_V \\nabla \\cdot \\mathbf{H} d V \\\\     \\rho_m &amp;= \\mu_0 \\int_V \\nabla \\cdot \\mathbf{H} d V \\\\ \\end{align} \\] <p>Definition. This is known as Gauss' Law.</p> <p>With applicable symmetry, the integral factor becomes simply \\(E(r)*A\\), where \\(A\\) is the area of the surface at \\(r\\).</p>"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/","title":"Chapter 3 - Electric and Magnetic Scalar Potentials","text":""},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-31-work-and-energy-in-electrostatics-and-magnetostatics","title":"Section 3.1 - Work and Energy in Electrostatics and Magnetostatics","text":"<p>The force on charge \\(q\\) is given by \\(\\mathbf{F}(\\mathbf{r}) = q_e \\mathbf{E}(\\mathbf{r})\\) or \\(\\mathbf{F}(\\mathbf{r}) = q_m \\mathbf{H}(\\mathbf{r})\\). If this charge is moved \\(d{\\mathbf{l}} = d x \\hat{\\mathbf{x}} + d y \\hat{\\mathbf{y}} + d z \\hat{\\mathbf{z}}\\), the change in internal energy (work) this produces can be written as</p> \\[ d{U}= - \\mathbf{F} \\cdot d{\\mathbf{l}} \\] <p>Rewriting this, \\(\\mathbf{F} = -\\nabla{U}\\), with \\(U\\) as potential energy. Now, we can denote this change in internal energy in terms of \\(q\\) as follows:</p> \\[ \\mathbf{E}(\\mathbf{r}) = \\frac{1}{q_e} \\mathbf{F_e}(\\mathbf{r}) = - \\frac{1}{q_e} \\nabla{U_e(\\mathbf{r})} = -\\nabla{V_e(\\mathbf{r})} \\] <p>The units of electrostatic potential is Joule/Coulomb, also known as a Volt. Thus, the units of the electric field should be expressed in Volts/meter. Similarly,</p> \\[ \\mathbf{H}(\\mathbf{r}) = \\frac{1}{q_m} \\mathbf{F_m}(\\mathbf{r}) = - \\frac{1}{q_m} \\nabla{U_m(\\mathbf{r})} = -\\nabla{V_m(\\mathbf{r})} \\] <p>The units of magnetostatic potential is Joule/Weber, also known as an Ampere. Thus, the units of the magnetic field can be written as Amperes/meter.</p> <p>With this, we can calculate work. Moving a charge \\(q\\) from \\(A\\) to \\(B\\), we see that</p> \\[ \\delta W = \\int_A^B \\mathbf{F} \\cdot d{\\mathbf{l}} = q_e \\int_A^B \\mathbf{E} \\cdot d{\\mathbf{l}} = -q_e \\int_A^B \\nabla{\\mathbf{V}} \\cdot d{\\mathbf{l}} = -q_e \\delta V_e \\] <p>Strictly speaking, this is a potential difference. To find the absolute potential, assume a point charge \\(Q\\) at the origin, and a charge \\(q\\). We take the work as \\(q\\) moves from \\(\\mathbf{r'} = \\mathbf{\\infty}\\) to \\(\\mathbf{r'} = \\mathbf{r}\\). Thus,</p> \\[ W = -q_e \\frac{Q_e}{4 \\pi \\varepsilon_0} \\int_{\\infty}^0 \\frac{\\hat{\\mathbf{r'}}}{r'^2} \\cdot (\\hat{\\mathbf{r'}}) d{r'} = -q_e \\frac{Q_e}{4 \\pi \\varepsilon_0} [\\frac{-1}{r'}]_{\\infty}^{r'} = q_e \\frac{Q_e}{4 \\pi \\varepsilon_0} \\frac{1}{r} \\] \\[ W = -q_m \\frac{Q_m}{4 \\pi \\mu_0} \\int_{\\infty}^0 \\frac{\\hat{\\mathbf{r'}}}{r'^2} \\cdot (\\hat{\\mathbf{r'}}) d{r'} = -q_m \\frac{Q_m}{4 \\pi \\mu_0} [\\frac{-1}{r'}]_{\\infty}^{r'} = q_m \\frac{Q_m}{4 \\pi \\mu_0} \\frac{1}{r} \\] <p>Letting the potential as \\(\\mathbf{r} \\rightarrow \\infty\\) equal \\(0\\) be our reference and dividing out \\(q\\), we find that the voltage for arrangement is the following:</p> \\[ V_e(\\mathbf{r}) = \\frac{Q_e}{4 \\pi \\varepsilon_0 r} \\text{ and } V_m(\\mathbf{r}) = \\frac{Q_m}{4 \\pi \\mu_0 r} \\] <p>Now, if we let the stationary charge \\(Q\\) be located at \\(\\mathbf{r'}\\), we see that</p> \\[ V_e(\\mathbf{r}) = \\frac{Q_e}{4 \\pi \\varepsilon_0}{|\\mathbf{r}-\\mathbf{r'}|} \\text{ and } V_m(\\mathbf{r}) = \\frac{Q_m}{4 \\pi \\mu_0}{|\\mathbf{r}-\\mathbf{r'}|} \\] <p>If we allow multiple charges, this becomes</p> \\[ V_e(\\mathbf{r}) = \\frac{1}{4\\pi \\varepsilon_0} \\sum_{i=1}^N \\frac{Q_{ei}}{|\\mathbf{r}-\\mathbf{r_i}|} \\] <p>Taking this to its natural limit,</p> \\[ V_e(\\mathbf{r}) = \\frac{1}{4 \\pi \\varepsilon_0} \\int_{V'} \\frac{\\rho_e(\\mathbf{r'})}{|\\mathbf{r}-\\mathbf{r'}}| d{V'} \\] \\[ V_m(\\mathbf{r}) = \\frac{1}{4 \\pi \\mu_0} \\int_{V'} \\frac{\\rho_m(\\mathbf{r'})}{|\\mathbf{r}-\\mathbf{r'}}| d{V'} \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-32-energy-of-a-charge-distribution","title":"Section 3.2 - Energy of a Charge Distribution","text":"<p>Given two point charges \\(Q_{e1}, Q_{e2}\\) we know the work to bring them together is</p> \\[ W_2 = W_{21} = \\frac{1}{4 \\pi \\varepsilon_0} \\frac{Q_{e1} Q_{e2}}{|\\mathbf{r_2} - \\mathbf{r_1}|} \\] <p>Superposition applies here. The energy to create \\(N\\) charges is</p> \\[ W_n = \\frac{1}{2} \\frac{4 \\pi \\varepsilon_0} \\sum_{i = 1}^{N} \\sum_{j &gt; i}^{N} \\frac{Q_{ei}Q_{ej}}{|\\mathbf{r_i}-\\mathbf{r_j}|} \\] <p>For the sake of symmetry, sum overall charges and divide by 2.</p> \\[ W_n = \\frac{1}{2} \\frac{1}{4 \\pi \\varepsilon_0} \\sum_{i = 1}^{N} \\sum_{j \\neq i}^{N} \\frac{Q_{ei}Q_{ej}}{|\\mathbf{r_i}-\\mathbf{r_j}|} \\] <p>Rearranging, we see the following:</p> \\[ W_n = \\frac{1}{2} \\sum_{i = 1}^{N}Q_{ei} \\sum_{i \\neq j}^{N} \\frac{1}{4 \\pi \\varepsilon_0} \\frac{Q_{ej}}{|\\mathbf{r_i}-\\mathbf{r_j}|} = \\frac{1}{2}\\sum_{i = 1}^{N} Q_{ei} V(\\mathbf{r_i}) \\] <p>We can rewrite this as a Riemann sum and convert to an integral.</p> \\[ W_e = \\frac{1}{2} \\int_V p_e(\\mathbf{r}) V_e(\\mathbf{r}) d V ; \\quad W_m = \\frac{1}{2} \\int_V p_m(\\mathbf{r}) V_m(\\mathbf{r}) d V \\] <p>We can also express this as</p> \\[ W_e = \\frac{1}{2} \\frac{1}{4 \\pi \\varepsilon_0} \\int_V \\int_{V'} \\frac{\\rho_e(\\mathbf{r})\\rho_e(\\mathbf{r'})}{|\\mathbf{r}-\\mathbf{r'}|} d{V'} d{V} \\] \\[ W_m = \\frac{1}{2} \\frac{1}{4 \\pi \\mu_0} \\int_V \\int_{V'} \\frac{\\rho_m(\\mathbf{r})\\rho_m(\\mathbf{r'})}{|\\mathbf{r}-\\mathbf{r'}|} d{V'} d{V} \\] <p>Note the \\(\\frac{1}{2}\\) is the same anti-double-counting factor introduced previously. If we were to determine the potential based on a different set of charges, the factor would be absent.</p> <p>We can now write an expression for energy of a charge density in terms of the field that it produces.</p> \\[ W = \\frac{\\varepsilon_0}{2} \\int_V (\\nabla \\cdot \\mathbf{E}(\\mathbf{r})) V(\\mathbf{r}) d V \\] <p>Simplifying, we see that</p> \\[ W_e = \\frac{\\varepsilon_0}{2} \\int_{V} E^2(\\mathbf{r}) d V ; \\quad W_m = \\frac{\\mu_0}{2} \\int_{V} H^2(\\mathbf{r}) d V \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-33-the-poisson-and-laplace-equations","title":"Section 3.3 - The Poisson and Laplace Equations","text":"<p>We know that \\(\\mathbf{E}(\\mathbf{r}) = -\\nabla \\cdot V_e(\\mathbf{r})\\) and \\(\\mathbf{H}(\\mathbf{r}) = -\\nabla \\cdot V_m(\\mathbf{r})\\)</p> <p>Combined this, as well as the first of the Maxwell equations, we see that</p> \\[ \\nabla \\cdot \\mathbf{E} = -\\nabla \\cdot \\nabla V_e = - \\nabla^2{V_e} = \\frac{\\rho_e}{\\varepsilon_0} \\] \\[ \\nabla \\cdot \\mathbf{H} = -\\nabla \\cdot \\nabla V_m = - \\nabla^2{V_m} = \\frac{\\rho_m}{\\mu_0} \\] <p>The last inequality is called the Poisson Equation, or the inhomogeneous Laplace equation.</p> <p>To solve this equation, we define a Green function as follows:</p> \\[ \\nabla^2 G(\\mathbf{r}, \\mathbf{r'}) = \\delta(\\mathbf{r} - \\mathbf{r'}) \\] <p>Now, we can construct a potential function in terms of said green function that satisfies the Laplace equation.</p> \\[ V_e(\\mathbf{r}) = - \\int_V G(\\mathbf{r}, \\mathbf{r'}) \\frac{\\rho_e(\\mathbf{r'})}{\\varepsilon_0} d{V'} \\] <p>This is the specific solution. Let \\(\\psi(\\mathbf{r})\\) be a solution to the homogenous equation. We can state the following:</p> \\[ V_e(\\mathbf{r}) = \\psi(\\mathbf{r}) - \\int_V G(\\mathbf{r}, \\mathbf{r'}) \\frac{\\rho_e(\\mathbf{r'})}{\\varepsilon_0} d{V'} \\] <p>We will consider the potential of a point charge. THat is, the limit of potential is zero as distance approaches infinity.</p> <p>Recall the potential of a point charge:</p> \\[ V_e(\\mathbf{r}) = \\frac{Q_e}{\\varepsilon_0} \\frac{1}{4 \\pi |\\mathbf{r} - \\mathbf{r}|} \\] <p>We know that \\(- \\nabla^2{V(\\mathbf{r})} = \\nabla \\cdot \\mathbf{E}(\\mathbf{r})\\). Thus, recall the electric field of a point charge.</p> \\[ \\mathbf{E}(\\mathbf{r}) = -\\nabla{V(\\mathbf{r})} = \\frac{Q_e}{\\varepsilon_0} \\nabla({\\frac{-1}{4\\pi|\\mathbf{r}-\\mathbf{r'}|}}) = \\frac{Q_e}{\\varepsilon_0} \\frac{\\mathbf{r} - \\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\] <p>Taking the divergence, we find that</p> \\[ - \\nabla^2{V(\\mathbf{r})} = G(\\mathbf{r}, \\mathbf{r'}) \\frac{Q_e}{\\varepsilon_0} = \\frac{Q_e}{\\varepsilon_0} \\nabla^2({\\frac{-1}{4\\pi|\\mathbf{r}-\\mathbf{r'}|}}) =  \\frac{Q_e}{\\varepsilon_0} \\nabla \\cdot \\frac{\\mathbf{r} - \\mathbf{r'}}{|\\mathbf{r}-\\mathbf{r'}|^3} = \\frac{Q_e}{\\varepsilon_0} \\delta(\\mathbf{r} - \\mathbf{r'}) \\] <p>Thus, we see that</p> \\[ \\nabla^2 {\\frac{-1}{4\\pi}}{|\\mathbf{r}-\\mathbf{r'}|} = \\delta(\\mathbf{r} - \\mathbf{r'}) \\quad \\Rightarrow \\quad G(\\mathbf{r}, \\mathbf{r'}) = {\\frac{-1}{4\\pi}}{|\\mathbf{r}-\\mathbf{r'}|} \\] <p>Finally,</p> \\[ V_e(\\mathbf{r}) = \\int_{V'} \\frac{1}{4 \\pi |\\mathbf{r} - \\mathbf{r}|} \\frac{\\rho_e}{\\varepsilon_0} d{V'} \\] \\[ V_m(\\mathbf{r}) = \\int_{V'} \\frac{1}{4 \\pi |\\mathbf{r} - \\mathbf{r}|} \\frac{\\rho_m}{\\mu_0} d{V'} \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-34-the-laplace-and-poisson-equations-with-boundary-conditions","title":"Section 3.4 - The Laplace and Poisson Equations with Boundary Conditions","text":"<p>Theorem. The Mean Value Theorem states that if a function satisfies the laplace equation for every point within a region, then the value of the function at the center of the applicable region is equal to the average of the function along the boundary of said region.</p> <p>This can be extended to the Method of Relaxations, in which each point in a mesh is defined by the average of its neighboring points. This is only useful in computer graphics.</p> <p>An interesting consequence of this states that there are no local minima or maxima within said region. The global maximum and minimum must be located at the boundary.</p> <p>Theorem. This leads to Earnshaw's Theorem. To make an electric trap to hold charges, more than zero forces must be applied to the charge, so that if the charge leaves its dedicated position it is forced back. Depending on the sign, at said point, potential must increase or decrease in all directions. However, this would force a local extrema. This cannot be possible.</p> <p>Theorem. The solution to a Laplace or Poisson equation is unique.</p>"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-35-multipole-expansion-of-the-electrostatic-or-magnetostatic-field","title":"Section 3.5 - Multipole Expansion of the Electrostatic or Magnetostatic Field","text":"<p>We want a simple way to write the Green function.</p> <p>Let us assume all charge is contained in a sphere with radius \\(R\\) centered at the origin. Then, for points \\(r\\) far from the origin, the Green function can be written as</p> \\[ \\frac{1}{4 \\pi|\\mathbf{r}-\\mathbf{r'}|} = \\frac{1}{4\\pi \\sqrt{r^2 - 2\\mathbf{r} \\cdot \\mathbf{r'} + r'^2}} = \\frac{1}{4\\pi r}(1 - 2 \\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{r'}} + \\frac{r'}{r} + \\frac{r'^2}{r^2})^{-\\frac{1}{2}} \\] <p>This inverse square root term \\((1 - 2 \\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{r'}} + \\frac{r'}{r} + \\frac{r'^2}{r^2})^{-\\frac{1}{2}}\\) can be expanded as a power series in \\(\\frac{r'}{r}\\).</p> <p>The first two terms of this power series are simple enough.</p> \\[ G(\\mathbf{r}, \\mathbf{r'}) = \\frac{1}{4 \\pi |\\mathbf{r}-\\mathbf{r'}|} \\approx \\frac{1}{4 \\pi r} ( 1 + \\frac{\\hat{\\mathbf{r}} \\cdot \\mathbf{r'}}{r}); \\quad \\text{ for}  r &gt; r' \\] <p>Applying this to the equation for voltage, we see that</p> \\[ V_e(r) = \\frac{1}{\\varepsilon_0} \\int_{V'} G(\\mathbf{r}, \\mathbf{r'}) p_e(\\mathbf{r'}) d{V'} \\approx \\frac{1}{4 \\pi \\varepsilon_0 r} \\int_{V'}  (1 + \\frac{\\hat{\\mathbf{r}} \\cdot \\mathbf{r'}}{r}) p_e(\\mathbf{r'}) d{V'} = \\frac{Q_e}{4 \\pi \\varepsilon_0 r} + \\frac{\\hat{\\mathbf{r}} \\cdot \\mathbf{p}}{4 \\pi \\varepsilon_0 r^2} \\] <p>Definition. The first and second terms of this equation are the monopole and dipole terms respectively.</p> <p>Definition. We define \\(\\mathbf{p}\\) as the electric dipole moment, and in the magnetic version, \\(\\mathbf{m}\\) as the magnetic dipole moment as follows:</p> \\[ \\mathbf{p} = \\int_{V'} \\mathbf{r'} \\rho_e(\\mathbf{r'}) d{V'} \\] <p>Notably, the moments only depend on the charge density, not the point at which the field is being examined. That is, this integral only needs to be computed once.</p> <p>To compute higher-order terms, let \\(\\varepsilon = 2\\frac{r'}{r}\\hat{\\mathbf{r}}\\cdot\\hat{\\mathbf{r'}}-(\\frac{r'}{r})^2\\). Now we can expand \\((1-\\varepsilon)^{-\\frac{1}{2}}\\).</p> \\[ (1-\\varepsilon)^{-\\frac{1}{2}} = 1 + \\frac{1}{2}\\varepsilon + \\frac{3}{8}\\varepsilon^2 + \\frac{5}{16}\\varepsilon^3 + \\ldots \\] <p>However, we want an expansion in terms of \\(t = \\frac{r'}{r}\\). To do this, we write the expansion as</p> \\[ \\frac{1}{4 \\pi |\\mathbf{r}-\\mathbf{r'}|} = \\frac{1}{4 \\pi r} \\sum_{n=0}^{\\infty} (\\frac{r'}{r}) P_n(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{r'}}) \\] <p>Here, \\(P_n(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{r'}})\\) is a polynomial. Because \\(|\\mathbf{r}-\\mathbf{r'}|\\) is symmetric, we can say that if \\(r' &gt; r\\) instead, simply switch the two. Thus the equation becomes</p> \\[ \\frac{1}{4 \\pi |\\mathbf{r}-\\mathbf{r'}|} = \\frac{1}{4 \\pi} \\sum_{n=0}^{\\infty} (\\frac{r'^n_{&lt;}}{r^{n+1}_{&gt;}}) P_n(\\hat{\\mathbf{r}} \\cdot \\hat{\\mathbf{r'}}) \\] <p>Where \\(r_&gt;\\) is the greater of \\(r, r'\\), and \\(r_&lt;\\) the lesser.</p> <p>Definition. The polynomials \\(P_n(x)\\) are Legendre Polynomials. We define them as follows:</p> \\[ (1 - 2tx + t^2)^{-\\frac{1}{2}} = \\sum_{n=0}^\\infty t^n P_n(x) \\] <p>Note that as a quirk of the function, \\(P_n(1) = 1\\) for all \\(n\\).</p> <p>We can apply these quadrupole and beyond terms to the violate or other equations, however, this becomes very messy.</p>"},{"location":"physics/electrostatics/4-conductors/","title":"Chapter 4 - Conductors and Static Electric Fields","text":""},{"location":"physics/electrostatics/4-conductors/#section-41-introduction","title":"Section 4.1 - Introduction","text":"<p>We will focus primarily on electric fields and charges. For the purposes for this section, we will assume insulators are perfect.</p>"},{"location":"physics/electrostatics/4-conductors/#section-42-electrostatic-properties-of-a-conductor","title":"Section 4.2 - Electrostatic Properties of a Conductor","text":"<p>In a metal or conductor, there are plentiful charges not bound to a particular atom and are thus free to move throughout the material.</p> <p>We note that there is no electric fiend inside a conductor, as charges internal to the material would move under the force it generates until they find a configuration that eliminates the field. This may happen, but not in electrostatics.</p> <p>Additionally, as the field is zero, it follows from Maxwell's equations that there is no charge inside a conductor. However, charge may be present at the surface. For sufficiently symmetric charges, this charge may be calculated.</p> <p>Consider any two points internal to the conductor. The voltage between said points is defined as \\(\\int_A^B \\mathbf{E} \\cdot d{\\mathbf{l}}\\). Since \\(\\mathbf{E} = 0\\) inside the conductor, the voltage difference must be zero. Thus, any two points in or on the surface (TODO: Why on the surface?) of a conductor must be at the same potential.</p> <p>The electric field at the surface of a conductor is perpendicular to its surface. Consider some displacement \\(d{\\mathbf{l}}\\). Now, \\(\\mathbf{E} \\cdot d{\\mathbf{l}} = \\mathbf{E}_s \\cdot d{\\mathbf{l}}_s + \\mathbf{E}_p \\cdot d{\\mathbf{l}}_p = d{V_s} + d{V_p}\\), in terms of parallel and perpendicular components. The parallel voltage difference is zero, so the electric field must be zero.</p> <p>Consider the surface of a conductor with surface charge density \\(\\sigma_e\\). A cylinder with one end inside and one end outside said surface, with its axis normal to said surface, will be a Gaussian \"pillbox\", which will show that with V being the volume of the pillbox, \\(\\int_V \\nabla \\cdot \\mathbf{E} d{V} = \\frac{Q_e}{\\varepsilon_0} = \\frac{A\\sigma_e}{\\varepsilon_0}\\). Thus, \\(\\sigma_e = \\varepsilon_0 E\\).</p>"},{"location":"physics/electrostatics/4-conductors/#section-43-exercises-involving-conductors-at-fixed-potentials","title":"Section 4.3 - Exercises involving conductors at fixed potentials","text":"<p>Consider a square with left and right potentials \\(V(0, y) = V(l, y) = V_1\\) and \\(V(x, 0) = V(x, l) = V_2\\). Since we are uniform in \\(z\\), we can say that \\(V(x, y) = X(x)Y(y)\\) and apply separation of variables.</p> <p>In spherical polar coordinates, we see that with azimuthal symmetry, \\(V(r, \\theta) = \\sum_{l=0}^\\infty a_l r^l P_l(cos\\theta)\\) where \\(P_l(x)\\) are Legendre polynomials.</p> <p>Theorem. 4.3.3: A Laplace equation's solution must be unique inside a volume \\(\\Omega\\) if \\(\\int_{d{\\Omega}}[\\Phi(\\mathbf{r})\\nabla{\\Phi{\\mathbf{r}}} \\cdot \\hat{\\mathbf{n}} d{S} = 0]\\). With this, consider a surface \\(d{\\Omega}\\) that surrounds conductors. The integral vanishes if a) the potential is specified on each conductor or b) the total charge on each conductor is specified.</p> <p>Now, define \\(\\Phi(\\mathbf{r})\\) as the difference between any two potential solutions to the Laplace equation at point \\(\\mathbf{r}\\). Since potential must be a constant,</p> \\[ \\int_{d{\\Omega}}[\\Phi(\\mathbf{r})\\nabla{\\Phi{\\mathbf{r}}}] \\cdot \\hat{\\mathbf{n}} d{S} = \\sum_{i=1}^N \\Phi_i \\int_{d{\\Omega_i}} \\nabla{\\Phi{\\mathbf{r}}} \\cdot \\hat{\\mathbf{n}} d{S} \\] <p>Thus, if potential is specified, \\(\\Phi_i\\) vanishes for that conductor. If the total charge is instead specified, the gradient vanishes because there is no difference in charge between any two points.</p>"},{"location":"physics/electrostatics/4-conductors/#section-44-electric-field-polarization-field-and-flux-density-in-the-presence-of-conductors","title":"Section 4.4 - Electric Field, Polarization Field, and Flux Density in the Presence of Conductors","text":"<p>Definition. A bound charge is any charge in a conductor that is bound to an atom and not free to be redistributed at the surface. We say that bound charges are the source of the polarization field \\(\\mathbf{P}\\). Additionally, we note the charge density of bound charges is \\(\\rho_{eb}\\). Thus,</p> \\[ \\nabla \\cdot \\mathbf{P} = - \\rho_{eb} \\] <p>This field is zero outside of a material, and if non-zero inside a material, will drop to zero at the surface discontinuously. If there is a component perpendicular to the surface, the discontinuity will generate curl. If there is a component parallel to the surface, it will generate divergence.</p> <p>Definition. Charges not bound are called free, with density denoted as \\(\\rho_{ef}\\). Combined with \\(\\rho_{eb}\\), they form the basis of the electric field. THat is,</p> \\[ \\varepsilon_0 \\nabla \\cdot \\mathbf{E} = \\rho_{ef} + \\rho_{eb} \\] <p>Definition. The electric flux density field \\(\\mathbf{D}\\) is defined as</p> \\[ \\mathbf{D} = \\varepsilon_0 \\mathbf{E} + \\mathbf{P} \\] <p>Both \\(\\mathbf{D}\\) and \\(\\mathbf{P}\\) have units of Coulombs/m^2. Additionally, we see that</p> \\[ \\nabla \\cdot \\mathbf{D} = \\nabla \\cdot (\\varepsilon_0 \\mathbf{E} + \\mathbf{P}) = \\nabla \\cdot \\varepsilon_0 \\mathbf{E} + \\nabla \\cdot \\mathbf{P} = (\\rho_{ef} + \\rho_{eb}) - \\rho_{eb} = \\rho_{ef} \\]"},{"location":"physics/electrostatics/4-conductors/#section-45-induced-electric-charges-their-potentials-and-fields","title":"Section 4.5 - Induced Electric Charges, their Potentials and Fields","text":"<p>This is an application chapter.</p>"},{"location":"physics/electrostatics/4-conductors/#section-46-capacitance","title":"Section 4.6 - Capacitance","text":"<p>Definition. The capacitance of an object \\(C\\) is the charge per volt, such that</p> \\[ C := \\frac{Q}{V} \\] <p>This unit, \\(\\frac{C}{V}\\), is known as a Farad. For a sphere, \\(C = 4 \\pi \\varepsilon_0 R\\). For a parallel plate capacitor, this reduces to \\(C = \\frac{epsilon_0 A}{d}\\).</p>"},{"location":"physics/electrostatics/4-conductors/#section-47-forces-on-charged-conductors-in-electric-fields","title":"Section 4.7 - Forces on Charged Conductors in Electric Fields","text":"<p>We know that \\(\\mathbf{F} = \\int \\mathbf{E}_{ext}(\\mathbf{r}) \\rho_e(\\mathbf{r}) dV\\), where \\(\\mathbf{E}_{ext}(\\mathbf{r})\\) is the external electric field and \\(\\rho_e(\\mathbf{r})\\) is the charge density of the object.</p>"},{"location":"physics/electrostatics/5-moving-charges/","title":"Chapter 5 - Electrodynamics with Moving Charges","text":""},{"location":"physics/electrostatics/5-moving-charges/#section-51-currents-in-steady-state-regime","title":"Section 5.1 - Currents in Steady-State Regime","text":"<p>We want to work in a steady-state system. Thus, we restrict ourselves to currents that do not change in time.</p> <p>With math, we see that \\(\\nabla \\cdot \\mathbf{J}(\\mathbf{r}) = -\\frac{\\partial \\rho(\\mathbf{r})}{\\partial t}\\). Since we are only considering a steady-state system, \\(\\nabla \\cdot \\mathbf{J}_e = \\nabla \\cdot \\mathbf{J}_m = 0\\).</p> <p>Definition. The conductance of a material is \\(G = \\frac{1}{R}\\), where \\(R\\) is the resistance of a material.</p> <p>For a wire of uniform cross-sectional area, we see that \\(G = \\sigma \\frac{A}{L}\\), where \\(A\\) is the cross-sectional area, \\(L\\) is the length of the wire, and \\(\\sigma\\) is the conductivity of a wire. Inverted, we see that \\(R\\) = \\(\\rho \\frac{L}{A}\\), where \\(\\rho = \\frac{1}{\\sigma}\\) is the resistivity of the wire.</p> <p>Definition. Ohm's Law can be written as \\(I = G V\\), or inverted, \\(V = IR\\). In a wire, we see that current density \\(\\mathbf{} = \\frac{I}{A} = \\sigma \\frac{V}{L} = \\sigma \\mathbf{E}\\)</p>"},{"location":"physics/electrostatics/5-moving-charges/#section-52-currents-and-curling-fields","title":"Section 5.2 - Currents and Curling Fields","text":"<p>We know that \\(\\mathbf{J}_e = \\nabla \\times{\\mathbf{H}}\\) and \\(\\mathbf{J}_m = -\\nabla \\times{\\mathbf{E}}\\). That is, current densities cause the opposing field to curl.</p> <p>For a wire with current \\(I_e\\), we see that applying Stoke's theorem to the first equation,</p> <p>$ \\int_S \\nabla \\times{\\mathbf{H}} \\cdot \\hat{\\mathbf{n}} d{S} = \\int_{\\partial S} = \\mathbf{H} \\cdot d{\\mathbf{l}}$. Apply the identity \\(\\nabla \\times{\\mathbf{H}} = \\mathbf{J}_e\\) to the left side to see that \\(\\int_S \\nabla \\times{\\mathbf{H}} \\cdot \\hat{\\mathbf{n}} d{S} = \\int_S \\mathbf{J}_e \\cdot \\hat{\\mathbf{n}} d{S} = (I_e)_S\\), or the current passing through the cross-sectional area. By the original equation, we see that \\((I_e)_S = \\mathbf{H} \\cdot d{\\mathbf{l}}\\).</p> <p>If we assume cylindrical coordinates and that \\(\\mathbf{H}(vb{r}) = H_\\varphi(s) \\hat{\\mathbf{\\varphi}}\\), then \\(\\mathbf{H} \\cdot d{\\mathbf{l}} = \\int_0^{2\\pi} H_\\varphi(S) s d{\\varphi}\\), so then \\((I_e)_S = \\int_0^{2\\pi} H_\\varphi(S) s d{\\varphi}\\). Thus, for \\(s &gt; a\\) (where \\(a\\) is the radius of the wire), \\(2\\pi s H_\\varphi = I_e\\), and for \\(s &lt; a\\), \\(2\\pi s H_\\varphi = I_e \\frac{s^2}{a^2}\\).</p> <p>By Helmholtz Theorem, we know that \\(\\mathbf{H}(\\mathbf{r}) = \\nabla \\times{\\mathbf{A}(\\mathbf{r})}\\). For a current-carrying wire, \\(\\mathbf{A}(\\mathbf{r}) = \\frac{I_e}{4\\pi} \\int_{\\text{wire}} \\frac{d{\\mathbf{l'}}}{|\\mathbf{r}-\\mathbf{r'}|}\\). Applying identities, we see the *Law of Biot and Savart$, where</p> \\[ \\mathbf{H}(\\mathbf{r}) = \\int{I_e}{4\\pi}\\int_{\\text{wire}} \\frac{-(\\mathbf{r}-\\mathbf{r'}) \\times d{\\mathbf{l'}}}{|\\mathbf{r}-\\mathbf{r'}|^3} \\] <p>Consider a current loop instead, on the \\(x-y\\) plane and current \\(I\\). Then, \\(r = z \\hat{\\mathbf{z}}\\) and \\(d{\\mathbf{l'}} = R \\hat{\\mathbf{\\varphi'}} d\\phi'\\), and the magnetic field collapses to \\(\\mathbf{H}(s = 0, z) = \\frac{I_e R^2}{2(R^2 + z^2)^{\\frac{3}{2}}} \\hat{\\mathbf{z}}\\)</p> <p>Consider some infinite bar magnet with height \\(h\\) and width \\(w\\). Then, the top and bottom surfaces will have a magnetic charge with density \\(\\mathbf{J}_m^+ = M_0 \\mathbf{b} \\delta(z - h)\\) and \\(\\mathbf{J}_m^- = -M_0 \\mathbf{v} \\delta(z)\\) respectively. By definition, \\(I_m = M_0 w v\\).</p> <p>Now, consider a loop around only the top of the conductor. Then,</p> \\[ \\int_S \\mathbf{J}_m \\cdot \\hat{\\mathbf{n}} d{S} = I_m = M_o w v \\] <p>By definition,</p> \\[ \\int_S \\mathbf{J}_m \\cdot \\hat{\\mathbf{n}} d{S} = -\\int_S (\\nabla \\times{\\mathbf{E}}) \\cdot \\hat{\\mathbf{n}} d{S} \\] <p>Applying Stokes theorem,</p> \\[ \\int_S (\\nabla \\times{\\mathbf{E}}) \\cdot \\hat{\\mathbf{n}} d{S} = M_0 w v \\]"},{"location":"physics/electrostatics/5-moving-charges/#section-53-forces-on-moving-charges-and-current","title":"Section 5.3 - Forces on Moving Charges and Current","text":"<p>Consider an electric charge moving with velocity \\(\\mathbf{v}\\) in a magnetic parallel plate capacitor with charge densities \\(\\pm \\sigma_m\\). That is, \\(\\mu_0 \\mathbf{H} = \\sigma_m \\hat{\\mathbf{z}}\\). Then, we can apply theorems to see the resulting force.</p> <p>Theorem. Lorentz Force Law states that \\(\\mathbf{F} = q_e \\mathbf{v} \\times \\mu_0 \\mathbf{H}\\) in the presence of a magnetic field. In the presence of both an electric and magnetic field, \\(\\mathbf{F} = q_e (\\mathbf{E} + \\mathbf{v} \\times \\mu_0 \\mathbf{H})\\).</p> <p>Theorem. Ampere's Force Law states that generalizing the previous theorem, we can see that</p> \\[d\\mathbf{F} = I_e d{\\mathbf{L}} \\times \\mu_0 \\mathbf{H}(\\mathbf{r})\\]"},{"location":"physics/electrostatics/5-moving-charges/#section-54-multipole-expansion-of-a-vector-potential","title":"Section 5.4 - Multipole Expansion of a Vector Potential","text":"<p>This is messy. Skipped.</p>"},{"location":"physics/electrostatics/6%20-polarization-magnetization/","title":"Chapter 6 - Polarization and Magnetization","text":"<p>TODO</p>"},{"location":"physics/electrostatics/7-time-dependent-em-fields/","title":"Chapter 7 - Time Dependent Electric and Magnetic Fields","text":"<p>TODO</p>"},{"location":"physics/mechanics/1-classical-mechanics/","title":"Chapter 1 - Classical Mechanics","text":""},{"location":"physics/mechanics/1-classical-mechanics/#section-12-space-and-time","title":"Section 1.2 - Space and Time","text":"<p>Each point \\(P \\in \\mathbb{R}^3\\) can be written as \\((x, y, z)\\). However, we will utilize the convention below:</p> \\[\\mathbf{r} = x \\hat{\\mathbf{x}} + y \\hat{\\mathbf{y}} + z \\hat{\\mathbf{z}}\\] <p>However, when working with ambiguous unit basis, it may be better to use the following:</p> \\[\\mathbf{r} = r_1 \\hat{\\mathbf{e_1}} + r_2 \\hat{\\mathbf{e_2}} + r_3 \\hat{\\mathbf{e_3}}\\] <p>Here, \\(r_1 = x, r_2 = y, r_3 = z\\) and \\(\\hat{\\mathbf{e_1}} = \\hat{\\mathbf{x_1}}, \\ldots\\). That is, \\(\\mathbf{r} = \\sum r_i \\hat{\\mathbf{e_i}}\\).</p> <p>Vectors are an \\(\\mathbb{R}\\)-module, with addition defined component-wise. Additionally, the scalar (dot) product and vector (cross) product are defined as usual.</p> <p>Regarding time, classically, time is universal.</p> <p>Definition. Reference frames are frames of motion with a defined coordinate system and origin position.</p> <p>Definition. Inertial frames are reference frames in which Newton's laws of motion hold true. An inertial frame may not be accelerating.</p>"},{"location":"physics/mechanics/1-classical-mechanics/#section-13-mass-and-force","title":"Section 1.3 - Mass and Force","text":"<p>Definition. The mass \\(m\\) of an object characterizes its translational inertia, and is measured in kilograms (kg)</p> <p>Definition. The force \\(\\mathbf{F}\\) exerted on an object is a push or pull on said object and is measured in Newtons (N). Note that force is a vector.</p>"},{"location":"physics/mechanics/1-classical-mechanics/#section-14-newtons-first-and-second-laws-inertial-frames","title":"Section 1.4 - Newton's First and Second Laws; Inertial Frames","text":"<p>Definition. A point mass or particle is a convenient fiction, in which an object with mass has no size. It may move in space but has no internal degrees of freedom. Additionally, it may not have any rotational or vibrational kinetic energy.</p> <p>Theorem. Newton's First Law. In the absence of external forces, a particle moves with constant velocity \\(\\mathbf{v}\\).</p> <p>Theorem. Newton's Second Law. Given any particle with mass \\(m\\), the net force \\(\\mathbf{F}\\) on the particle is always equal to the particle's mass times its acceleration. That is,</p> \\[\\mathbf{F} = m\\mathbf{a} = m dot{\\mathbf{r}}\\] <p>This can also be rewritten in terms of momentum. We know that momentum \\(\\mathbf{p}\\) can be written as \\(\\mathbf{p} = m\\mathbf{v} = m \\dot{\\mathbf{r}}\\). Then,</p> \\[\\mathbf{F} = m\\mathbf{a} = m \\dot{\\mathbf{p}} = m dot{\\mathbf{r}}\\] <p>If we have a constant force \\(\\mathbf{F} = F_0 \\hat{\\mathbf{x}}\\), we can write \\(dot{\\hat{\\mathbf{x}}}(t) = \\frac{F_0}{m}\\). Then,</p> \\[\\dot{\\mathbf{x}}(t) = \\int dot{\\mathbf{x}} dt = v_0 + \\frac{F_0}{m}t\\] \\[\\mathbf{x}(t) = \\int \\dot{\\mathbf{x}} dt = x_0 + v_0t + \\frac{F_0}{2m}t^2\\] <p>Definition. An inertial frame is a reference frame relative to some fixed frame if they are moving with constant velocity in regards to each other. Otherwise, the frames are non-inertial.</p>"},{"location":"physics/mechanics/1-classical-mechanics/#section-15-the-third-law-and-conservation-of-momentum","title":"Section 1.5 - The Third Law and Conservation of Momentum","text":"<p>Definition. Newton's Third Law. Every force has an equal and opposite. If \\(F_{21}\\) is the force exerted on object \\(2\\) by object \\(1\\), there exists some force with equal magnitude \\(F_{12}\\) exerted on object \\(1\\) by object \\(2\\).</p> <p>Definition. Force pairs that operate in the same line as each other (eg. gravitational attraction) are called central forces.</p> <p>Recall that the change in momentum of any particle can be defined as \\(\\dot{\\mathbf{p}}_1 = \\mathbf{F}_1 = \\mathbf{F}_{12} + \\mathbf{F}_{1}^{ext}\\). Then, \\(\\dot{\\mathbf{p}}_2 = \\mathbf{F}_{21} + \\mathbf{F}_2^{ext}\\). As \\(\\mathbf{P} = \\mathbf{p}_1 + \\mathbf{p}_2\\), we can see that</p> \\[\\dot{\\mathbf{P}} = \\mathbf{p}_1 + \\mathbf{p}_2 = \\mathbf{F}_{12} +\\mathbf{F}_{21} + \\mathbf{F}_{1}^{ext} + \\mathbf{F}_{2}^{ext} = \\mathbf{F}_{1}^{ext} + \\mathbf{F}_{2}^{ext} = \\mathbf{F}^{ext}\\] <p>From this, we can see that if \\(\\mathbf{F}^{ext} = 0\\), then \\(\\mathbf{P}\\) is a constant.</p> <p>This argument can be generalized to multi-particle systems. Consider particle \\(\\alpha\\). Then, \\(\\dot{\\mathbf{p}}_\\alpha = \\mathbf{F}_\\alpha = \\sum_{\\beta \\neq \\alpha} \\mathbf{F}_{\\alpha\\beta} + \\mathbf{F}_\\alpha^{ext}\\). We  can then see that</p> \\[\\dot{\\mathbf{P}} = \\sum_\\alpha \\mathbf{p}_\\alpha = \\sum_\\alpha \\sum_{\\beta \\neq \\alpha} \\mathbf{F}_{\\alpha\\beta} + \\sum_\\alpha \\mathbf{F}_\\alpha^{ext}\\] <p>With \\(\\sum_\\alpha \\sum_{\\beta \\neq \\alpha} \\mathbf{F}_{\\alpha\\beta} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} \\mathbf{F}_{\\alpha\\beta}( + \\mathbf{F}_{\\beta\\alpha})\\), we can see that \\(\\dot{\\mathbf{P}} = \\sum_\\alpha \\mathbf{F}_\\alpha^{ext}\\).</p> <p>Note that Newton's Third Law breaks down as the relative objects approach the speed of light, as it presumes the forces are equal at the same time. As events that are coincident in time in one frame may not be in another, it is clear that this law cannot hold.</p>"},{"location":"physics/mechanics/1-classical-mechanics/#section-16-newtons-second-law-in-cartesian-coordinates","title":"Section 1.6 - Newton's Second Law in Cartesian Coordinates","text":"<p>This is trivial. Skipped.</p>"},{"location":"physics/mechanics/1-classical-mechanics/#section-17-two-dimensional-polar-coordinates","title":"Section 1.7 - Two-Dimensional Polar Coordinates","text":"<p>This is trivial. Skipped.</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/","title":"Chapter 2 - Projectiles and Charged Particles","text":""},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-21-air-resistance","title":"Section 2.1 - Air Resistance","text":"<p>Definition. The drag, or resistive force on an object due to the atmosphere, is denoted as \\(\\mathbf{f}\\). Note that this is not. the force density, but the overall force. In most cases, this force directly opposes the direction of motion. If not, the other component is known as lift, however this is mostly negligible.</p> <p>We define air resistance as \\(\\mathbf{f} = -f(v) \\hat{\\mathbf{v}}\\). We consider two types in this text: linear, where \\(f(v) = f_{lin} = bv\\), and quadratic, where \\(f(v) = f_{quad} = cv^2\\). Note that often times we consider both, and state that \\(f(v) = f_{lin} + f_{quad} = bv + cv^2\\).</p> <p>The linear term comes from viscous drag and is generally proportional to the viscosity of the medium, while quadratic drag tends to arise from the particle needing to accelerate the mass of air which it is constantly colliding against.</p> <p>In some cases, we can calculate these coefficients. With \\(D\\) as the diameter of a spherical object, and \\(\\beta\\) and \\(\\gamma\\) as properties of the medium, we can state that \\(b = \\beta D\\) and \\(c = \\gamma D^2\\). In air at STP, \\(\\beta = 1.6 \\times 10^{-4} \\text{N} \\cdot \\text{s}/\\text{m}^2\\), and \\(\\gamma = 0.25 \\text{N} \\cdot \\text{s}^2/\\text{m}^4\\).</p> <p>Oftentimes, one factor is far more impactful than the other, meaning that the smaller of the two may be neglected. To do so, compute the following ratio:</p> \\[\\frac{f_{quad}}{f_{lin}} = \\frac{cv^2}{bv} = \\frac{\\gamma D}{\\beta}v\\] <p>Note that the result is expected to be of the same order of magnitude as the Reynolds number \\(R = Dv \\mathcal{Q}/\\eta\\), where \\(\\mathcal{Q}\\) is the density of the medium and \\(\\eta\\) the viscosity.</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-22-linear-air-resistance","title":"Section 2.2 - Linear Air Resistance","text":"<p>First, consider the case in which the drag force is negligible. Then, we see that with \\(\\mathbf{F}_g = \\mathbf{w} = mg\\) and \\(\\mathbf{F}_{drag} = \\mathbf{f} = -b\\mathbf{v}\\), then Newton's second law tells us that</p> \\[\\mathbf{F} = m \\dot{\\mathbf{v}} = m\\mathbf{g} - b\\mathbf{b}\\] <p>This separates into two equations:</p> \\[\\begin{align} \\dot{v}_x &amp;= -\\frac{b}{m}v_x \\\\ \\dot{v}_y &amp;= g - \\frac{b}{m}v_y \\end{align}\\] <p>These are separable equations and can be trivially solved.</p> <p>For an object on a surface in which the only motion is in the \\(x\\)-direction, only the first of the above equations applies. Then, we can define \\(k = \\frac{b}{m}\\), so that \\(\\dot{v_x} = -k v_x\\). This can be integrated to see that \\(v_x(t) = Ae^{-kt}\\). We can also define \\(\\tau = \\frac{1}{t} = \\frac{m}{b}\\), so that \\(v(t) = Ae^{-\\frac{t}{\\tau}}\\)</p> <p>Now, consider the second equation. This is still separable, however, the math is more complicated. Note that when we set \\(\\dot{v}_y = 0\\), we are at some maximum (terminal) velocity \\(v_{ter}\\). Solving, we see that \\(v_{ter} = \\frac{mg}{b}\\). Then, we can rewrite our equation as</p> \\[m\\dot{v}_y = -b(v_y - v_ter)\\] <p>This can be solved to see that \\(v(y) = v_{ter} + (v_{y0} - v_{ter})e^{-\\frac{t}{\\tau}}\\).</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-23-trajectory-and-range-in-a-linear-medium","title":"Section 2.3 - Trajectory and Range in a Linear Medium","text":"<p>From the velocity functions in the previous section, we can integrate to obtain the position functions.</p> \\[\\begin{align} x(t) &amp;= v_{x0}\\tau(1-e^{-\\frac{t}{\\tau}}) \\\\ y(t) &amp;= (v_{y0} + v_{ter})\\tau(1-e^{-\\frac{t}{\\tau}})-v_{ter}t \\end{align}\\] <p>We can then solve the first equation for \\(t\\) and substitute into the second to see that</p> \\[y(t) = \\frac{v_{y_0} + v_{ter}}{v_{x0}} + v_{ter} \\tau \\ln(1 - \\frac{x}{v_{x0}\\tau})\\] <p>Note the trajectory has a vertical asymptote at \\(x = v_{x0} \\tau\\).</p> <p>We can find the horizontal range by setting \\(y(R) = 0\\). This results in an equation that contains many exponential terms, so we want to find an approximation. Truncating the Taylor series for \\(\\ln(1 - \\varepsilon)\\), we see that \\(R \\approx R_{vac}(1 - \\frac{4}{3} \\frac{v_{y0}}{v_{ter}})\\), where \\(R_{vac}\\), or the air resistance assuming no air resistance, is \\(R_{vac} = \\frac{2 v_{x0}v_{y0}}{g}\\).</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-24-quadratic-air-resistance","title":"Section 2.4 - Quadratic Air Resistance","text":"<p>In the case of quadratic air resistance, we see that \\(\\mathbf{f} = -c v^2 \\hat{\\mathbf{v}}\\). Then, we can see that in the horizontal case, \\(m\\frac{dv}{v_x^2} = -c dt\\). Thus,</p> \\[v_x(t) = \\frac{v_{x0}}{1+cv_{x0}t/m} = \\frac{v_{x0}}{1+\\tau/t}\\] <p>where \\(\\tau = m/cv_{x0}\\). We can see that at \\(t = \\tau\\), \\(v = v_{x0}/2\\).</p> <p>Thus, \\(x(t) = v_{x0} \\tau \\ln(1 + t / \\tau)\\).</p> <p>For exclusively vertical motion, we first define \\(v_{ter} = \\sqrt{\\frac{mg}{c}}\\). Then, we see that \\(\\dot{v_y} = g(1 - \\frac{v^2}{v_{ter}^2})\\). Solving via separation of variables yields</p> \\[v_y(t) = v_{ter}\\tanh(\\frac{gt}{v_{ter}})\\] <p>Then, we see that</p> \\[y(t) = \\frac{v_{ter}^2}{g} \\ln(\\cosh(\\frac{gt}{v_{ter}}))\\] <p>In the case where there is both vertical and horizontal motion, the differential equation no longer has an analytic solution.</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-25-motion-of-a-charge-in-a-uniform-magnetic-field","title":"Section 2.5 - Motion of a Charge in a Uniform Magnetic Field","text":"<p>Skipped. This is a practical exercise.</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-26-complex-exponentials","title":"Section 2.6 - Complex Exponentials","text":"<p>Skipped. This covers Euler's formula, Taylor series, etc.</p>"},{"location":"physics/mechanics/2-projectiles-charged-particles/#section-27-solution-for-the-charge-in-a-b-field","title":"Section 2.7 - Solution for the Charge in a B Field","text":"<p>Skipped.</p>"},{"location":"physics/mechanics/3-momentum-angular-momentum/","title":"Chapter 3 - Momentum and Angular Momentum","text":""},{"location":"physics/mechanics/3-momentum-angular-momentum/#section-31-conservation-of-momentum","title":"Section 3.1 - Conservation of Momentum","text":"<p>Consider a system of \\(N\\) particles, indexed with \\(\\alpha \\in {1, 2, \\ldots, N}\\). We found that as long as Newton's third law applies, \\(\\mathbf{P} = \\mathbf{p}_1 + \\ldots + \\mathbf{p}_N = \\sum \\mathbf{p}_\\alpha\\) is determined entirely by external forces. That is, \\(\\dot{\\mathbf{P}} = F^{ext}\\).</p>"},{"location":"physics/mechanics/3-momentum-angular-momentum/#section-32-rockets","title":"Section 3.2 - Rockets","text":"<p>Consider a rocket with mass \\(m\\) traveling in the \\(\\hat{\\mathbf{x}}\\) direction and exhausting fuel at a rate of \\(dm\\) at speed \\(v_{ex}\\). At time \\(t\\), the rocket's momentum is \\(P(t) = mv\\). Then, \\(P(t + dt) = (m + dm)(v + dv)\\), where the fuel ejected in time \\(dt\\) has mass \\(-dm\\) and velocity \\(v - v_{ex}\\) relative to the ground. Thus, the total momentum becomes \\(P(t + dt) = (m + dm)(v + dv) - dm(v - v_{ex}) = mv + m dv + dm v_{ex}\\), as \\(dm dv\\) becomes infinitesimally small.</p> <p>Then, \\(dP = P(t + dt) - P(t) = m dv + dm v_{ex}\\). If there is a net external force, this would equal \\(F^{ext}dt\\). In this case, we assume \\(F^{ext} = 0\\). Then, \\(m dv = -dm v_ex\\), or \\(m \\dot{v} = -\\dot{m} v_ex\\). This is Newton's second law, where \\(F = m \\dot{v} = -\\dot{m} v_ex\\).</p> <p>Definition. Conventionally, we say thrust is \\(-\\dot{m} v_{ex}\\).</p> <p>We  can solve this via separation of variables. If \\(v_{ex}\\) is constant, we see \\(v - v_0 = v_{ex} \\ln\\frac{m_0}{m}\\).</p>"},{"location":"physics/mechanics/3-momentum-angular-momentum/#section-33-the-center-of-mass","title":"Section 3.3 - The Center of Mass","text":"<p>Definition. Given a system of \\(\\alpha \\in {1, \\ldots, N}\\) particles, we define the center of mass of the system with respect to the origin O is</p> \\[\\mathbf{R} = \\frac{1}{M} \\sum_{\\alpha = 1}^{N} m_\\alpha \\mathbf{r}_\\alpha\\] <p>This is effectively a weighted average of the positions with masses as weights. Using this definition, we can rewrite the total momentum as</p> \\[\\mathbf{P} = \\sum_\\alpha \\mathbf{p}_\\alpha = \\sum_\\alpha m_\\alpha \\dot{\\mathbf{r}}_\\alpha = M \\dot{\\mathbf{R}}\\] <p>Notably, this says that the total momentum of a mass of particles can be replaced by one particle with the same momentum of the center of mass.</p> <p>We know that \\(\\dot{\\mathbf{P}} = \\mathbf{F}^{ext}\\), so we can then say that \\(\\mathbf{F}^{ext} = M \\ddot{\\mathbf{R}}\\). That is, the center of mass moves as if it was a single particle with mass \\(M\\) subject to the net external force. This is why we can approximate bodies as single point masses.</p> <p>Note that when mass is distributed evenly, we see that if</p> \\[\\mathbf{R} = \\frac{1}{M} \\int \\mathbf{r} dm = \\frac{1}{m} \\int \\rho \\mathbf{r} dV\\]"},{"location":"physics/mechanics/3-momentum-angular-momentum/#section-34-angular-momentum-for-a-single-particle","title":"Section 3.4 - Angular Momentum for a Single Particle","text":"<p>Definition. Angular momentum for a single particle is defined as \\(\\mathbf{\\ell} = \\mathbf{r} \\times \\mathbf{p}\\). Note that as \\(\\mathbf{r}\\) is origin-dependent, so is \\(\\mathbf{\\ell}\\). With a vector identity, we see that</p> \\[\\dot{\\mathbf{\\ell}} = \\frac{d}{dt}(\\mathbf{r} \\times \\mathbf{p}) = (\\dot{\\mathbf{r}} \\times \\mathbf{p}) + (\\mathbf{r} \\times \\dot{\\mathbf{p}})\\] <p>Notably, we can replace \\(\\mathbf{p} = m \\dot{\\mathbf{r}}\\), so we say that \\(\\dot{\\mathbf{r}} \\times \\mathbf{p} = \\dot{\\mathbf{r}} \\times m \\dot{\\mathbf{r}} = 0\\), so</p> \\[\\dot{\\mathbf{\\ell}} = \\mathbf{r} \\times \\dot{\\mathbf{p}} = \\mathbf{r} \\times m \\ddot{\\mathbf{r}} = \\mathbf{r} \\times \\mathbf{F} = \\mathbf{\\Gamma}\\] <p>Here, \\(\\mathbf{\\Gamma}\\) represents the net torque about the origin \\(O\\) on the particle, defined as \\(\\mathbf{r} \\times \\mathbf{F}\\). This is often described as the rotational form of Newton's second law.</p> <p>Note that in many one-particle systems, the origin \\(O\\) is deliberately chosen such that \\(\\mathbf{\\Gamma} = 0\\), so that the angular momentum is constant.</p> <p>Definition. For a central force between objects, \\(\\mathbf{F}\\) is parallel to the vector between the two objects.</p> <p>We will now attempt to prove Kepler's Second Law. Consider a planet at position \\(\\mathbf{r}\\) orbiting a star at the origin \\(O\\). Then, we define \\(d\\mathbf{r} = \\mathbf{v} dt\\) as the planet moves from position \\(P\\) to position \\(Q\\) in its orbit. The area swept out by the planet when orbiting thus becomes \\(dA = \\frac{1}{2} |\\mathbf{r} \\times d\\mathbf{r} = \\frac{1}{2} |\\mathbf{r} \\times \\mathbf{v} dt|\\). As \\(\\mathbf{p} = m\\mathbf{v}\\), we can say that \\(\\mathbf{v} = \\frac{\\mathbf{p}}{v}\\), so that \\(dA = \\frac{1}{2m} |\\mathbf{r} \\times \\mathbf{p}| = \\frac{\\ell dt}{2m}\\), and we can say that \\(\\frac{dA}{dt} = \\frac{\\ell}{2m}\\). This is constant.</p> <p>An alternative proof shows that \\(\\ell = mr^2 \\omega\\) where \\(\\omega = \\dot{\\phi}\\), so that \\(\\frac{dA}{dt} = \\frac{1}{2} r^2 \\omega\\).</p>"},{"location":"physics/mechanics/3-momentum-angular-momentum/#section-35-angular-momentum-for-several-particles","title":"Section 3.5 - Angular Momentum for Several Particles","text":"<p>Next, define a system with \\(\\alpha\\) particles, where each particle has angular momentum \\(\\mathbf{\\ell}_\\alpha = \\mathbf{r}_\\alpha \\times \\mathbf{p}_\\alpha\\), with common origin \\(O\\). We can then define total angular momentum as</p> \\[\\mathbf{L} = \\sum_{\\alpha=1}^N \\mathbf{\\ell}_\\alpha = \\sum_{\\alpha = 1}^N \\mathbf{r}_\\alpha \\times \\mathbf{p}_\\alpha\\] <p>We can then differentiate to see that</p> \\[\\dot{\\mathbf{L}} = \\sum_{\\alpha=1}^N \\dot{\\mathbf{\\ell}}_\\alpha = \\sum_{\\alpha = 1}^N \\mathbf{r}_\\alpha \\times \\mathbf{F}_\\alpha\\] <p>Then, we see the rate of change of angular momentum is simply net torque on the system.</p> <p>We can then state that \\(\\mathbf{F}_alpha = \\sum_{\\beta \\neq \\alpha} \\mathbf{F}_{\\alpha\\beta} + \\mathbf{F}_\\alpha^{ext}\\). Then,</p> \\[\\dot{\\mathbf{L}} = \\sum_\\alpha \\sum_{\\beta \\neq \\alpha} \\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} + \\sum_\\alpha \\mathbf{r}_\\alpha \\times \\mathbf{F}_\\alpha^{ext}\\] <p>Now, we cam convert the force between particles in the double sum by pairing forces.</p> \\[\\sum_\\alpha \\sum_{\\beta \\neq \\alpha} \\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} (\\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} + \\mathbf{r}_\\beta \\times \\mathbf{F}_{\\beta \\alpha})\\] <p>Then, if we assume all internal forces obey the third law such that \\(\\mathbf{F}_{\\alpha \\beta} = -\\mathbf{F}_{\\beta\\alpha}\\), we can write that</p> \\[\\sum_\\alpha \\sum_{\\beta &gt; \\alpha} (\\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} + \\mathbf{r}_\\beta \\times \\mathbf{F}_{\\beta \\alpha}) = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} (\\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} - \\mathbf{r}_\\beta \\times \\mathbf{F}_{\\alpha\\beta}) = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} (\\mathbf{r}_\\alpha - \\mathbf{r}_\\beta) \\times \\mathbf{F}_{\\alpha\\beta}\\] <p>We can denote \\(\\mathbf{r}_{\\alpha}{\\beta} = \\mathbf{r}_\\alpha - \\mathbf{r}_\\beta\\) as the vector pointing from \\(\\beta\\) to \\(\\alpha\\). Thus,</p> \\[\\sum_\\alpha \\sum_{\\beta \\neq \\alpha} \\mathbf{r}_\\alpha \\times \\mathbf{F}_{\\alpha \\beta} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} (\\mathbf{r}_\\alpha - \\mathbf{r}_\\beta) \\times \\mathbf{F}_{\\alpha\\beta}\\] <p>If the internal forces are all central, the cross product becomes zero. Then, the rate of change of angular momentum becomes</p> \\[\\dot{\\mathbf{L}} = \\sum_\\alpha \\mathbf{r}_\\alpha \\times \\mathbf{F}_\\alpha^{ext} = \\mathbf{\\Gamma}^{ext}\\] <p>where \\(\\mathbf{\\Gamma}^{ext}\\) is the net external torque. Notably, if this value is \\(0\\), the total angular momentum is a constant.</p> <p>Definition. About any axis of rotation \\(\\hat{\\mathbf{k}}\\), if an object is rotating at angular velocity \\(\\omega\\), then \\(L_k\\), the angular momentum in the \\(\\hat{\\mathbf{k}}\\)-direction, can be written as \\(L_k = O \\omega\\), where \\(I\\) is the moment of inertia of the object. For any multi-particle system, \\(I = \\sum m_\\alpha \\rho_\\alpha^2\\), where \\(\\rho_\\alpha\\) is the distance from particle \\(\\alpha\\) to the axis of rotation.</p>"},{"location":"physics/mechanics/4-energy/","title":"Chapter 4 - Energy","text":""},{"location":"physics/mechanics/4-energy/#section-41-kinetic-energy-and-work","title":"Section 4.1 - Kinetic Energy and Work","text":"<p>Theorem. Work-Energy Theorem. \\(\\frac{dT}{dt} = \\frac{1}{2} m \\frac{d}{dT}v^2 = \\frac{1}{2} m \\frac{d}{dT}(\\mathbf{v} \\cdot \\mathbf{v}) = \\frac{1}{2} m (\\dot{\\mathbf{v}} \\cdot \\mathbf{v} + \\mathbf{v} \\cdot \\dot{\\mathbf{v}}) = m \\dot{\\mathbf{v}} \\cdot \\mathbf{v} = \\mathbf{F} \\cdot \\mathbf{v}\\). Then, we can see that \\(dT = \\mathbf{F} \\cdot d\\mathbf{r}\\), where \\(\\mathbf{F} \\cdot d\\mathbf{r}\\) is the work done by force \\(\\mathbf{F}\\) over a small displacement \\(d\\mathbf{r}\\).</p>"},{"location":"physics/mechanics/4-energy/#section-42-potential-energy-and-conservative-forces","title":"Section 4.2 - Potential Energy and Conservative Forces","text":"<p>Theorem. For a force to be conservative, it must be able to be written as some \\(\\mathbf{F}(\\mathbf{r}) = f(r) \\hat{\\mathbf{r}}\\).</p> <p>Theorem. For a conservative force (and thus a conservative vector field), we can apply the Fundamental Theorem of Line Integrals to see that \\(\\int_1^2 \\mathbf{F} \\cdot d\\mathbf{r} = F(2) - F(1)\\), where \\(F\\) is the antiderivative of the integral. That is, the work done by said force is independent of the path taken.</p> <p>Theorem. For a conservative force, we can find some function \\(U(r)\\), where \\(U(r)\\) is the potential of said force. This function will make the mechanical energy of a system \\(E = T + U\\) a constant.</p> <p>In the presence of a nonconservative force, we see that \\(W = W_{cons} + W_{nc}\\). We can then rewrite \\(\\Delta E = \\Delta(T + U) = W_{nc}\\). Mechanical energy is no longer conserved, but the change in energy is equal to the work done by nonconservative forces.</p>"},{"location":"physics/mechanics/4-energy/#section-43-force-as-the-gradient-of-potential-energy","title":"Section 4.3 - Force as the Gradient of Potential Energy","text":"<p>We know that \\(W = \\mathbf{F} \\cdot d\\mathbf{r} = F_x dx + F_y dy + F_z dz\\).</p> <p>Additionally, \\(W = -dU = -[U(\\mathbf{r} + d\\mathbf{r}) - U(\\mathbf{r})] = -[U(x + dx, y + dy, z + dz) - U(x, y, z)] = -[\\frac{\\partial U}{\\partial x}dx + \\frac{\\partial U}{\\partial y} dy + \\frac{\\partial U}{\\partial z} dz]\\).</p> <p>We can match terms to see that \\(F_x = -\\frac{\\partial U}{\\partial x}\\) and so on, so that \\(\\mathbf{F} = -[\\hat{\\mathbf{x}} \\frac{\\partial U}{\\partial x} + \\hat{\\mathbf{y}} \\frac{\\partial U}{\\partial y} + \\hat{\\mathbf{z}} \\frac{\\partial U}{\\partial z}] = -\\nabla U\\)</p>"},{"location":"physics/mechanics/4-energy/#section-44-the-second-condition-that-f-be-conservative","title":"Section 4.4 - The Second Condition that F be Conservative","text":"<p>An easier way to prove a force is conservative is to show that \\(\\nabla \\times \\mathbf{F} = 0\\).</p> <p>If this is true, then \\(\\oint_\\Gamma (\\nabla \\times \\mathbf{F}) \\cdot \\hat{\\mathbf{n}} = 0\\). We can split the curve \\(\\Gamma\\) into \\(\\mathbf{r}_1\\) and \\(\\mathbf{r}_2\\), to show that \\(0 = \\int_1^2 \\mathbf{F} \\cdot d\\mathbf{r}_1 + \\int_2^1 \\mathbf{F} \\cdot d\\mathbf{r}_2\\), so then \\(\\int_1^2 \\mathbf{F} \\cdot d\\mathbf{r}_1 = \\int_1^2 \\mathbf{F} \\cdot d\\mathbf{r}_2\\) and thus the integral is path independent.</p>"},{"location":"physics/mechanics/4-energy/#section-45-time-dependent-potential-energy","title":"Section 4.5 - Time-Dependent Potential Energy","text":"<p>Sometimes, \\(\\mathbf{F} = \\mathbf{F}(\\mathbf{r}, t)\\). Then, if the curl of the force is zero, the work integral is now path-independent. We can define the potential as \\(U(\\mathbf{r}, t) = -\\int_{\\mathbf{r}_0}^\\mathbf{r} \\mathbf{F}(\\mathbf{r}', t) \\cdot d\\mathbf{r}'\\).</p> <p>In this case, it is still true that \\(\\mathbf{F}(\\mathbf{r}, t) = -\\nabla U(\\mathbf{r}, t)\\). Again, \\(dT = \\frac{dT}{dt} dt = (m \\hat{\\mathbf{v}} \\cdot \\mathbf{v}) dt = \\mathbf{F} \\cdot d\\mathbf{r}\\).</p> <p>However, we see that \\(dU = \\frac{\\partial U}{\\partial x}dx + \\frac{\\partial U}{\\partial y} dy + \\frac{\\partial U}{\\partial z} + \\frac{\\partial U}{\\partial t} dt\\). Then, we see that \\(dU = -\\mathbf{F} \\cdot d\\mathbf{r} + \\frac{\\partial U}{\\partial t} dt\\).</p> <p>Then, \\(dE = d(T + U) = \\frac{\\partial U}{\\partial t} dt\\). So, energy is only conserved if \\(\\frac{\\partial U}{\\partial t} = 0\\).</p>"},{"location":"physics/mechanics/4-energy/#section-46-energy-for-linear-one-dimensional-systems","title":"Section 4.6 - Energy for Linear One-Dimensional Systems","text":"<p>For a particle only moving in the \\(x\\)-direction, we can define \\(U(x) = -\\int_{x_0}^x F_x(x') dx'\\). If we let \\(F_x = -kx\\) (by Hooke's law), we see that \\(U = \\frac{1}{2} kx^2\\).</p> <p>We can plot the potential energy as a function of \\(x\\), and visually see what position an object will tend towards. Notably, where \\(dU/dx = 0\\) and \\(U\\) is at a minimum or maximum, net force is zero.</p> <p>Definition. When \\(d^2 U/dx^2 &gt; 0\\) and \\(U(x)\\) is at a minimum, the particle is said to be at stable equilibrium. That is, a small displacement will lead to a corrective force. When \\(d^2 U / dx^2 &lt; 0\\) and \\(U(x)\\) is at a maximum, the particle is said to be unstable. In this case, a small displacement will result in a force moving the particle further from equilibrium.</p> <p>Additionally, in one-dimensional systems, \\(E = T + U(x)\\) and \\(T(x) = \\frac{1}{2} m \\dot{x} = E - U(x)\\), so we can solve for \\(\\dot(x) = \\pm \\sqrt{\\frac{2}{m}} \\sqrt{E - U(x)}\\)</p> <p>If we then let \\(dt = \\frac{dx}{\\dot{x}}\\), so that \\(t = \\sqrt{\\frac{m}{2}} \\int_{x_0}^x \\frac{dx'}{\\sqrt{E - U(x')}}\\), we can then perform the integral and solve for \\(x\\).</p>"},{"location":"physics/mechanics/4-energy/#section-47-curvilinear-one-dimensional-systems","title":"Section 4.7 - Curvilinear One-Dimensional Systems","text":"<p>Consider the direction of motion \\(s\\) such that \\(T = \\frac{1}{2} m \\dot{s}\\), in which the direction of motion is not in a fixed direction. Notably, the normal force is constraining the object to a fixed path, yet doesn't move the object and thus does no work. Thus, it is the force tangential to the path that does work. We can see then that</p> \\[F_{\\perp} = m \\ddot{s}\\] <p>Here, we can define a potential \\(U(s)\\) such that \\(F_{tang} = -dU/ds\\) and the total mechanical energy \\(E = T + U(s)\\) is constant.</p> <p>Definition. In an Atwood machine, there are two masses of mass \\(m_1\\) and m_2$, suspended with an inextensible massless string over a pulley. The system can be constrained by a single parameter \\(x\\), where \\(x\\) is the vertical distance from the center of the pulley and the center of mass of \\(m_1\\).</p> <p>Then, we can see that \\(\\Delta T_1  + \\Delta U_1 = W_1^{tension}\\), and respectively \\(\\Delta T_2 + \\Delta U_2 = W_2^{tension}\\). Then, we can see that \\(\\W_1^{tension} = -W_2^{tension}\\), so \\(\\Delta(T_1 + U_1 + T_2 + U_2) = 0\\). That is, \\(E = T_1 + U_1 + T_2 + U_2\\), which is conserved.</p> <p>Notably, if all forces are conservative, we can define a potential \\(U_\\alpha\\) for each particle \\(\\alpha\\) such that</p> <p>\\(E = \\sum_\\alpha^N (T_\\alpha U_\\alpha)\\)</p>"},{"location":"physics/mechanics/4-energy/#section-48-central-forces","title":"Section 4.8 - Central Forces","text":"<p>Definition. A central force is some force such that \\(\\mathbf{F}(\\mathbf{r}) = f(\\mathbf{r}) \\hat{\\mathbf{r}}\\)..</p> <p>Definition. A spherically symmetric or rotationally invariant force is a force such that \\(f(\\mathbf{r}) = f(r)\\). Notably, this is equivalent to the force being conservative.</p> <p>This book uses spherical polar coordinates.</p> <p>Definition. \\(\\phi\\), the azimuth, is the angle between the \\(x\\)-axis and the projection of the vector \\(\\mathbf{r}\\) on the \\(x\\)-\\(y\\) plane.</p> <p>This book uses the convention of \\(r\\), \\(\\theta\\), and \\(\\phi\\), where \\(x = r \\sin \\theta \\cos \\phi\\), \\(y = r \\sin \\theta \\sin \\phi\\), and \\(z = r \\cos \\theta\\). That is, \\(\\theta\\) is the only angle extending into the \\(z\\)-direction.</p> <p>Recall that the dot product is defined as normal.</p> <p>Additionally, we see that \\(d\\mathbf{r} = dr \\hat{\\mathbf{r}} + r d\\theta \\hat{\\mathbf{\\theta}} + r \\sin \\theta d \\phi\\). Then, as \\(df = \\nabla f \\cdot d\\mathbf{r}\\), we see that \\(df = (\\nabla f)_r dr + (\\nabla f)_\\theta r d\\theta + (\\nabla f)_\\phi r \\sin \\theta d\\phi\\).</p> <p>Notably, \\((\\nabla f)_r = \\frac{\\partial f}{\\partial r}\\), \\((\\nabla f)_\\theta = \\frac{1}{r} \\frac{\\partial f}{\\partial \\theta}\\), and \\((\\nabla f)_\\phi = \\frac{1}{r \\sin \\theta} \\frac{\\partial f}{\\partial \\phi}\\).</p> <p>An important detail is that for \\(\\mathbf{F}(\\mathbf{r}) = -\\nabla U = f(r) \\hat{\\mathbf{r}}\\), this forces \\(U = U(r)\\).</p>"},{"location":"physics/mechanics/4-energy/#section-49-energy-interaction-of-two-particles","title":"Section 4.9 - Energy Interaction of Two Particles","text":"<p>Consider two particles, with forces \\(\\mathbf{F}_{12}\\) the force on particle \\(1\\) by particle \\(2\\) and the equal and opposite force \\(\\mathbf{F}_{21}\\). Notably, we can write \\(\\mathbf{F}_{12} = f(r) \\hat{\\mathbf{r}} = f(r) \\frac{\\mathbf{r}}{r}\\).</p> <p>For two forces not at the origin, we see that \\(\\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2\\) for \\(\\mathbf{F}_{12}\\). As a consequence, we tend to write \\(\\mathbf{F}_{12} = \\mathbf{F}_{12} (\\mathbf{r}_1 - \\mathbf{r}_2)\\).</p> <p>We can extend this to the potential. If we fix \\(\\mathbf{r}_2\\), we see that \\(\\mathbf{F}_{12} = -\\nabla_1 U(\\mathbf{r}_1)\\), and for an unfixed second particle, \\(\\mathbf{F}_{12} = -\\nabla_1 U(\\mathbf{r}_1 - \\mathbf{r}_2)\\).</p> <p>An important note is that \\(\\nabla_1 U(\\mathbf{r}_1 - \\mathbf{r}_2) = -\\nabla_2 U(\\mathbf{r}_1 - \\mathbf{r}_2)\\). We can then say that \\(\\mathbf{F}_{21} - \\nabla_2 U(\\mathbf{r}_1 - \\mathbf{r}_2)\\).</p> <p>Now, we can generalize this to see that \\(W_{tot} = d\\mathbf{r}_1 \\cdot \\mathbf{F}_{12} + d\\mathbf{r}_2 \\cdot \\mathbf{F}_{21} = (d\\mathbf{r}_1 - d\\mathbf{r}_2) \\cdot \\mathbf{F}_{12} = d(\\mathbf{r}_1 - \\mathbf{r}_2) \\cdot [-\\nabla_1 U(\\mathbf{r}_1 - \\mathbf{r}_2)] = -dU\\). We then see that \\(d(T + U) = 0\\). That is, \\(E = T_1 + T_2 + U\\).</p> <p>Recall elastic collisions, in which \\(T_{in} = T_{fin}\\).</p>"},{"location":"physics/mechanics/4-energy/#section-410-the-energy-of-a-multiparticle-system","title":"Section 4.10 - The Energy of a Multiparticle System","text":"<p>Consider a system with \\(N\\) particles, each donated $\\alpha. Then, we see that</p> \\[T = \\sum_\\alpha T_\\alpha = \\sum_\\alpha \\frac{1}{2}m_\\alpha v_\\alpha^2\\] <p>Assuming all forces are conservative, for each pair of particles \\(\\alpha \\beta\\), there exists a potential energy \\(U_{\\alpha\\beta}\\) between the two. Then, total potential energy can be written as</p> \\[U = U^{int}+U^{ext} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} U_{\\alpha \\beta} + \\sum_\\alpha U_\\alpha\\] <p>With this, the net force on any particle is given by \\(-\\nabla_\\alpha U\\), and total energy is conserved given no external potential energy.</p> <p>For a rigid body, we can they state \\(U^{int} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} U_{\\alpha \\beta}(\\mathbf{r}_\\alpha - \\mathbf{r}_\\beta)\\). In the case that the forces are central, \\(U^{int} = \\sum_\\alpha \\sum_{\\beta &gt; \\alpha} U_{\\alpha \\beta}(|\\mathbf{r}_\\alpha - \\mathbf{r}_\\beta|)\\).</p>"},{"location":"physics/mechanics/5-oscillations/","title":"Chapter 5 - Oscillations","text":""},{"location":"physics/mechanics/5-oscillations/#section-51-hookes-law","title":"Section 5.1 - Hooke's Law","text":"<p>Theorem. Hooke's Law. The force exerted by a spring has the form \\(F_x = -kx\\), where \\(x\\) is the displacement from its equilibrium length and \\(k\\) is the spring constant.</p> <p>From this, knowing \\(F = -\\nabla U\\), we see that the potential energy of a spring is \\(U(x) = \\frac{1}{2} kx^2\\).</p>"},{"location":"physics/mechanics/5-oscillations/#section-52-simple-harmonic-motion","title":"Section 5.2 - Simple Harmonic Motion","text":"<p>From \\(F = m\\ddot{x} = -kx\\), we see that \\(\\ddot{x} = -\\frac{k}{m} x\\). We now establish \\(\\omega = \\sqrt{\\frac{k}{m}}\\), or the angular frequency. This allows us to state</p> \\[\\ddot{x} = -\\omega^2 x\\] <p>This may also be written as \\(\\ddot{x} + \\omega^2 x = 0\\), leading to the characteristic equation \\(r = \\pm \\omega i\\). Now, we can write the solutions as \\(x(t) = e^{\\pm i \\omega t}\\), so that \\(x(t) = C_1 e^{i \\omega t} + C_2 e^{-i \\omega t}\\) by superposition.</p> <p>We know from Differential Equations that for \\(r = a \\pm bi\\), we can apply Euler's formula to see the solution is \\(e^a(A \\cos(b t) + B \\sin(bt))\\), where \\(A = C_1 + C_2\\) and \\(B = i(C_1 - C_2)\\). Applying this to the spring, we see that</p> \\[x(t) = B_1 \\cos (\\omega t) + B_2 \\sin(\\omega t)\\] <p>The displacement function being real thus mandates that \\(B_2\\) is real.</p> <p>Notably, at \\(t = 0\\), we see that \\(x(0) = B_1\\), and \\(\\dot{x}(0) = B_2\\).</p> <p>Definition. The period \\(\\tau\\) is the time in which a function repeats itself. The period for \\(\\sin\\) and \\(\\cos\\) are \\(\\tau = 2\\pi\\), so the period for this oscillator is $\\tau = \\frac{2\\pi}{\\omega} = 2\\pi \\sqrt{\\frac{m}{k}}.</p> <p>Now, let us simplify. We can write \\(x\\) as</p> \\[x(t) = \\sqrt{B_1^2 + B_2^2}[\\frac{B_1}{\\sqrt{B_1^2 + B_2^2}} \\cos(\\omega t) + \\frac{B_2}{\\sqrt{B_1^2 + B_2^2}} \\sin(\\omega t)]\\] <p>Now, if we have a right triangle with legs \\(B_1\\) and \\(B_2\\), we can define some \\(\\delta\\) in said triangle such that \\(\\cos(\\delta) = \\frac{B_1}{\\sqrt{B_1^2 + B_2^2}}\\) and \\(\\sin(\\delta) = \\frac{B_2}{\\sqrt{B_1^2 + B_2^2}}\\). Note that by this definition, \\(\\tan(\\delta) = \\frac{B_2}{B_1}\\). We  can thus simplify the equation for \\(x\\) to</p> \\[x(t) = \\sqrt{B_1^2 + B_2^2}[\\cos(\\delta) \\cos(\\omega t) + \\sin(\\delta) \\sin(\\omega t)] = \\sqrt{B_1^2 + B_2^2} \\cos(\\omega t - \\delta)\\] <p>A different way to see this is to write \\(C_1 = \\frac{1}{2}(B_1 - iB_2)\\) and \\(C_2 = \\frac{1}{2}(B_1 + iB_2)\\). We can then say that \\(C_2 = C_1^*\\). Then, \\(x(t) = C_1 e^{i\\omega t} + C_1^* e^{-i\\omega t}\\)</p> <p>We can then solve to see \\(x(t) = 2\\Re(C_1 e^{i \\omega t})\\), as \\(x(t) \\in \\mathbb{R}\\). Then, we can see that \\(2C_1 = B_1 - i B_2 = \\frac{1}{\\sqrt{B_1^2 + B_2^2}} e^{-i \\delta}\\). This implies that \\(x(t) = \\Re 2C_1 e^{i \\omega t} = \\Re \\frac{1}{\\sqrt{B_1^2 + B_2^2}} A e^{i(\\omega t - \\delta)}\\)</p> <p>If we consider energy, we see that \\(U = \\frac{1}{2}kx^2 = \\frac{1}{2}kA^2 \\cos^2(\\omega t - \\delta)\\), and that \\(T = \\frac{1}{2}m \\dot{x}^2 = \\frac{1}{2}m \\omega^2 A^2 \\sin^2(\\omega t - \\delta) = \\frac{1}{2} k A^2 \\sin^2 (\\omega t - \\delta)\\), as \\(\\omega^2 = k/m\\) from the original differential equation.</p> <p>Note that this implies the total energy is a constant, where \\(E = T + U = \\frac{1}{2}kA^2\\).</p>"},{"location":"physics/mechanics/5-oscillations/#section-53-two-dimensional-oscillators","title":"Section 5.3 - Two-Dimensional Oscillators","text":"<p>Definition. An isotropic harmonic oscillator is an oscillator in which \\(\\mathbf{F} = -k \\mathbf{r}\\). That is, \\(F_x = -kx\\), F_y = -ky$, and \\(F_z = -kz\\). Then, with \\(\\omega = \\sqrt{k/m}\\), we see that \\(x(t) = A_x \\cos(\\omega t - \\delta_x)\\), and \\(y(t) = A_y \\cos(\\omega t - \\delta_y)\\). We can redefine the origin of time such that \\(\\delta_x = 0\\) and \\(\\delta_y = delta\\).</p> <p>Definition. An \\(anisotropic oscillator\\) is one in which \\(k_x \\neq k_y\\). Then, we see that \\(x(t) = A_x \\cos(\\omega_x t)\\) and \\(y(t) = A_y \\cos(\\omega_y t - \\delta)\\).</p>"},{"location":"physics/mechanics/5-oscillations/#section-54-damped-oscillations","title":"Section 5.4 - Damped Oscillations","text":"<p>We now consider a damping term, in which \\(\\mathbf{F}_{damping} = -b \\hat{\\mathbf{x}}\\). Then, we write</p> \\[m\\ddot{x} + b\\dot{x} + kx = 0\\] <p>An important note is that this similar to an RLC circuit, in which</p> \\[L\\ddot{q} + R\\dot{q} + \\frac{1}{C}q=0\\] <p>Definition. We will define a damping constant \\(2\\beta = \\frac{b}{m}\\). Additionally, we will rename the constant \\(\\omega_0^2 = k/m\\), which denotes the natural frequency, or the frequency the system operates at for \\(b = 0\\), that is, there are no resistive forces present. This allows us to rewrite the differential equation as</p> \\[\\ddot{x} + 2 \\beta \\dot{x} + \\omega_0^2 x = 0\\] <p>Solving the characteristic (or, as this textbook calls it, the auxiliary equation), we see that \\(r = -\\beta \\pm \\sqrt{\\beta^2 - \\omega_0^2}\\). Then, we can write \\(x(t) = e^{-\\beta t}(C_1 e^{\\sqrt{\\beta^2 - \\omega_0^2}t} + C_2 e^{-\\sqrt{\\beta^2 - \\omega_0^2}t})\\).</p> <p>Applying \\(\\beta = 0\\), we recover the solution for an undamped oscillator.</p> <p>Now, consider \\(\\beta \\leq \\omega_0\\), which we call underdamping. In this case, \\(\\sqrt{\\beta^2 - \\omega_0^2} = i\\sqrt{\\omega_0^2 - \\beta^2} = i \\omega_1\\), where \\(\\omega_1 = \\sqrt{\\omega_0^2 - \\beta^2}\\). We can then write \\(x(t) = e^{-\\beta t}(C_1 e^{i\\omega_1 t} + C_2 e^{-i\\omega_1 t}) = Ae^{-\\beta t}\\cos(\\omega_1 t - \\delta)\\).</p> <p>Conversely, when \\(\\beta &gt; \\omega_0\\), we see overcamping. Here, \\(x(t) = C_1 e^{-\\beta + \\sqrt{\\beta^2 - \\omega_0^2}} + C_2 e^{-\\beta - \\sqrt{\\beta^2 - \\omega_0^2}}\\).</p> <p>When \\(\\beta = \\omega_0\\), we are critically damped, in which case \\(x(t) = C_1 e^{-\\beta t} + C_2 t e^(-\\beta t)\\).</p>"},{"location":"physics/mechanics/5-oscillations/#section-55-driven-damped-oscillations","title":"Section 5.5 - Driven Damped Oscillations","text":"<p>Consider an external force \\(F(x)\\), such that \\(m\\ddot{x} + b\\dot{x} + kx = F(x)\\). Then, for \\(f(t) = F(t) / m\\),</p> \\[\\ddot{x} = 2\\beta\\dot{x} + \\omega_0^2 x = f(t)\\] <p>We can write the differential operator \\(D = \\frac{d^2}{dt^2} + 2\\beta \\frac{d}{dt} + \\omega_0^2\\), so that the differential equation becomes \\(Dx = f\\). Notably, this operator is linear.</p> <p>Definition. The equation \\(Dx = 0\\) is the homogenous equation.</p> <p>Definition. Some \\(x_p(t)\\) that satisfies \\(Dx = f\\) is a particular solution.</p> <p>Definition. Any \\(x_h(t)\\) that satisfies \\(Dx = 0\\) is a homogenous solution.</p> <p>Note that if \\(x_p\\) is a solution to a differential equation, by the linearity of \\(D\\), so is \\(x_p + x_h\\).</p> <p>Now consider \\(f(t) = f_0 \\cos(\\omega t)\\), or a driving force with frequency \\(\\omega\\). Then, by the method of undetermined coefficients, our particular solution is \\(x_p = A \\cos (\\omega t) + B \\sin (\\omega t)\\). However, we're not allowed to use that. We are only allowed to assume there is a second solution \\(y(t)\\) that satisfies \\(f(t) = f_0 \\sin(\\omega t)\\) instead that also satisfies the differential equation for the original function.</p> <p>Then, we set \\(z(t) = x(t) + i y(t)\\) and rewrite the differential equation in terms of \\(z\\), where \\(f(t) = f_0 e^{i \\omega t}\\). We can then try the solution \\(C e^{i \\omega t}\\) to see that \\(C = \\frac{f_0}{\\omega_0^2 - \\omega^2 + 2 i \\beta \\omega}\\). Then, we can define some \\(A, \\delta\\) such that \\(C = Ae^{-i \\delta}\\), and applying \\(A^2 = C C^*\\), we see that \\(A^2 = \\frac{f_0^2}{(\\omega_0^2 - \\omega^2)^2 + 4 \\beta^2 \\omega^2}\\), and \\(\\delta = \\arctan(\\frac{2\\beta \\omega}{\\omega_0^2 - \\omega^2})\\).</p> <p>We can then take the real part of \\(z\\) to see \\(x(t) = A \\cos(\\omega t - \\delta)\\).</p> <p>Definition. Any term that dies out exponentially as time passes is called transient. The motion that remains is called the attractor, as it can arise from many different initial conditions.</p>"},{"location":"physics/mechanics/5-oscillations/#section-56-resonance","title":"Section 5.6 - Resonance","text":"<p>When \\(\\beta\\) is small, we see the second term in the denominator becomes small. Then, if the driving and natural frequency are very different, the amplitude of the resulting wave becomes small. Conversely, if the driving and natural frequency are close together, the amplitude of the wave becomes large.</p> <p>Notably, if we try and force the oscillator to move at a frequency \\(\\omega\\), when close to \\(\\omega_0\\), the oscillator responds well, but fails to respond significantly otherwise. This is known as resonance.</p> <p>Then, we can see that \\(\\omega_2 = \\sqrt{\\omega_0^2 - 2 \\beta^2}\\) is the frequency at which the response is maximum. This then lets us see that \\(A_{max} \\approx \\frac{f_0}{2\\beta \\omega_0}\\).</p> <p>We can then calculate the FWHM, or full-width at half maximum, and the HWHM, or the half width at half maximum. These are the distance between the two points in which \\(A^2\\) is at half its maximum value. Note that \\(\\omega \\approx \\omega_0 \\pm \\beta\\), so \\(\\text {FWHM} \\approx 2\\beta\\) and \\(\\txt{HWHM} \\approx \\beta\\).</p> <p>We can then calculate the sharpness of the peak as the natural frequency over the FWHM, or \\(Q = \\frac{\\omega_0}{2\\beta} = \\pi \\frac{1 / \\beta}{2\\pi \\omega_0} = \\pi \\frac{\\text{decay time}}{\\text{period}}\\).</p> <p>Additionally, we see that \\(\\delta = \\arctan(\\frac{2\\beta \\omega}{\\omega_0^2 - \\omega^2})\\). At resonance, this implies that \\(\\delta = \\pi / 2\\).</p>"},{"location":"physics/mechanics/5-oscillations/#section-57-fourier-series","title":"Section 5.7 - Fourier Series","text":"<p>Skipped.</p>"},{"location":"physics/mechanics/5-oscillations/#section-58-fourier-series-solution-for-the-driven-oscillator","title":"Section 5.8 - Fourier Series Solution for the Driven Oscillator","text":"<p>If we set \\(f(t) = f_1(t) + f_2(t)\\), we can write \\(Dx_1 = f_1\\) and \\(Dx_2 = f_2\\) such that \\(D_x = D(x_1 + x_2) = f_1 + x_2 = f\\) so then \\(D_x = f\\).</p> <p>Then, we let \\(f(t) = \\sum_{n=0}^\\infty f_n \\cos(n \\omega t)\\), where \\(f_n\\) is a constant.</p> <p>We then find that \\(\\omega = 2\\pi / \\tau\\).</p> <p>We can then set \\(x_n(t) = A_n \\cos(n  \\omega t - \\delta_n)\\), where</p> \\[A_n = \\frac{f_n}{\\sqrt{(\\omega_0^2 - n^2 \\omegaa^2)^2 + 4\\beta^2n^2\\omega^2}}\\] <p>We also see that</p> \\[\\delta_n = \\arctan(\\frac{2\\beta n \\omega}{\\omega_0^2 - n^2 \\omega^2})\\]"},{"location":"physics/mechanics/5-oscillations/#section-59-the-rms-displacement-parsevals-theorem","title":"Section 5.9 - The RMS Displacement; Parseval's Theorem","text":"<p>We know that \\(x_{rms} = \\sqrt{\\langle x^2 \\rangle}\\). Then, we see that \\(\\langle x^2 \\rangle = \\frac{1}{\\tau} \\int_{-\\tau/2}^{\\tau/2}x^2 dt\\). In this case,</p> \\[\\langle x^2 \\rangle = \\frac{1}{\\tau} \\int_{-\\tau/2}^{\\tau/2} \\sum_n \\sum_m A_n \\cos(n\\omega t - \\delta_n) A_m \\cos(n \\omega t - \\delta_m) dt\\] <p>We can then apply identities to see that</p> \\[\\langle x^2 \\rangle = A_0^2 + \\frac{1}{2} \\sum_{n=1}^\\infty A_n^2\\] <p>This assumes that \\(A_0\\) is defined differently than \\(A_{n \\geq 1}\\).</p> <p>This is known as Parseval's theorem, and allows us to find the response of an oscillator.</p>"},{"location":"physics/mechanics/6-calculus-of-variations/","title":"Chapter 6 - Calculus of Variations","text":""},{"location":"physics/mechanics/6-calculus-of-variations/#section-61-two-examples","title":"Section 6.1 - Two Examples","text":"<p>Consider two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\), and some path \\(y = y(x)\\) joining them. Then, we know that the length of a path segment is \\(ds = \\sqrt{dx^2 + dy^2}\\). Then, we know \\(dy = \\frac{dy}{dx}dx = y'(x) dx\\), so we can rewrite \\(ds\\) as</p> \\[ds = \\sqrt{1 + y'(x)^2} dx\\] <p>This allows us to say that \\(L = \\int_1^2 ds = \\int_{x_1}^{x_2} \\sqrt{1 + y'(x)^2} dx\\). Now, to minimize \\(L\\),  we must find a value of \\(y'(x)\\) that makes the integral minimal.</p> <p>Now, consider light. The time \\(dt\\)  for light to travel \\(ds\\) is \\(ds/v\\) where \\(n = c / v\\) tells us that \\(dt = n / c ds\\). Then, we see that</p> \\[t = \\frac{1}{c} \\int_1^2 n ds\\] <p>Now, we assume that \\(n = n(x, y)\\), so then</p> \\[int_1^2 n(x, y)ds = \\int_{x_1}^{x_2} n(x, y) \\sqrt{1 + y'(x)^2} dx\\] <p>Thus, to find the shortest path, we must minimize again.</p> <p>We know that for some function \\(f(x)\\), a critical point is when \\(df / dx = 0\\). A critical point is a minimum depending on the sign of \\(df^2 / d^2x\\), or neither of it equals zero.</p> <p>Definition. A stationary point \\(x_0\\) is a critical point in which the minimum/maximum status is unknown.</p>"},{"location":"physics/mechanics/6-calculus-of-variations/#section-62-the-euler-lagrange-equation","title":"Section 6.2 - The Euler-Lagrange Equation","text":"<p>We have an integral of the form</p> \\[S = \\int_{x_1}^{x_2} f[y(x), y'(x), x] dx\\] <p>Here, \\(f\\) depends only on \\(x\\) and functions of \\(x\\).</p> <p>Now, let \\(y(x)\\) represent the path that minimizes \\(S\\). Then, we can say that any other path can be represented as \\(Y(x) = y(x) + \\alpha\\eta\\), with the boundary points forcing \\(\\eta(x_1) = \\eta(x_2) = 0\\). Then, when \\(\\alpha = 0, Y(x) = y(x)\\). We can then parameterize \\(S\\) as</p> \\[S(\\alpha) = \\int_{x_1}^{x_2} f(Y, Y', x) dx = \\int_{x_1}^{x_2} f(y + \\alpha \\eta, y' + \\alpha \\eta', x) dx\\] <p>Then, we can find \\(\\partial S / \\partial \\alpha\\) as \\(\\int \\partial f / \\partial \\alpha dx\\). Applying the chain rule, \\(\\partial f / \\partial \\alpha = \\eta \\frac{\\partial f}{\\partial y} + \\eta' \\frac{\\partial f}{\\partial y'}\\). Then, we can minimize \\(S\\) by setting its derivative to zero so that</p> \\[0 = \\frac{dS}{d\\alpha}\\int_{x_1}^{x_2}\\frac{\\partial f}{\\partial \\alpha}dx = \\int_{x_1}^{x_2}(\\eta \\frac{\\partial f}{\\partial y} + \\eta' \\frac{\\partial f}{\\partial y'})dx = 0\\] <p>Then, we can rewrite the second term by integrating by parts so that</p> \\[\\int_{x_1}^{x_2} = \\eta'(x) \\frac{\\partial f}{\\partial y'} dx = [\\eta(x) \\frac{\\partial f}{\\partial y'}]_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta(x) \\frac{d}{dx}(\\frac{\\partial f}{\\partial y'})dx\\] <p>The first term is zero due to the conditions on \\(\\eta(x)\\). Then, we see that</p> \\[\\int_{x_1}^{x_2} = \\eta'(x) \\frac{\\partial f}{\\partial y'} dx = - \\int_{x_1}^{x_2} \\eta(x) \\frac{d}{dx}(\\frac{\\partial f}{\\partial y'})dx\\] <p>We can substitute this into the above equation for the minimal location of \\(S\\) to see that</p> \\[\\int_{x_1}^{x_2} \\eta(x) (\\frac{\\partial f}{\\partial y} - \\frac{d}{dx} \\frac{\\partial f}{\\partial y'}) dx = 0\\] <p>This must be true for any \\(\\eta(x)\\). If we let \\(\\eta(x)\\) be an arbitrary function with the same sign as the differential terms over the integral, we see that the integral would be non-negative. This forces the differential terms to be zero.</p> <p>Definition. The Euler-Lagrange Equation. The following criteria holds true for all \\(f(y(x), y'(x), x)\\):</p> \\[\\frac{\\partial f}{\\partial y} - \\frac{d}{dx}\\frac{\\partial f}{\\partial y'} = 0\\]"},{"location":"physics/mechanics/6-calculus-of-variations/#section-63-applications-of-the-euler-lagrange-equation","title":"Section 6.3 - Applications of the Euler-Lagrange Equation","text":"<p>Recall the distance between two points, in which \\(f(y, y', x) = \\sqrt{1 + y'^2}\\). Then, the Euler-Lagrange Equation becomes \\(\\frac{d}{dx} \\frac{\\partial f}{\\partial y'} = 0\\), so \\(\\frac{\\partial f}{\\partial y'} = \\frac{y'}{\\sqrt{1 + y'^2}}\\) is a constant with regards to \\(x\\), which implies that \\(y'\\) is a constant and thus this is a straight line.</p> <p>Now consider a case where a fixed, frictionless track is placed in a gravitational field. What is the most efficient track from point \\(1\\) to \\(2\\), assuming some horizontal displacement?</p> <p>We know that due to conservation of energy, \\(v = \\sqrt{2gy}\\). From time equations, \\(t = \\int_1^2 \\frac{ds}{v}\\), so that \\(t = \\int_0^{y_2} \\frac{\\sqrt{x'(y)^2 + 1}}{\\sqrt{2gy}}dy\\). Then, \\(f(x, x', y) = \\frac{\\sqrt{x'^2 + 1}}{\\sqrt{y}}\\), and the Euler-Lagrange Equation tells us that \\(y = a(1 - \\cos \\theta)\\) and \\(x = a(\\theta - \\sin \\theta) + C\\), for some \\(\\theta\\).</p> <p>Note that in no case did we check that the paths found were minimal and not maximal. We  only know that these are statioary points.</p>"},{"location":"physics/mechanics/6-calculus-of-variations/#section-64-more-than-t-wo-variables","title":"Section 6.4 - More than T wo Variables","text":"<p>Consider \\(x = x(u), y = y(u)\\). Then, \\(ds = \\sqrt{x'(u)^2 + y'(u)^2}du\\), so that \\(L = \\int_{u_1}^{u_2} \\sqrt{x'(u)^2 + y'(u)^2}du\\). Then, \\(f = f(x, y, x', y', u)\\) so that \\(S = \\int_{u_1}^{u_2} f du\\). Now, if \\(x = x(u) + \\alpha \\xi(u)\\) and \\(y = y(u) + \\beta \\eta(u)\\), we see that \\(\\partial S / \\partial \\alpha = 0 = \\partial S / \\partial \\Beta\\), so that \\(\\frac{\\partial f}{\\partial x} = \\frac{d}{dx} \\frac{\\partial f}{\\partial x'}\\), and the same respectively with \\(y\\).</p> <p>Definition. Now, we can say that for a system with \\(n\\) coordinates, we have \\(q_1, q_2, \\ldots, q_n\\) position vectors, otherwise known as generalized coordinates. Then, we can think of \\(n\\) generalized coordinates as defining a point in an \\(n\\)-dimensional configuration space.</p>"},{"location":"physics/thermal/1-energy/","title":"Chapter 1 - Energy in Thermal Physics","text":""},{"location":"physics/thermal/1-energy/#section-11-thermal-equilibrium","title":"Section 1.1 - Thermal Equilibrium","text":"<p>Definition. The theoretical definition for temperature is the quantity that is the same for two objects when in thermal equilibrium.</p> <p>Definition. The time for a system to reach thermal equilibrium is the relaxation time.</p> <p>Definition. A substance is in diffusive equilibrium when the composition molecules of each substance is in equilibrium.</p> <p>Definition. A substance is in mechanical equilibrium if there is no net torque and no net force.</p> <p>Definition. Temperature is the measure of the tendency of an object to spontaneously give up energy to its surroundings.</p> <p>Definition. Absolute zero is the temperature at which the volume of an expanding gas should go to zero given constant pressure, or if volume is constant, pressure goes to zero.</p> <p>Definition. An absolute temperature scale is any temperature scale at which \\(0\\) is absolute zero.</p> <p>Definition. The SI absolute temperature unit is the kelvin.</p>"},{"location":"physics/thermal/1-energy/#section-12-the-ideal-gas","title":"Section 1.2 - The Ideal Gas","text":"<p>Theorem. Recall the Ideal Gas Law from chemistry, in which given \\(P = \\text{pressure}\\), \\(V = \\text{volume}\\), \\(n = \\text{number of moles of a gas}\\), \\(T = \\text{temperature in an absolute scale}\\), and \\(R = \\text{the ideal gas constant}\\),</p> \\[PV = nRT\\] <p>In SI units, \\(R = 8.31 \\frac{\\text{J}}{\\text{mol} \\cdot \\text{K}}\\).</p> <p>Definition. Recall that one mole of a substance is \\(6.022 \\times 10^{23}\\) units of said substance. This constant, \\(N_A\\), is Avogadro's Number.</p> <p>Using Avogadro's Number, we can rewrite the Ideal Gas law in terms of molecules, with \\(N = \\text{number of molecules of a gas}\\) and \\(n \\cdot N_A = N\\). Thus,</p> \\[PV = NkT\\] <p>for some constant \\(k = R / N_A\\).</p> <p>Definition. This constant \\(k = R / N_A\\) is Boltzmann's constant.</p> <p>Note that if the number of moles is constant, we can rewrite this as</p> \\[\\frac{P_1 V_1}{T_1} = \\frac{P_2 V_2}{T_2}\\]"},{"location":"physics/thermal/1-energy/#microscopic-model-of-an-ideal-gas","title":"Microscopic Model of an Ideal Gas","text":"<p>Consider a piston with length \\(L\\) with a piston area of \\(A\\). Then, the average pressure \\(\\overline{P}\\) can be defined as</p> \\[\\overline{P} = \\frac{\\overline{F_{x, \\text{piston}}}}{A} = - \\frac{\\overline{F_{x, \\text{on molecule}}}}{A}\\] <p>Now, consider some arbitrary molecule of gas with velocity \\(v\\). Then, we can apply \\(F = ma\\) to see</p> \\[\\overline{P} = -\\frac{m\\overline{a}}{A} = \\frac{m\\frac{\\overline{\\Delta v_x}}{\\Delta t}}{A}\\] <p>Now, let \\(\\delta t = 2L / v_x\\), or the time it takes for half an oscillate between the piston boundary in regards to the \\(x\\)-direction. Then, \\(\\delta v_x = -2v_x\\), as we are only considering acceleration due to the piston and not the chamber wall. Then,</p> \\[\\overline{P} = \\frac{mv_x^2}{AL} = \\frac{mv_x^2}{V}\\] <p>As velocity is a distribution in an ideal gas, with \\(N\\) as the sum of all molecules, we can rewrite this equation as</p> <p>\\(\\(PV = Nm\\overline{v_x^2}\\)\\),</p> <p>where \\(N\\) is the number of molecules and \\(\\overline{v_x^2}\\) is the expected value of the square of the velocity. Now, apply the ideal gas law to see that</p> \\[kT = m\\overline{v_x^2}\\] <p>Divide by \\(2\\) to see that</p> \\[\\frac{1}{2} kT = \\frac{1}{2} m\\overline{v_x^2}\\] <p>Summing over all directions, we see that</p> \\[\\frac{1}{2} m \\overline{v^2} = \\frac{1}{2} m (\\overline{v_x^2} + \\overline{v_y^2} + \\overline{v_z^2}) = \\frac{3}{2} k T\\] <p>This is the average translational kinetic energy for an ideal gas.</p> <p>Definition. A useful unit to measure energy on this scale is the electron-volt (eV), which is the kinetic energy gained by an electron that has been accelerated through a voltage difference of one volt. Note that \\(1 \\text{eV} = 1.6 \\times 10^{-19} \\text{J}\\).</p> <p>Note that the average speed in this model can be obtained as follows:</p> \\[\\overline{v^2} = \\frac{3kT}{m}\\] <p>Then, taking the square root results in</p> \\[v_\\text{rms} \\equiv \\sqrt{\\overline{v^2}} = \\sqrt{\\frac{3kT}{m}}\\]"},{"location":"physics/thermal/1-energy/#section-13-equipartition-of-energy","title":"Section 1.3 - Equipartition of Energy","text":"<p>We are familiar with energy in the form of \\(\\frac{1}{2}ab_{x, y, z}^2\\), where \\(a\\) is some fixed property of an object.</p> <p>Theorem. Equipartition Theorem. The average energy of any quadratic degree of freedom is \\(\\frac{1}{2}kT\\).</p> <p>If an object contains \\(N\\) molecules, each with \\(f\\) degrees of freedom, the total (average) thermal energy is</p> \\[U_\\text{thermal} = N \\cdot f \\cdot \\frac{1}{2}{k}{T} \\] <p>In monoatomic molecules, each molecule has \\(3\\) degrees of freedom, corresponding to the translational position. In diatomic gasses, there are \\(2\\) additional rotational degrees of freedom.</p> <p>Additionally, there exist modes of vibration, which each contribute two degrees of freedom (positional energy and vibrational kinetic energy). At room temperature, these are negligible in gasses. In solids, each atom may vibrate in three directions (as there are 3 translational axis), but atoms may not rotate, leading to \\(6\\) total degrees of freedom.</p> <p>In liquids, we are sad.</p>"},{"location":"physics/thermal/1-energy/#section-14-heat-and-work","title":"Section 1.4 - Heat and Work","text":"<p>We are familiar with energy, temperature, and work.</p> <p>Theorem. The Law of Conservation of Energy. Energy cannot be created or destroyed, only moved.</p> <p>Theorem. In thermodynamics, energy may only enter or leave a closed system via heat and work.</p> <p>Definition. Heat is any spontaneous flow of energy between two objects due to a difference in temperature.</p> <p>Definition. Work is any other flow of energy in or out of a system.</p> <p>With \\(U\\) being the total energy of a system, we can write</p> \\[\\Delta U = Q + W\\] <p>That is, the change in total energy of a system is equal to the heat being added to the system and work done on the system.</p> <p>Note that with heat engines, we often see \\(Q - W\\), where \\(W\\) instead represents the work done by the system.</p> <p>Theorem. The First Law of Thermodynamics. \\(\\Delta U = Q + W\\), along with the Law of Conservation of Energy. In  other words, you can't win the game.</p> <p>Definition. The SI unit of energy is the Joule, where \\(1J = 1 \\text{kg} \\cdot \\text{m}^2 / \\text{s}^2\\).</p> <p>Definition. The imperial unit of energy is the calorie, or the amount of energy to heat one gram of water by \\(1 \\degree \\text{C}\\). The exact conversion factor is defined as \\(1 \\text{J} = 4.184 \\text{cal}\\)</p> <p>Definition. Conduction is the transfer of heat by molecular contact, in which fast moving molecules bump into slow moving molecules and transfer energy.</p> <p>Definition. Convection is the transfer of heat by the bulk motion of a liquid or gas.</p> <p>Definition. Radiation is the transfer of heat via electromagnetic waves.</p>"},{"location":"physics/thermal/1-energy/#section-15-compression-work","title":"Section 1.5 - Compression Work","text":"<p>From classical mechanics, we know that \\(W = \\vec{F} \\cdot d\\vec{r}\\).</p> <p>Consider a piston with a compressible gas. In this case, with \\(\\Delta x\\) positive as the piston moves inwards, we can state that \\(W = F \\Delta x\\). Now, we want the gas to always maintain internal equilibrium. For this to be true, the piston must be moving relatively slowly. Note that any volume change that happens in this way is said to be quasistatic.</p> <p>We know that the force exerted by the piston is equal to the pressure times the area. Thus, \\(W = P A \\Delta  x\\). But \\(A\\Delta x\\) is just volume (in this case, negative, as volume is decreasing). So, \\(W = -P \\Delta V\\).</p> <p>However, this assumes constant pressure. For a non-constant pressure, \\(P(V)\\), we know that \\(W = \\int F dx = - \\int P(V) dV\\).</p> <p>Definition. In isothermal compression, we see that \\(\\Delta T = 0\\). Then, \\(T\\) is a constant, so \\(nRT\\) is constant meaning \\(PV\\) is constant. Then, as \\(P(V) = (P_0V_0)/(V)\\), we can see that \\(W = \\int_{V_1}^{V_2} P(V) dV = nRT \\ln(V_i/V_f) = PV \\ln(P_f/P_i)\\)</p> <p>Definition. The line formed by isothermal compression on a \\(PV\\)-diagram is called an isotherm.</p> <p>Definition. In adiabatic compression, we assume that \\(Q = 0\\). Then, we see that \\(PV^\\gamma\\) is constant, with \\(\\gamma\\) being the adiabatic constant. Note that as \\(U = \\frac{f}{2}NkT = W\\), we can see that \\(V_2 T_2^{f/2} = V_1 T_1^{f/2}\\), and \\(\\gamma = (f+2)(f)\\). Additionally, we see that \\(W\\) = \\(\\frac{P_2 V_2 - P_1 V_1}{1-\\gamma}\\)</p>"},{"location":"physics/thermal/1-energy/#section-16-heat-capacity","title":"Section 1.6 - Heat Capacity","text":"<p>Definition. The heat capacity \\(C = Q / \\Delta T\\) of an object is the energy required to raise its temperature by one degree.</p> <p>Definition. The specific heat capacity \\(c = C / m = C / (m \\Delta T)\\) is the energy needed to raise a substance by one degree per unit mass.</p> <p>In a system, we can note that</p> \\[C = \\frac{Q}{\\Delta T} = \\frac{\\Delta U - W}{\\Delta T}\\] <p>Consider the case in which volume is constant. Under these circumstances, we can calculate the heat capacity at constant volume, and as volume does not change, \\(W = 0\\), so</p> \\[C_V = (\\frac{\\Delta U}{\\Delta T})_V = (\\frac{\\partial U}{\\partial T})_V\\] <p>In the case where pressure is constant instead, we see that the heat capacity at constant pressure is defined as</p> \\[C_P = (\\frac{\\Delta U}{\\Delta T})_V = (\\frac{\\Delta  U - (- P \\Delta V)}{\\Delta T})_P = (\\frac{\\partial U}{\\partial T})_P + P (\\frac{\\partial V}{\\partial T})_P\\] <p>Knowing that \\(U = \\frac{1}{2}NfkT\\), we can see that \\(C_V = \\frac{NfK}{2}\\).</p> <p>Theorem. The Rule of Dulong and Pitt states that in solids, \\(f = 6\\), so the heat capacity per mole is \\(3R\\). In reality, all degrees of freedom freeze out as \\(T\\) approaches \\(0\\), so \\(C\\) approaches \\(0\\).</p> <p>For an ideal gas, at constant pressure,</p> \\[(\\frac{\\partial V}{\\partial T})_P = \\frac{\\partial}{\\partial T} (\\frac{NkT}{P}) = \\frac{Nk}{P}\\] <p>So, \\(C_P = C_V + Nk = C_V + nR\\).</p> <p>There are times when heat can be added without increasing the temperature at all, such as during a phase transformation. Then,</p> <p>Definition. The latent heat \\(L\\) of an object is the energy required to melt or boil a substance completely. We can also define specific latent heat \\(l\\) as</p> \\[l = \\frac{L}{m} = \\frac{Q}{m}\\] <p>Note that during this, we assume the pressure is constant and no other work is done aside from constant-pressure volume change. Additionally, the latent heat for freezing and boiling does not have to be (and almost never is) equal.</p> <p>Definition. We define enthalpy, \\(H\\), as the total energy needed to create any given system out of nothing in a set environment, and is defined as</p> \\[H = U + PV\\] <p>This can also be interpreted as the maximum possible energy extracted from annihilating the system, consisting of the system's internal energy \\(U\\) and the work \\(PV\\) done by the atmosphere to fill its absence.</p> <p>In a system, if the pressure is held constant, we see that \\(\\Delta H = \\Delta U + P \\Delta V\\). Thus, enthalpy can only increase due to expansion or internal energy changes. From the First Law of Thermodynamics, \\(\\Delta H = Q + W_{other}\\) if pressure is constant. Notably, the change in enthalpy per degree of temperature at a constant pressure is the same as the heat capacity at constant pressure \\(C_P\\).</p> <p>Definition. The Enthalpy of Formation is the energy needed to create a compound or undergo a phase transition from base constituents in their most stable states.</p>"},{"location":"physics/thermal/1-energy/#section-17-rates-of-processes","title":"Section 1.7 - Rates of Processes","text":"<p>Skipped.</p>"},{"location":"physics/thermal/2-second-law/","title":"Chapter 2 - The Second Law","text":""},{"location":"physics/thermal/2-second-law/#section-21-two-state-systems","title":"Section 2.1 - Two-State Systems","text":"<p>Definition. Given a system defined by a test statistic \\(XS\\) and positive integer \\(N\\), an ordered tuple with length \\(N\\) and elements in the range of \\(X\\) is known as a microstate. An unordered tuple with the same length \\(N\\) and elements in the range of \\(X\\) is known as a macrostate.</p> <p>Definition. The multiplicity of a macrostate is the number of possible microstates that, when when written as an unordered tuple, produce said macrostate. In this work, we will define \\(\\Omega(Macrostate) = Multiplicity\\).</p> <p>Note that if the test statistic \\(X\\) is a uniform and discrete test statistic, the probability of generating a given macrostate \\(m\\) can be written as \\(P(m) = \\frac{\\Omega(M)}{\\sum_M \\Omega M}\\).</p> <p>Recall. From Statistics, \\(C(n, k) = \\binom{n}{k}\\), or \\(n\\) choose \\(k\\), is the number of unordered pairs of length \\(k\\) that can be generated from a list of \\(n\\) distinct elements.</p> <p>Definition. A paramagnet is a material whose molecular magnetic moments do not align unless in the presence of an external magnetic field.</p> <p>Definition. A ferromagnet is a material whose molecular magnetic moments will be aligned in the presence of an external magnetic field and retain their alignment in its absence.</p> <p>Definition. The individual magnetic particles in a material are referred to as dipoles, as each contains a unique magnetic vector.</p> <p>Definition. In a two-state paramagnet, when exposed to a magnetic field, each dipole may only be parallel or antiparallel to the applied field. We denote \\(N = N_\\uparrow + N_\\downarrow\\) to represent the number of dipoles pointing up or down.</p> <p>Assuming the external magnetic field points up, we note that an up-dipole contains less energy than a down-dipole. The total energy of a system is determined by \\(N_\\uparrow\\) and \\(N_\\downarrow\\), so the macrostate of this system can be used to determine the total energy.</p>"},{"location":"physics/thermal/2-second-law/#section-22-the-einstein-model-of-a-solid","title":"Section 2.2 - The Einstein Model of a Solid","text":"<p>Consider a collection of microscopic systems that can each store any discrete amount of quantized energy. If each system is a harmonic oscillator, we know from Quantum Mechanics that the potential energy is \\(\\frac{1}{2}k_s x^2\\), where \\(k_s\\) is the spring constant. Then, the size of energy units is \\(hf\\), where \\(h\\) is Planck's constant (\\(h = 6.63 \\times 10^{-34} \\text{J} \\cdot \\text{s}\\)) and \\(f\\) is the natural frequency of the oscillator (\\(f = \\frac{1}{2\\pi} \\sqrt{k_s / m}\\)).</p> <p>Definition. For a three-dimensional solid, each particle can oscillate in three dimensions. Thus, if there are \\(N\\) oscillators, there are \\(N/3\\) particles. A solid modeled as such is known as an Einstein solid.</p> <p>Consider a system in which \\(N = 3\\). Then, \\(\\Omega(0) = 1\\), \\(\\Omega(1) = 3\\), \\(\\Omega(2) = 6\\), \\(\\Omega(3) = 10\\), and so on. We can extend this to see that for an Einstein solid with \\(N\\) oscillators, the multiplicity of the total energy \\(q\\) is</p> \\[\\Omega(N, q) = \\binom{q+N-1}{q} = \\frac{(q+N-1)!}{q!(N-1)!}\\]"},{"location":"physics/thermal/2-second-law/#section-23-interacting-systems","title":"Section 2.3 - Interacting Systems","text":"<p>Consider a system containing two solids, \\(A\\) and \\(B\\), that can share energy back and forth.</p> <p>Definition. Two solids are weakly coupled when the rate of energy transfer between the two solids is substantially less than the rate of energy transfer within the solids. That is, the total energy of each solid \\(U_A\\) and \\(U_B\\) only change slowly. Note that \\(U = U_A + U_B\\) is fixed.</p> <p>Consider a small system, in which \\(N_A = N_B = 3\\), with a total energy of \\(q_{total} = q_A + q_B = 6\\) (Note that the actual value of energy is \\(U = qhf\\)). There are seven possible macrostates of the system, each defined by \\(q_A \\in {0, 1, \\ldots, 6}\\) (as \\(q_B = 6 - q_A\\)). We can find the multiplicity of the overall macrostate by multiplying the multiplicity of \\(q_A\\) in solid \\(A\\) with the multiplicity of \\(q_B\\) in solid \\(B\\).</p> <p>Theorem. The fundamental assumption of statistical mechanics states that in an isolated system in thermal equilibrium, all microstates are equally probably. Note that this does not imply that macrostates are equally probably.</p> <p>Theorem. The second law of thermodynamics states that the spontaneous flow of energy stops when a system is at the macrostate with the greatest multiplicity.</p>"},{"location":"physics/thermal/2-second-law/#section-24-large-systems","title":"Section 2.4 - Large Systems","text":"<p>Definition. When dealing with large and small numbers, it is important to note that a small number may be added to a large number without significantly changing it.</p> <p>We can also define very large numbers, of which large or small numbers may be added or multiplied by each other and remain unchanged.</p> <p>Theorem. Stirling's Approximation. For some large \\(N \\in \\mathbb{N}\\), we can estimate \\(N! \\approx N^N e^{-N} \\sqrt{2\\pi N}\\). This comes from \\(N! \\approx N^N\\), with the additional correction terms \\(e^{-N}\\) and \\(\\sqrt{2\\pi N}\\). If we care about \\(\\ln(N!)\\), we can omit the last term to see \\(\\ln (N!) \\approx N \\ln N - N\\).</p> <p>We can use this to simplify \\(\\Omega(N, q) = \\binom{q + N - 1}{q} = \\frac{(q + N - 1)!}{q!(N-1)!} \\approx \\frac{(q+N)!}{q!N!}\\). Take the logarithm to see that \\(\\ln \\Omega = \\ln(\\frac{(q+N)!}{q!N!}) = \\ln((q+N)!) - \\ln(q!) - \\ln(N!)\\). We can now apply the simplification to see that \\(\\Omega \\approx (q + N)\\ln(q+N) - (q - N) - q\\ln q + q - N \\ln N + N = (q+N)\\ln(q+N) - q\\ln q - N\\ln N\\).</p> <p>We can factor \\(\\ln (q+N)\\) to see that \\(\\ln(q+N) = \\ln q + \\ln(1 + \\frac{N}{q})\\), and if \\(N \\gg q\\), we can use the Taylor series of \\(\\ln(x)\\) at \\(x_0 = 1\\) to see that \\(\\ln(q+N) \\approx \\ln q + \\frac{N}{q}\\) Thus, \\(\\ln \\Omega \\approx N \\frac{q}{N} + N + \\frac{N^2}{q}\\).</p> <p>Note that \\(N^2/q\\) becomes negligible. This, we can exponentiate to see that \\(\\Omega \\approx e^{N\\ln(q/N)}e^N = (\\frac{eq}{N})^N\\).</p> <p>Now, for a system of two large Einstein solids, we wish to know the width of the peak in the multiplicity function. We know that with \\(q = q_{total} = q_A + q_B\\),</p> \\[\\Omega = \\Omega(q_A) \\Omega(q_B) = (\\frac{eq_A}{N})^N (\\frac{eq_B}{N})^N = (\\frac{e}{N})^{2N}(q_A q_B)^N\\] <p>The highest peak will be at \\(q = 2 q_A\\), where \\(\\Omega_max = (\\frac{e}{N})^{2N} (\\frac{q}{2})^{2N}\\). If we instead let \\(q_A = \\frac{q}{2} + x\\) and \\(q_B = \\frac{q}{2} - x\\), we will see that \\(\\Omega = (\\frac{e}{N})^{2N}((\\frac{q}{2})^2 - x^2)^N\\). By taking the logarithm of the second factor and then applying simplifications, we can reduce this to \\(\\Omega = \\Omega_{max} e{-N (2x/q)^2}\\). This function should be familiar, as it is a Gaussian.</p> <p>Note that the multiplicity falls to \\(1/e\\) of its maximum value when \\(N(\\frac{2x}{q})^2 = 1\\), or when \\(x = \\frac{q}{2\\sqrt{N}}\\). This means that the approximate width of the peak is \\(q/\\sqrt{N}\\)</p> <p>Definition. The thermodynamic limit is when a system becomes infinitely large, so that measurable fluctuations away from the most likely microstate never occur.</p>"},{"location":"physics/thermal/2-second-law/#section-25-the-ideal-gas","title":"Section 2.5 - The Ideal Gas","text":"<p>Assume we have a single atom of a monoatomic gas with kinetic energy \\(U\\) in a container of volume \\(V\\). By considering a container with volume \\(2V\\), we can see that \\(\\Omega_1 \\propto V \\vdot \\V_p\\), where \\(V\\) is the volume of ordinary space, and \\(V_p\\) the  volume of momentum space.</p> <p>Definition. Momentum space is the space of all possible momentum values with axis \\(p_x, p_y, p_z\\).</p> <p>We know that since \\(KE = U = \\frac{1}{2}m(v_x^2 + v_y^2 + v_z^2) = \\frac{1}{2} \\frac{1}{m} (p_x^2 + p_y^2 + p_z^2)\\), we can write \\(2mU = p_x^2 + p_y^2 + p_z^2\\), which describes the surface of a sphere with radius \\(\\sqrt{2mU}\\). As such, we can say for a given total momentum \\(P\\), the multiplicity of momentum space is equal to the surface area of the sphere. However, this does not aid us in counting the total number of microstates as area is infinite.</p> <p>Definition. In Quantum Mechanics, the Heisenberg Uncertainty Principle states that \\((\\Delta x)(\\Delta p_x) \\approx h\\). Under this criteria, the number of independent waveforms is thus fixed.</p> <p>If the number of distinct position states is \\(L / (\\Delta x)\\), and number of distinct momentum states is \\(L_p / (\\Delta p_x)\\), then we know that the number of distinct sates is \\(\\frac{L}{\\Delta x} \\frac{L_p}{\\Delta x}\\), which becomes \\(\\frac{L L_p}{h}\\) by the uncertainty principle. Thus, when cubed, we see that \\(\\Omega_1 = \\frac{V V_p}{h^3}\\).</p> <p>Note that this is not a rigorous proof, and we have not shown that there are no additional factors for \\(\\Omega_1\\), such as a multiplicative factor of \\(2\\) to describe an additional dimension of freedom.</p> <p>Now, consider a two-molecule system. As we only fix total kinetic energy, the momentum constraint thus becomes \\(p_{1x}^2 + \\ldots + p_{2x}^2 + \\ldots = 2mU\\), assuming both molecules have the same mass. Then, we can write</p> \\[\\Omega_2 = \\frac{V^2}{h^6} \\times \\text{area of momentum 6-dimensional sphere}\\] <p>This only holds true, however, if the two molecules are somehow distinguishable. In reality, molecules are indistinguishable and swapping the two molecules will not yield a distinct state. Thus, we have over-counted by a factor of 2.</p> <p>If we have \\(N\\) indistinguishable molecules, we see that \\(\\Omega_n = \\frac{1}{N!} \\frac{V^N}{h^{3N}} \\times \\text{area of momentum N-dimensional sphere}\\). We can define this area as \\(\\frac{2\\pi^(N/2)}{(\\frac{N}{2} - 1)!} r^(N-1)\\), where \\(r\\) is \\(\\sqrt{2mU}\\). This derivation is left as a textbook appendix. Thus, we see that</p> \\[\\Omega_N = \\frac{1}{N!} \\frac{V^N}{h^{3N}} \\frac{2\\pi^{\\frac{3N}{2}}}{(\\frac{3N}{2} - 1)!} (\\sqrt{2mU})^{3N-1} \\approx \\frac{1}{N!} \\frac{V^N}{h^{3N}} \\frac{2\\pi^{\\frac{3N}{2}}}{(\\frac{3N}{2})!} (\\sqrt{2mU})^{3N}\\] <p>We can write this as \\(\\Omega(U, V, N) = f(N) V^N U^{\\frac{3N}{2}}\\) where \\(f(N)\\) is some function of \\(N\\).</p> <p>Now, suppose we have two ideal gasses separated by a partition that allows energy (but not mass) transfer. Then, \\(\\Omega = \\Omega_1 \\Omega_2\\). If \\(N_1 = N_2 = N\\), then \\(\\Omega = (f(N))^2 (V_A V_B)^N (U_A U_B)^{3N/2}\\). Following the last section, we see that the width of the peak of the probability distribution is \\(\\frac{U_{total}}{\\sqrt{\\frac{3N}{2}}}\\).</p> <p>If the partition is thus moveable (where one gas expands and the other compresses), allows energy exchange, and disallows mass transfer, we see that the width of the peak in distribution is \\(\\frac{V_{total}}{\\sqrt{N}}\\).</p> <p>If we allow mass transfer, we see that while the analysis becomes difficult, we would expect a sharp peak, and that \\(\\Omega\\) is \\(\\Omega(N_A, U_A)\\).</p>"},{"location":"physics/thermal/2-second-law/#section-26-entropy","title":"Section 2.6 - Entropy","text":"<p>We can rewrite the second law of thermodynamics as, given a system, the multiplicity of the current state tends to increase.</p> <p>Definition. We define Entropy as \\(S \\cong k \\ln \\Omega\\), where \\(k\\) is included due to historical reasons. This takes the very large number into an ordinary large number. While \\(\\Omega\\) is unitless, due to the factor of \\(k\\), we see that \\(S\\) has units of energy over temperature, or \\(J/K\\) in the SI system.</p> <p>Recall that in an Einstein solid with \\(N\\) oscillators and \\(q\\) energy units, we see that \\(\\Omega = \\binom{q + N - 1}{q}\\). If we introduce the constraint \\(q \\gg N\\) and apply Stirling's approximation, we see that \\(\\Omega \\approx (eq/N)^N\\), so that</p> \\[S = k \\ln \\Omega = Nk(\\ln \\frac{q}{N} + 1)\\] <p>Note that in this case, increasing either \\(q\\) or \\(N\\) increases the entropy of an Einstein solid.</p> <p>A convenient property of entropy is that the composite entropy of a total system is additive. That is,</p> \\[S = k \\ln \\Omega = k \\ln(\\Omega_A \\Omega_B) = k \\ln \\Omega_A + k \\ln \\Omega_B = S_A + S_B\\] <p>We can thus use this to rewrite the second law.</p> <p>Theorem. The Second Law of Thermodynamics. Any large system in equilibrium will be found in the macrostate with the greatest entropy (with some small, unmeasurable fluctuations).</p>"},{"location":"physics/thermal/2-second-law/#entropy-of-an-ideal-gas","title":"Entropy of an Ideal Gas","text":"<p>We know that, for an ideal gas,</p> \\[\\Omega \\approx \\frac{1}{N!} \\frac{V^N}{h^{3N}} \\frac{2\\pi^{\\frac{3N}{2}}}{(\\frac{3N}{2})!} (\\sqrt{2mU})^{3N}\\] <p>This is horrible, and nobody likes it. If we then apply Stirling's approximation and then discard some factors, we can see that</p> \\[S = Nk [ \\ln(\\frac{V}{N}(\\frac{4\\pi m U}{3Nh^2})^{3/2}) + \\frac{5}{2}]\\] <p>This is known as the Sackur-Tetrode Equation.</p> <p>A notable consequence of this is that if \\(N\\) and \\(U\\) are held fixed, \\(\\Delta S = Nk \\ln \\frac{V_f}{V_i}\\). This applies, for example, to quasistatic isothermal expansion work, in which heat is added to maintain constant pressure while undergoing expansion.</p> <p>Definition. Free expansion of a gas is a gas expanding from a container into a vacuum. Note that as nothing external to the system is being moved, \\(W=0\\), and as no heat is flowing, \\(Q=0\\), so \\(\\Delta U = Q + W = 0\\).</p> <p>Definition. Now, consider two gasses in containers of equal volume, in which \\(U_1 = U_2\\) and \\(N_1 = N_2\\). The, if \\(V_1 = V_2\\), when the containers are combined, \\(\\Delta S = Nk \\ln 2\\) for gas \\(A\\) and gas \\(B\\), so \\(\\Delta S_{total} = \\Delta S_A + \\Delta S_B = 2 Nk \\ln 2\\). This is known as the entropy of mixing. Note that this only applies if the gasses are distinguishable, as otherwise \\(\\Delta S = 0\\).</p> <p>Note that for distinguishable molecules, the \\(\\Omega\\) term lacks a factor of \\(1 / N!\\), so then \\(S = Nk[\\ln(V(\\frac{4\\pi m U}{3Nh^2})^{3/2}) + \\frac{3}{2}]\\). For this formula, dividing volume (and thus energy and number of molecules) by two results in the total entropy being larger than the sum of the partitions by a large factor. That is, by inserting a partition, the second law of thermodynamics can be violated. This is known as Gibbs Paradox.</p>"},{"location":"physics/thermal/2-second-law/#reversible-and-irreversible-processes","title":"Reversible and Irreversible Processes","text":"<p>Definition. A process is said to be irreversible if it creates new entropy, as to reverse it would be to violate the second law of thermodynamics. Processes that do not generate new entropy (or only generate a negligible amount) are said to be reversible.</p> <p>Notably, any reversible expansion is quasistatic, in which \\(W = -P \\Delta V\\). Note that a quasistatic process is only reversible if \\(Q = 0\\).</p>"},{"location":"physics/thermal/3-interactions-implications/","title":"Chapter 3 - Interactions and Implications","text":""},{"location":"physics/thermal/3-interactions-implications/#section-31-temperature","title":"Section 3.1 - Temperature","text":"<p>Consider two Einstein solids, \\(A\\) and \\(B\\), that are weakly coupled (that is, they can exchange energy, but the total amount of energy is fixed). If \\(N_A = 300\\) and \\(N_B = 200\\), with a total energy of \\(q = 100\\), we can compute the multiplicity of each macrostate as well as its entropy. By calculation, we can see that the most likely macrostate is when \\(q_A = 60\\). From the graph of \\(q_A\\) vs \\(\\Omega\\), we see the expected spike. Then, by the definition of the maximum, we see that</p> \\[\\frac{\\partial S_{total}}{\\partial q_A} = 0\\; \\text{ and } \\; \\frac{\\partial S_{total}}{\\partial U_A} = 0\\] <p>Note that the second equation is derived from the first, as \\(U_A\\) is simply \\(q_A\\) times a constant (as \\(q_A\\) is the number of energy quanta), and \\(S = k \\ln \\Omega\\). Furthermore, from \\(S_{total} = S_A + S_B\\), we can differentiate with respect to \\(U_A\\) to see that</p> \\[\\frac{\\partial S_{total}}{\\partial U_A} = \\frac{\\partial S_A}{\\partial U_A} + \\frac{\\partial S_B}{\\partial U_A} = 0\\] <p>Here, we note that for the second term, \\(dU_A = -dU_B\\). From this, we can see that at equilibrium,</p> \\[\\frac{\\partial S_A}{\\partial U_A} = \\frac{\\partial S_B}{\\partial U_B}\\] <p>Note that due Boltzmann's constant, these terms will have units of \\(1/\\text{K}\\).</p> <p>Definition. The temperature of a system is defined as \\(T = (\\frac{\\partial S}{\\partial U})^{-1}\\), with the volume and number of particles held fixed. That is,</p> \\[\\frac{1}{T} = (\\frac{\\partial S}{\\partial U})_{N,V}\\] <p>For a practical example, consider an Einstein solid in which \\(q \\gg N\\). Here, we say that \\(U = q\\epsilon\\). Then, we know that</p> \\[S = Nk[\\ln(\\frac{q}{N}) + 1] = Nk \\ln U - Nk\\ln(\\frac{UN}{q}) + Nk = Nk \\ln U - Nk \\ln (\\epsilon N) + Nk\\] <p>Then, we can see that \\(T = (\\frac{Nk}{U})^{-1}\\), so then \\(U = NkT\\), which given that \\(f = 2\\) for an Einstein solid, allows us to recover the Equipartition theorem.</p> <p>Now, consider a monatomic ideal gas. Then, \\(S = Nk \\ln V + Nk \\ln U^\\frac{3}{2} + f(N)\\). Then, \\(T = (\\frac{\\frac{3}{2}Nk}{U})^{-1}\\), so \\(U = \\frac{3}{2}NkT\\).</p>"},{"location":"physics/thermal/3-interactions-implications/#section-32-entropy-and-heat","title":"Section 3.2 - Entropy and Heat","text":"<p>If we differentiate \\(U(T)\\) with respect to \\(T\\) given a constant particle count and volume, by definition we obtain the heat capacity at constant volume. That is,</p> \\[C_V = (\\frac{\\partial U}{\\partial T})_{N, V}\\] <p>For an Einstein solid with \\(q \\gg N\\), we see that \\(C_V = \\frac{\\partial}{\\partial T}(NkT) = Nk\\), and for a monatomic ideal gas, \\(C_V = \\frac{\\partial}{\\partial T}(\\frac{3}{2}NkT) = \\frac{3}{2} Nk\\).</p> <p>Note that obtaining \\(C_V\\) from this is difficult, and nobody wants to do it.</p> <p>Even if entropy cannot be written mathematically due to the complexity of a system, it can still be measured. For a given system if a small amount of heat \\(Q\\) is added while holding volume constant and applying no work, we see that \\(\\frac{Q}{T} = \\frac{dU}{T} = dS\\). This is taken from the definition of temperature as given previously.</p> <p>If the temperature remains constant while heat is added (such as during a phase change), then the previous equation can be applied when \\(Q\\) and \\(dS\\) are not infinitesimal, so that \\(dS = \\frac{C_V dT}{T}\\). From this, if we allow temperature to vary, we see that</p> \\[\\Delta S = S_f - S_i = \\int_{T_i}^{T_f} \\frac{C_V}{T} dT\\] <p>Note that often, \\(C_V\\) is constant over the temperature range of interest and can be taken out of the integral. For certain materials, especially at low temperatures, this may not hold true. Additionally, if we know \\(C_V\\) all the way to absolute zero, we can find the total entropy. That is, \\(S = S_f - S(0) = S_f\\), as theoretically, \\(S(0) = 0\\).</p> <p>Theorem. The Third Law of Thermodynamics. At zero temperature, a system should settle into its unique lowest-energy state. That is, \\(\\Omega = 1\\) and \\(S = 0\\).</p> <p>However, in many materials, \\(S(0) \\neq 0\\). This is often due to the fact that orientations of molecules may be changed with very little effect on overall energy.</p> <p>Definition. Residual entropy, is the entropy at absolute zero. For compounds with multiple arrangements, it is equal to \\(k\\) times the logarithm times the number of possible molecular arrangements. For compounds with multiple isotopes, at \\(T = 0\\), the isotopes are often distributed randomly across lattice sites as the material is a solid (and thus the isotopes cannot flow between lattice sites). Additionally, some materials gain multiplicity due to the alignment of nuclear spins (and while these do align themselves eventually, most labs are not capable of getting that close to absolute zero).</p> <p>Note that by convention, tabulated entropies include residual entropy due to molecular orientations but neglect mixing or spin entropy.</p> <p>Note that to force the integral to converge at absolute zero, we force \\(\\lim_{T \\rightarrow 0} C_V = 0\\).</p> <p>Historically, \\(dS = Q/T\\) was the original definition of entropy. This tells us nothing about what entropy is, but is sufficient enough for use in most purposes.</p>"},{"location":"physics/thermal/3-interactions-implications/#section-33-paramagnetism","title":"Section 3.3 - Paramagnetism","text":"<p>Consider a two-state paramagnet, with \\(N\\) spin-\\(1/2\\) particles, in a constant magnetic field \\(\\mathbf{B} = B\\hat{\\mathbf{z}}\\). Then, with each particle acting as a dipole, and neglecting dipole-dipole interactions, we consider the two potential spin values, labeled here as up and down. Then, we can say the energy of any dipole is \\(\\pm \\mu B\\) for the material's value of \\(\\mu\\). If we let a dipole oriented with the field have a negative energy, we can then write the total energy as</p> \\[U = \\mu B(N_\\darr - N_\\uparrow) = U = \\mu B((N - N_\\uparrow) - N_\\uparrow) = \\mu B(N - 2N_\\uparrow)\\] <p>This holds true as \\(N = N_\\uparrow + N_\\darr\\). Additionally, this energy can be negative. In this material, we know there is no internal magnetic field, so \\(B = \\mu(H + M)\\) (we are not working with the symmetric case) simplifies so that</p> \\[M = \\mu(N_\\uparrow - N_\\darr) = -U/B\\] <p>Now, we can also consider the multiplicity of the system \\(\\Omega\\) as \\(\\Omega(N_\\uparrow) = \\binom{N}{N_\\uparrow}\\).</p> <p>A notable consequence of this is if we plot \\(x = U\\) vs \\(y = S\\), the temperature is the reciprocal of the slope (as \\(1/T = \\partial S / \\partial U\\)). We can then normalize this by writing \\(x = U / \\mu B\\) (as \\(B\\) is a constant external field) and \\(y = S / k\\). The resulting graph will appear to be a semicircle. Notably, we can find negative temperatures when more than half the dipoles point up.</p> <p>Analytically, Stirling's approximation tells us that</p> \\[S = k[N \\ln N - N_\\uparrow \\ln N_\\uparrow - (N - N_\\uparrow) \\ln(N - N_\\uparrow)]\\] <p>We then see that</p> \\[\\frac{1}{T} = (\\frac{\\partial S}{\\partial U})_{N, V} = \\frac{\\partial N_\\uparrow}{\\partial U} \\frac{\\partial S}{\\partial N_\\uparrow} = -\\frac{1}{2\\mu B} k \\ln(\\frac{N - U/\\mu B}{N + U / \\mu B})\\] <p>We can then solve to see that</p> \\[U = N \\mu B \\frac{1 - \\exp(2 \\mu B / kT)}{1 + \\exp(2 \\mu B / kT)} = -N \\mu B \\tanh(\\frac{\\mu B}{kT})\\] <p>Then, we see that \\(M = N \\mu \\tanh(\\frac{\\mu B}{kT})\\). Knowing that the heat capacity \\(C_B\\) can be written as \\(C_B =  (\\frac{\\partial U}{\\partial T})_{N, B}\\), we see that</p> \\[C_B = Nk \\frac{(\\mu B / kT)^2}{\\cosh(\\mu B / kT)}\\] <p>Note that for an electronic two-state paramagnet, the value of \\(\\mu\\) is the Bohr magneton, where \\(\\mu_B = \\frac{eh}{4\\pi m_e}\\). As \\(x = \\mu B / kT \\ll 1\\) in most cases, we can write \\(\\tanh x \\approx x\\), so that \\(M \\approx \\frac{N \\mu^2 B}{kT}\\). The fact that \\(M \\propto 1/T\\) is known as Curie's Law, and holds in the high temperature limit for all paramagnets.</p>"},{"location":"physics/thermal/3-interactions-implications/#section-34-mechanical-equilibrium-and-pressure","title":"Section 3.4 - Mechanical Equilibrium and Pressure","text":"<p>Consider two systems separated by a movable partition, in which energy and volume may be exchanged, but the total energy and volume is fixed. Then, at equilibrium, we see that \\(\\frac{\\partial S}{\\partial U_A} = 0 = \\frac{\\partial S}{\\partial V_A}\\). Then, we know that \\(S = S_A + S_B\\), so</p> \\[\\0 = \\frac{\\partial S}{\\partial V_A} = \\frac{\\partial S_A}{\\partial V_A} + \\frac{\\partial S_B}{\\partial V_A} = \\frac{\\partial S_A}{\\partial V_A} - \\frac{\\partial S_B}{\\partial V_B} \\Rightarrow \\frac{\\partial S_A}{\\partial V_A} = \\frac{\\partial S_B}{\\partial V_B}\\] <p>We can state this as \\(V = V_A + V_B\\) being fixed forces \\(0 = \\partial V_A + \\partial V_B\\) so then \\(\\partial V_B = -\\partial V_A\\). Then, at equilibrium, \\(T(\\partial S / \\partial V)\\) is the same for both systems. We then define pressure as \\(P = T (\\frac{\\partial S}{\\partial V})_{U, N}\\). For an ideal gas, with \\(\\Omega = f(N) V^N U^{3N/2}\\), we see that \\(P = \\frac{NkT}{V}\\) under this definition, which agrees with the ideal gas law.</p> <p>Now, consider a two-step process. In the first step, energy is changed by \\(\\Delta U\\) while volume is fixed. Then, in step two, volume is changed by \\(\\Delta V\\) while energy is fixed. Then, \\(\\Delta S = \\Delta S_1 + \\Delta S_2\\). We can write this as \\(\\Delta S = (\\frac{\\Delta S}{\\Delta U})_V \\Delta U + (\\frac{\\Delta S}{\\Delta V})_U \\Delta V\\). As we take the limit, we see that</p> \\[dS = (\\frac{\\partial S}{\\partial U})_V dU + (\\frac{\\partial S}{\\partial V})_U dV = \\frac{1}{T} dU + \\frac{P}{T} dV\\] <p>Theorem. This gives us the thermodynamic identity, in which \\(dU = T dS - P dV\\), which is true in any system.</p> <p>We can then recall that \\(dU = Q + W\\). Then if \\(W = -P dV\\), we can say \\(Q = T dS\\). However, this is only true if this is a quasistatic process, no other work is done, and many variables are held constant. Additionally, in a quasistatic process that is also adiabatic (\\(Q = 0\\)), we call the process isentropic. Then, \\(\\Delta S = \\frac{Q}{T}\\).</p> <p>For constant pressure processes, we can then write \\((\\Delta S)_P\\) = \\int_{T_i}^{T_f}\\frac{C_P}{T} dT$.</p> <p>Note that in non-quasistatic processes, \\(dS &gt; \\frac{Q}{T}\\), as \\(W &gt; -P dV\\).</p>"},{"location":"physics/thermal/3-interactions-implications/#section-35-diffusive-equilibrium-and-chemical-potential","title":"Section 3.5 - Diffusive Equilibrium and Chemical Potential","text":"<p>Now consider two systems, \\(A\\) and \\(B\\), that can also exchange particles. Then, we see that \\(\\partial S_A / \\partial N_A = \\partial S_B / \\partial N_B\\) at equilibrium. Then,</p> \\[ -T \\frac{\\partial S_A}{\\partial N_A} = -T \\frac{\\partial S_B}{\\partial N_B}\\] <p>We then define a chemical potential \\(\\mu\\) as</p> \\[\\mu = -T (\\frac{\\partial S}{\\partial N})_{U, V}\\] <p>Then, we can say that \\(\\mu_A = \\mu_B\\) at equilibrium. Otherwise, the system with the lesser chemical potential will tend to gain particles, as it will gain more entropy than the other loses. This makes intuitive sense, as the system with more energy tends to have more particles.</p> <p>Then, we know that</p> \\[dS = (\\frac{\\partial S}{\\partial U})_{N, V} dU + (\\frac{\\partial S}{\\partial V})_{N, U} dV + (\\frac{\\partial S}{\\partial N})_{U, V} dN\\] <p>We can then say that \\(dS = \\frac{1}{T} dU + \\frac{P}{T} dV - \\frac{\\mu}{T} dN\\), so that \\(dU = T dS - P dV + \\mu dN\\). Then, we tend to associate \\(\\mu dN\\) with chemical work.</p> <p>Then, if we fix \\(U\\) and \\(V\\), the system only has chemical work, so that \\(0 = T dS + \\mu dN\\), which implies \\(\\mu = -T (\\frac{\\partial S}{\\partial N})_{U, V}\\). Then, if we instead fix \\(S\\) and \\(V\\), we see that \\(dU = \\mu dN\\), which implies that \\(\\mu = (\\frac{\\partial U}{\\partial N})_{S, V}\\). This tells us that \\(\\mu\\) has units of energy.</p>"},{"location":"physics/thermal/3-interactions-implications/#section-36-summary-a-look-ahead","title":"Section 3.6 - Summary, A Look Ahead","text":"<p>Definition. In Classic Thermodynamics, we see that \\(dU = T dS - P dV + \\mu dN\\), where \\(\\frac{1}{T} = (\\frac{\\partial S}{\\partial U})_{V, N}\\), \\(\\frac{P}{T} = (\\frac{\\partial S}{\\partial V})_{U, N}\\), and \\(-\\frac{\\mu}{T} = \\frac{\\partial S}{\\partial N}_{U, V}\\).</p> <p>Additionally, we can find info for systems such as paramagnets using statistical mechanics.</p>"},{"location":"physics/thermal/4-engines-refrigerators/","title":"Chapter 4 - Engines and Refrigerators","text":""},{"location":"physics/thermal/4-engines-refrigerators/#section-41-heat-engines","title":"Section 4.1 - Heat Engines","text":"<p>Definition. A heat engine is any machine that absorbs heat and converts part of it into work. We model heat engines as accepting heat from a hot reservoir with temperature \\(T_h\\) and rejecting heat to a cold reservoir with temperature \\(T_c\\), as well as outputting work.</p> <p>Definition. A reservoir is any object so large that its temperature does not change as it accepts or rejects heat</p> <p>For a a heat engine, we will denote \\(Q_h\\) and \\(Q_c\\) to represent the heat absorbed from the hot reservoir and heat rejected to the cold reservoir. Then, the net work done by the engine is \\(W\\). In this model, all signs are positive.</p> <p>Definition. The efficiency \\(e\\) is the benefit/cost ratio. Fora  heat engine, we see that \\(e = \\frac{W}{Q_h}\\). As \\(\\Delta U = Q_h - Q_c - W\\) and \\(U\\) is a state variable (so \\(\\Delta U = 0\\) as this engines are cyclic), we know that \\(Q_h = W + Q_c\\), so that \\(W = Q_h - Q_c\\). Then, we can write \\(e = \\frac{W}{Q_h} = \\frac{Q_h - Q_c}{Q_h} = 1 - \\frac{Q_c}{Q_h}\\). We then see that efficiency is always in the range \\([0, 1]\\).</p> <p>By the second law of thermodynamics, \\(S_h \\geq S_c\\). We know that \\(S = \\frac{Q}{T}\\) for a reservoir, so \\(\\frac{Q_h}{T_h} \\geq \\frac{Q_c}{T_c}\\), which can be rewritten as \\(\\frac{T_c}{T_h} \\geq \\frac{Q_c}{Q_h}\\). Then, substituting into the equation for efficiency, \\(e \\geq 1 - \\frac{T_c}{T_h}\\). Note that actual efficiency will be less than this limit as entropy will be produced within the engine as well.</p> <p>Let us now revisit a classic: the Carnot cycle. This cycle consists of isothermal expansion of a gas at temperature \\(T_h\\), adiabatic expansion of the gas from \\(T = T_h\\) to \\(T = T_c\\), isothermal compression at \\(T = T_c\\), and adiabatic compression from \\(T = T_c\\) to \\(T = T_h\\). By applying the formula of the ideal gas, we see this cycle reaches the maximum efficiency of \\(e = 1 - \\frac{T_c}{T_h}\\). However, this engine is not very practical.</p>"},{"location":"physics/thermal/4-engines-refrigerators/#section-42-refrigerators","title":"Section 4.2 - Refrigerators","text":"<p>Definition. A refrigerator is a heat engine operated in reverse.</p> <p>Definition. The coefficient of performance is a fancy name for efficiency, in which</p> \\[\\text{COP} = \\frac{Q_c}{W} = \\frac{Q_c}{Q_h - Q_c} = \\frac{1}{Q_h/Q_c - 1}\\] <p>We can then apply the second law to see that</p> \\[\\text{COP} \\leq \\frac{1}{T_h/T_c - 1} = \\frac{T_c}{T_h - T_c}\\] <p>This textbook does not account for heat pumps.</p>"},{"location":"physics/thermal/4-engines-refrigerators/#section-43-real-heat-engines","title":"Section 4.3 - Real Heat Engines","text":"<p>An internal combustion engine is a classic example, in which the working substance is a gas.</p> <p>Definition. An internal combustion engine follows an Otto cycle, in which gas is compressed adiabatically by a piston. Then, during ignition, the temperature and pressure are raised while volume is constant, followed by a power stroke in which the gas expands and does work. Lastly, the gas is vented as pressure is held constant and volume drops.</p> <p>We can then show that \\(e = 1 - (\\frac{V_2}{V_1})^{\\gamma - 1}\\), where \\(V_1 / V_2\\) is the compression ratio. Unfortunately, if the compression ratio becomes too high, the gas will preignite spontaneously before compression is finished.</p> <p>Definition. In a diesel engine, only air is compressed, before fuel is sprayed into the engine when air temperature is high enough for ignition. The efficiency then becomes a function of the cutoff ratio.</p> <p>Definition. In a steam engine, a gas will follow the Rankine cycle, in which water is pumped to a high pressure, converted to a gas and expanded, sent through a turbine as it expands and pressure drops, and then condensed back to an initial volume. The efficiency then becomes a function of enthalpy (\\(H = U + PV\\)), where \\(e = \\frac{H_4 - H_1}{H_3 - H_2} \\approx 1 - \\frac{H_4 - H_1}{H_3 - H_1}\\).</p> <p>For a steam engine, we see steam tables, and everybody becomes unhappy.</p>"},{"location":"physics/thermal/4-engines-refrigerators/#section-44-real-refrigerators","title":"Section 4.4 - Real Refrigerators","text":"<p>A refrigerator is normally the reverse of a Rankine cycle. Notably, refrigerants are used instead of water due to the lower freezing and boiling temperatures. However, many are CFCs. We then see that</p> \\[\\text{COP} = \\frac{H_1 - H_4}{H_2 - H_3 - H_1 + H_4}\\] <p>Definition. The throttling or Joule-Thomson process is used in refrigerators, and is complex. This class skips it for now.</p> <p>I've also skipped the Liquefaction of Gasses and Towards Absolute Zero sections.</p>"}]}