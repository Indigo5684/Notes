{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal Notes Collection","text":""},{"location":"#textbook-reference","title":"Textbook Reference","text":"<p>Abstract Algebra: Abstract Algebra, Theory and Applications</p>"},{"location":"math/abstract-algebra/16-rings/","title":"Chapter 16 - Rings","text":""},{"location":"math/abstract-algebra/16-rings/#section-161-rings","title":"Section 16.1 - Rings","text":"<p>Definition. A nonempty set \\(S\\) is a ring if, with two binary operations called addition and multipllication, the following are satisfied:</p> <ol> <li>Addition is commutative. \\(a + b = b + a\\) for \\(a, b \\in R\\)</li> <li>Addition is associative. \\((a + b) + c = a + (b + c)\\) for \\(a, b, c \\in R\\)</li> <li>There exists a zero-element \\(0_R\\) in \\(R\\) such that \\(a + 0 = a\\) for all $a \\in $</li> <li>Every element \\(a\\) has an additive inverse \\(-a \\in R\\) such that \\(a + (-a) = 0_R\\)</li> <li>Multiplication is associative. That is, \\(a(bc) = (ab)c\\) for \\(a, b, c \\in R\\)</li> <li>The Distributive Property holds. That is, \\(\\forall a, b, c \\in R,\\)</li> </ol> \\[ a(b+c) = ab+bc \\\\ (a+b)c = ac + bc \\] <p>Definition. If there exists some element \\(1_R \\in R\\) such that \\(1a = a1 = a\\) for all \\(a \\in R\\), we say that \\(R\\) is a ring with unity or identity.</p> <p>Note that some books impose the condition that \\(1 \\neq 0\\). If \\(1 = 0\\), we can show the ring only has one element.</p> <p>Definition. If \\(ab = ba\\) for all \\(a, b \\in R\\), the ring is said to be a commutative ring.</p> <p>Definition. If a ring \\(R\\) is commutative, \\(R\\) is an integral domain if and only if for every \\(a, b \\in R\\), \\(ab = 0\\) implies that either \\(a = 0\\) or \\(b = 0\\).</p> <p>Definition. An element \\(a \\in R\\) is called a unit if there exists some \\(a^{-1}\\) such that \\(a a^{-1} = a^{-1} a = 1\\).</p> <p>Definition. A ring \\(R\\) with identity is called a division ring if every nonzero element in \\(R\\) is a unit.</p> <p>Definition. A commutative division ring is called a field. That is, in a field, every element has an inverse.</p> <p>Definition. A subset \\(S\\) of ring \\(R\\) is a subring if given any \\(r, s \\in S\\), then \\(rs \\in S\\) and \\(r - s \\in S\\).</p>"},{"location":"math/abstract-algebra/16-rings/#section-162-integral-domains-and-fields","title":"Section 16.2 - Integral Domains and Fields","text":"<p>Definition. If \\(R\\) is a commutative ring and \\(r \\in R\\), then \\(r\\) is said to be a zero divisor if there is some nonzero \\(s \\in R\\) such that \\(rs = 0\\).</p> <p>Definition. A commutative ring with no zero divisors is called an integral domain.</p> <p>Example. Consider the set \\(\\mathbb{Z}[i] = \\{m + ni | m, n \\in \\mathbb{Z}\\}\\). This ring is called the Gaussian integers. Prove that the Gaussian integers are not a field, and are an integral domain.</p> <p>Example. Proposition 16.15: Cancellation law. Let \\(D\\) be a commutative ring with identity. Then, \\(D\\) is an integral domain if and only if for every nonzero \\(a \\in R\\), \\(ab = ac\\) implies \\(b = c\\).</p> <p>Theorem. 16.16: Every finite integral domain is a field.</p> <p>Definition. For any non-negative integer \\(n \\in \\mathbb{N}\\) and \\(r \\in R\\), we say that \\(nr = r + \\ldots + r \\text{(n times)}\\).</p> <p>Definition. The charactaristic of a ring is the leat possible \\(n \\in \\mathbb{N}\\) such that \\(nr = 0\\) for all \\(r \\in R\\).</p> <p>Example. For every prime number \\(p\\), \\(\\mathbb{N}_p\\) is a field of charactaristic \\(p\\).</p> <p>Lemma. 16.18: Given \\(R\\) is a ring with identity, the charactaristic of \\(1\\) is the charactartistic of the field.</p> <p>Theorem. 16.19: The charactaristic of an integral domain is prime or zero.</p>"},{"location":"math/abstract-algebra/16-rings/#section-163-ring-homomorphisms-and-ideals","title":"Section 16.3 - Ring Homomorphisms and Ideals","text":"<p>Definition Given rins \\(R\\) and \\(S\\), and a mapping \\(\\varphi: R \\rightarrow S\\), we say that \\(\\varphi\\) is a ring homomorphism if the following are satisfied for all elements of \\(R\\):</p> \\[ \\begin{align}     \\varphi(a + b) &amp;= \\varphi(a) + \\varphi(b) \\\\     \\varphi(ab) &amp;= \\varphi(a) \\varphi(b) \\end{align} \\] <p>Definition. If \\(\\varphi\\) is one-to-one and onto, it is an isomorphism.</p> <p>Definition. For any ring homomorphism \\(\\varphi\\), the kernel of \\(\\varphi\\) is the set</p> \\[ \\ker \\varphi = \\{ r \\in R | \\varphi(r) = 0 \\} \\] <p>Definition. Proposition 16.22: Let \\(\\varphi: R \\rightarrow S\\) be a ring homomorphism. Then,</p> <ol> <li>If \\(R\\) is a commutative ring, then \\(\\varphi(R) \\subseteq S\\) is a commutative ring.</li> <li>\\(\\varphi(0_R) = 0_S\\)</li> <li>Let \\(1_R\\) and \\(1_S\\) be the identities in \\(R\\) and \\(S\\). If \\(\\varphi\\) is onto, then \\(\\varphi(1_R) = 1_S\\)</li> <li>If \\(R\\) is a field an \\(\\varphi(R) \\neq \\{0\\}\\), then \\(\\varphi(R) \\subseteq S\\) is a field.</li> </ol> <p>Definition. A subring \\(I \\subseteq R\\) is asn ideal of \\(R\\) if, when given \\(a \\in I, r \\in R\\), then \\(ar\\) and \\(ra\\) are both in \\(I\\). That is, \\(rI \\subseteq I\\) and \\(Ir \\subseteq I\\).</p> <p>Definition. Given a commutative ring \\(R\\) with identity, and \\(r \\in R\\), the set</p> \\[ \\langle a \\rangle = (r)R = \\{ ar : r \\in R \\} \\] <p>is an ideal in \\(R\\). Specifically, \\(\\langle a \\rangle\\) is a principal ideal.</p> <p>Example. Theorem 16.25. Every ideal in \\(\\mathbb{Z}\\) is a principal ideal.</p> <p>Examplee. With \\(\\varphi: R \\rightarrow S\\), \\(\\ker \\varphi\\) is an ideal of \\(R\\).</p> <p>Remark. 16.28: We are working with two-sided ideals. If rings are not commutative, we may deal with left ideals and right ideals.</p> <p>Theorem. 16.29: Let  \\(I\\) be an ideal of \\(R\\). Then, the factor/quotient ring \\(R/I\\) is a ring with multiplication defined by</p> \\[ (r + I)(s + I) = rs + I \\] <p>Theorem. 16.30: Let \\(I\\) be an ideal of \\(R\\). Then, the map \\(\\varphi: R \\rightarrow R/I\\) defined by \\(\\varphi(r) = r + I\\) is a ring homomorphism of \\(R\\) onto \\(R/I\\) with \\(\\ker \\varphi = I\\).</p> <p>Theorem. 16.31, First Isomorphism Theorem. Let \\(\\psi: R \\rightarrow S\\). Then, \\(\\ker \\psi\\) is an ideal of \\(R\\). Consider the isomorphism \\(\\varphi: R \\rightarrow R/\\ker \\psi\\). There exists an isomorphism \\(\\eta: R / \\ker \\psi \\rightarrow \\psi(R)\\) such that \\(\\psi = \\eta \\varphi\\).</p> <p>Theorem. 16.32, Second Isomorphism Theorem. Let \\(I\\) be a subring of \\(R\\) and \\(J\\) be an ideal of \\(R\\). Then, \\(I \\cap J\\) is an ideal of \\(I\\) and</p> \\[ I/I \\cap J \\cong (I + J) / J \\] <p>Theorem. 16.33, Third Isomorphism Theorem. Let \\(R\\) be a ring and \\(I, J\\) be ideals of J. If \\(J \\subsetneq I\\), then</p> \\[ R/I \\cong \\frac{R/J}{I/J} \\] <p>Theorem. 16.34, Correspondence Theorem. Let \\(I\\) be an ideal of \\(R\\). Then, \\(S \\mapsto S/I\\) is a one-to-one correspeondence between the set of subrings \\(S\\) containing \\(I\\) (that is, \\(I \\in S\\)) and the set of subrings of \\(R/I\\). Furthermore, the ideals of \\(R\\) containing \\(I\\) correspond to the ideals of \\(R/I\\).</p>"},{"location":"math/abstract-algebra/16-rings/#section-164-maximal-and-prime-ideals","title":"Section 16.4 - Maximal and Prime Ideals","text":"<p>Definition. Consider ring \\(R\\) and proper ideal \\(M \\subseteq R\\). Then, \\(M\\) is a maximal ideal of \\(R\\) if the ideal \\(M\\) is not a subset of any ideal except \\(R\\) itself. That is, given any ideal \\(I\\) properly containing \\(M\\), \\(I = R\\).</p> <p>Theorem. 16.35: Given a commutative ring with identity \\(R\\), \\(M\\) is a maximal ideal if and only if \\(R/M\\) is a field.</p> <p>Definition. Consider ring \\(R\\) and proper ideal \\(P \\subseteq R\\). Then, \\(P\\) is a prime ideal if given \\(ab \\in P\\), either \\(a \\in P\\) or \\(b \\in P\\).</p> <p>Theorem. 16.38: Let \\(R\\) be a commutative ring with identity \\(1\\). Then, \\(P \\subseteq R\\) is a prime ideal of \\(R\\) if and only if \\(R/P\\) is a field.</p> <p>Let us assume that \\(P\\) is an ideal in \\(R\\) and \\(R/P\\) is an integral domain. Take two elements \\(ab \\in P\\). Now, consider \\(a + P\\) and \\(b + P\\) in \\(R/P\\) such that \\((a+P)(b+P) = 0+P = P\\). As \\(R/P\\) is a field, either \\(a + P = 0 + P = P\\) or \\(b + P = 0 + P = P\\), meaning either \\(a \\in P\\) or \\(b \\in P\\). Thus, \\(P\\) is as prime ideal.</p> <p>Now, assume the opposite. Let \\(P\\) be prime. Now, we want to show that \\(R/P\\) is an integral domain.</p> <p>Consider two elements \\(a + P\\), \\(b + P\\) in \\(R/P\\). We know that</p> \\[ (a + P)(b + P) = ab + P = 0 + P = P \\] <p>Thus, \\(ab \\in P\\). By symnetry, assume \\(a \\notin P\\). Thus, \\(b \\in P\\) by the  devinition of a prime ideal, so \\(b + P = 0 + P\\), meaning \\(R/P\\) is an integral domain.</p> <p>Theorem. 16.40: In a commutative ring with identity, every maximal ideal is also a prime ideal.</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/","title":"Chapter 17 - Polynomial Rings","text":""},{"location":"math/abstract-algebra/17-polynomial-rings/#section-171-polynomial-rings","title":"Section 17.1 - Polynomial Rings","text":"<p>Throughout this chapter, we will assume that \\(R\\) is a commutative ring with identity.</p> <p>Definition. Any expression of the form</p> \\[ f(x) = \\sum_{i=0}^n a_i x^i = a_0 + a_1x + a_2x^2 + \\ldots + a_n x^n \\] <p>where \\(a_i \\in R\\) and \\(a_n \\neq 0\\) is called a polynomial over \\(R\\) with indeterminate \\(x\\). The elements \\(a_0, a_1, \\ldots, a_n\\) are the coefficients of \\(f\\). The coefficient \\(a_n\\) is the leading coefficient.</p> <p>Definition. A polynomial is known as monic if the leading coefficient is equal to \\(1\\).</p> <p>Definition. The degree of \\(f\\) is the largest nonnegative number such that \\(a_n \\neq 0\\), written as \\(\\deg f(x) = n\\). If no such mumber exists, that is, \\(f(x) = 0\\), we say the degree of \\(f\\) is $-\\infty%.</p> <p>Definition. We denote the set of all polynomials with coefficients in \\(R\\) as \\(R[x]\\).</p> <p>Two polynomials are equal if and only if their corresponding coefficients are equal. When combined with standard addition and multiplication, \\(R[x]\\) forms a ring.</p> <p>Theorem. If \\(R\\) is commutative and has identity, so does \\(R[x]\\).</p> <p>Definition. The ring of polynomials with \\(n\\) indeterminates and coefficients in \\(R\\) is defined as \\(R[x_1][x_2][\\ldots][x_n] = R[x_1, x_2, \\ldots, x_n]\\).</p> <p>Definition. The evaluation homomorphism is the homomorphism \\(\\varphi: R[x] \\rightarrow R\\) defined as \\(\\varphi(p(x)) = p(\\alpha)\\) for some \\(\\alpha \\in R\\).</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/#section-172-the-division-algorithm","title":"Section 17.2 - The Division Algorithm","text":"<p>Theorem. Given \\(f(x), g(x) \\in F[x]\\), where \\(F\\) is a field and \\(g(x) \\neq 0\\), there exist unique polynomials \\(q(x), r(x) \\in F[x]\\) such that</p> \\[ f(x) = g(x)q(x) + r(x) \\] <p>where either \\(\\deg r(x) &lt; \\deg g(x)\\) or \\(r(x)\\) is the zero polynomial.</p> <p>Collary. Let \\(F\\) be a field. Then, an element \\(\\alpha \\in F\\) is a zero of \\(p(x) \\ in F[x]\\) if and only if \\((x-\\alpha)\\) is a factor of \\(p(x)\\).</p> <p>Collary. Let \\(F\\) be a field. Then, a nonzero polynomial \\(p(x) \\in F[x]\\) with degree \\(n\\) can have at most \\(n\\) distinct zeros in \\(F\\).</p> <p>Definition. A monic polynomial \\(d(x)\\) is the greatest common divisor of polynomials \\(p(x), q(x) \\in F[x]\\) if \\(d(x)\\) evenly divides both \\(p(x)\\) and \\(q(x)\\). We write \\(\\gcd(p(x), q(x)) = d(x)\\). This polynomial is unique.</p> <p>Definition. Two polynomials are relatively prime if their greatest common divisor is \\(1\\).</p>"},{"location":"math/abstract-algebra/17-polynomial-rings/#section-173-irreducible-polynomials","title":"Section 17.3 Irreducible Polynomials","text":"<p>Definition A nonconstant polynomial \\(f(x) \\ in F[x]\\) is irreducible over a field \\(F\\) if it cannot be expressed as the product of two non-identity polynomials \\(g(x)\\) and \\(h(x)\\) in \\(F[x]\\), with the degree of both polynomials strictly less than the  degree of \\(f(x)\\).</p> <p>Lemma. Let \\(p(x) \\in \\mathbb{Q}[x]\\). Then, with \\(r, s \\in \\mathbb{Z}, a(x) \\in \\mathbb{N}[x]\\), we can write \\(p(x) = \\frac{r}{s} a(x)\\).</p> <p>Lemma. Gauss's Lemma. Let \\(p(x) \\in \\mathbb{Z}[x]\\) be monic such that \\(p(x)\\) factors into two polynomials \\(\\alpha(x), \\beta{x} \\in \\mathbb{Q}[x]\\), with the degrees of both strictly less than the degree of \\(p(x)\\). Then, there exists two polynomials \\(a(x), b(x) \\in \\mathbb{Z}[x]\\) such that \\(p(x) = a(x)b(x)\\), and \\(\\deg \\alpha(x) = \\deg a(x)\\) and \\(\\deg \\beta(x) = \\deg b(x)\\).</p> <p>Collary. Let \\(p(x) \\in \\mathbb{Z}[x]\\) be monic with constant term \\(a_0\\). Then, if \\(p(x)\\) has a zero in \\(\\mathbb{Q}\\), then it also has a zero \\(\\alpha\\) in \\(\\mathbb[Z]\\). Furthermore, \\(\\alpha\\) divides \\(a_0\\).</p> <p>Theorem. Eisenstein's Criterion. Let \\(p\\) be prime, and suppose that</p> \\[ f(x) = a_n x^n + \\ldots + a_0 \\in \\mathbb{Z}[x] \\] <p>Then, if \\(p | a_i\\) for \\(0 \\leq i &lt; n\\), but \\(p \\nmid a_n\\) and \\(p^2 \\nmid a_0\\), then \\(f(x)\\) is irreducible over \\(\\mathbb{Q}[x]\\).</p> <p>Theorem. If \\(F\\) is a field, then every ideal in \\(F[x]\\) is a principal ideal.</p> <p>Theorem. Let \\(F\\) be a field, and suppose \\(p(x) \\in F[x]\\). Then, the ideal \\(&lt;p(x)&gt;\\) is maximal if and only if \\(p(x)\\) is irreducible.</p>"},{"location":"math/abstract-algebra/18-integral-domains/","title":"Chapter 18 - Integral Domains","text":""},{"location":"math/abstract-algebra/18-integral-domains/#section-181-fields-of-fractions","title":"Section 18.1 - Fields of Fractions","text":"<p>Definition. Given an integral domain \\(D\\), we can construct a field \\(F\\) containing \\(D\\) by stating that any \\(p/q \\in F\\), annd that any two elements \\(a/b = c/d\\) if and only if \\(ad = bc\\). We can consider this akin o a set of ordered pairs</p> \\[ S = \\{(a, b) : a, b \\in D \\text{ and } b \\neq 0 \\} \\] <p>Lemma. 18.1: The relation \\((a, b) ~ (c, d) \\text{ if } ad = bc\\) is an equivalence relation.</p> <p>Lemma. 18.2: The operations of addition and multiplication on \\(F\\) are well-defined.</p> <p>Lemma. 18.3: The set of equivalence classes of \\(S, F\\) under \\(~\\) form a field.</p> <p>Theorem. 18.4: Let \\(D\\) be an integral domain. Then, \\(D\\) can be embedded in a field of fractions \\(F_D\\) where any element in \\(F_D\\) can be expressed as the quotient of two elements in \\(D\\).</p> <p>Additionally, \\(F_D\\) is unique. That is, given field \\(E\\) such that \\(E \\supset D\\), there exists a map \\(\\psi: F_D \\rightarrow D\\) giving an isomorphism such that \\(\\psi(a) = a\\) for all \\(a \\in D\\).</p> <p>Collary. 18.6: Let \\(F\\) be a field of charactaristic \\(0\\). Then, \\(F\\) contains a subfield isomorphic to \\(\\mathbb{Q}\\).</p> <p>Collary. 18.6: Let \\(F\\) be a field of charactaristic \\(p\\). Then, \\(F\\) contains a subfield isomorphic to \\(\\mathbb{Z}_p\\).</p>"},{"location":"math/abstract-algebra/18-integral-domains/#section-182-factorization-in-integral-domains","title":"Section 18.2 - Factorization in Integral Domains","text":"<p>Definition. Let \\(R\\) be a commutative ring with identity, and \\(a, b \\in R\\). We say that \\(a\\) divideds \\(b\\), that is, \\(a | b\\), if there exists some \\(c \\in R\\) such that \\(b = ac\\).</p> <p>Definition. A unit element is any element that has a multiplicative inverse.</p> <p>Definition. Two elements \\(a, b \\in R\\) are said to be associates if there exists some unit \\(u \\in R\\) such that \\(a = ub\\).</p> <p>Definition. Let \\(D\\) be an integral domain. A nonzero element \\(p \\in D\\) is said to be irreducible if when given \\(p = ab\\), either \\(a\\) or \\(b\\) is a unit.</p> <p>Definition. Let \\(D\\) be an integral domain. A nonzero element \\(p\\) is prime if when given \\(p = ab\\), either \\(p | a\\) or \\(p | b\\).</p> <p>Definition. Given integral domain \\(D\\), we say that \\(D\\) is a Unique Factorization Domain (UFD) if it satisfies the following criteria:</p> <ol> <li>Given \\(a \\in D, a \\neq 0\\), and \\(a\\) is not a unit, \\(a\\) can be written as a product of irreducible elements in \\(D\\).</li> <li>Let \\(a = p_1 \\ldots p_r = q_1 \\ldots q_s\\), where \\(p_i\\) and \\(q_i\\) are all irreducible. Then, \\(r = s\\), and there exists some fuction \\(\\pi \\in S_r\\) such that \\(p_i\\) and \\(q_{\\pi(j)}\\) are associates for \\(j = 1, \\ldots, r\\).</li> </ol> <p>Definition. A ring \\(R\\) is a principal ideal domain (PID) if every ideal of \\(R\\) is principal.</p> <p>Lemma. 18.11: Let \\(D\\) be an integral domain and \\(a, b \\in D\\). Then,</p> <ol> <li>\\(a | b\\) if and only if \\(\\langle b \\rangle \\subseteq \\langle a \\rangle\\)</li> <li>\\(a\\) and \\(b\\) are associates if and only if \\(\\langle b \\rangle = \\langle a \\rangle\\)</li> <li>\\(a\\) is a unit in \\(D\\) if and only if \\(\\langle a \\rangle = D\\).</li> </ol> <p>Theorem. 18.12: Let \\(D\\) be a PID, and let \\(\\langle p \\rangle\\) be a nonzero ideal in \\(D\\). Thus, \\(\\langle p \\rangle\\) is a maximal ideal if and only if \\(p\\) is irreducible.</p> <p>Collary. 18.13: Let \\(D\\) be a PID. For any \\(p \\in D\\), if \\(p\\) is irreducible, then \\(p\\) is prime.</p> <p>Lemma. 18.14: Let \\(D\\) be a PID. Let \\(I_1 \\subseteq I_2 \\subseteq \\ldots\\). Then, there exists some integer \\(N\\) such that \\(I_n = I_N\\) for all \\(n &gt; N\\). That is, any chain of ideals converges.</p> <p>Definition. Any commutative ring that satisfies the above condition (the ascending chain condition), even if it's not a PID, is called a Noetherien ring.</p> <p>Theorem. 18.15: Every PID is a UFD. Note that the converse is not true.</p> <p>Collary 18.16: Let \\(F\\) be a field. Then, \\(F[x]\\) is a UFD.</p> <p>Definition. Any integral domain \\(D\\) is a euclidian domain with a euclidian function \\(nu: D \\\\ \\{0\\} \\rightarrow \\mathbb{N}\\) that satisfies the following:</p> <ol> <li>Given \\(a, b \\neq 0\\), then \\(\\nu(a) \\leq \\nu(ab)\\).</li> <li>Given, \\(a, b \\in D\\) and \\(b \\neq 0\\), there exists some \\(q, r \\in D\\) such that \\(a = bq + r\\) and either \\(r = 0\\) or \\(\\nu(r) &lt; \\nu(b)\\).</li> </ol> <p>Example. Absolute value on \\(\\mathbb{Z}\\) is a Euclidian validation.</p> <p>Example. Degree on \\(F[x]\\) is a Euclidian validation.</p> <p>Example. \\(\\nu(a + bi) = a^2 + b^2\\) is a Euclidian validation over \\(\\mathbb{Z}[i]\\).</p> <p>Theorem. 18.21: Every Euclidian domain is a PID.</p> <p>Collary. Every Euclidian domain is a UFD.</p> <p>Definition. Given a polynomial \\(p(x) \\in D\\), with \\(D\\) bein an integer domain, we say that the content of \\(p(x)\\) is the greatest common divisor of its coefficients. Additionally, if the content is \\(1\\), we say that \\(p(x)\\) is primitive.</p> <p>Theorem. 18.24: Let \\(D\\) be a UFD, and \\(f(x), g(x) \\in D[x]\\) be primitive. Then, \\(f(x)g(x)\\) is primitive.</p> <p>Lemma. 18.25: Given \\(D\\) is a UFD, and \\(p(x), q(x) \\in D[x]\\), the content of \\(p(x)q(x)\\) is equal to the product of the contents of the individual polynomials</p> <p>Lemma. 18.26: Let \\(D\\) be a UFD and \\(F = F_D\\) be its field of fractions. Given \\(p(x) \\in D[x]\\), and \\(p(x) = f(x)g(x)\\) with \\(f(x), g(x) \\in F_D\\), we can say that \\(p(x) = f_1(x)g_1(x)\\) with \\(f_1(x), g_1(x) \\in D\\). Additionally, \\(\\deg f_1(x) = \\deg f(x)\\) and \\(\\deg g_1(x) = \\deg g(x)\\).</p> <p>As a direct consequence, we see the following.</p> <p>Collary. Let \\(D\\) be a UFD, and \\(F = F_D\\). Then, a primitive polynomial \\(p(x) \\in D[x]\\) is irreducible in \\(D[x]\\) if and only if it is irreducible in \\(F[x]\\).</p> <p>Collary. Let \\(D\\) be a UDF, and \\(F = F_D\\). Then, if a monic polynomial \\(p(x) \\ in D[x]\\) can be written as \\(p(x) = f(x)g(x)\\) with \\(f(x), g(x) \\in F_D[x]\\), then \\(p(x)\\) can be written as \\(p(x) = f_1(x)g_1(x)\\), where \\(f_1(x), g_1(x) \\in D[x]\\).</p> <p>Theorem. If \\(D\\) is as UFD, then \\(D[x]\\) is a UFD.</p> <p>Collary. This theorem has several collaries:</p> <ol> <li>Given a field \\(F\\), since \\(F\\) is a PID, it is also a UFD. Thus, \\(F[x]\\) is a UFD.</li> <li>The ring of polynomials over integers, \\(\\mathbb{Z}[x]\\) is a UFD.</li> <li>Given \\(D\\) is a UFD, \\(D[x]\\) is a UFD. Thus, \\(D[x_1, x_2]\\) is a UFD, and by induction, \\(D[x_1, \\ldots, x_n]\\) is a UFD.</li> </ol>"},{"location":"math/abstract-algebra/DF-10-modules/","title":"Dummit &amp; Foote Chapter 10 - Modules","text":""},{"location":"math/abstract-algebra/DF-10-modules/#section-101-basic-definitions-and-examples","title":"Section 10.1 - Basic Definitions and Examples","text":"<p>Definition. Let \\(R\\) be a ring. A left \\(R\\)-module or a left module over \\(R\\) is a nonempty set \\(M\\) together with</p> <ol> <li>A binary operation \\(+\\) on \\(M\\) under which \\(M\\) is an abelian group</li> <li>An action \\(\\cross\\) of \\(R\\) on \\(M\\), that is, a map or function \\(R \\cross M \\rightarrow M\\), denoted \\(rm\\), that for all \\(r, s \\in R, m, n \\in M\\) satisfies<ul> <li>\\((r + s)m = rm + sm\\)</li> <li>\\((rs)m = r(sm)\\)</li> <li>\\(r(m + n) = rm + rn\\)</li> <li>If \\(R\\) has identity \\(1\\), then \\(1m = m\\)</li> </ul> </li> </ol> <p>Theorem. If \\(R\\) is commutative, any left-module is also a right-module.</p> <p>Remark. Modules over a field \\(F\\) and vector spaces over \\(F\\) are identical.</p> <p>Definition An R-submodule is a subset\\(N \\subseteq M\\) which is closed under the action taken forall \\(r \\in R\\). That is, given \\(r \\in R, n \\in N\\), tthen \\(rn \\in N\\). Every module has at least two submodules: itself and the trivial (empty) submodule.</p> <p>Remark. If \\(F\\) is a field, submodules are equivilent to subspaces.</p> <p>Example. Let \\(F\\) be a field and \\(F[x]\\) a polynomial ring. Then, let \\(V\\) be a vector space of \\(F\\), and \\(T\\) be a linear transformaion from \\(V\\) to itself. That is, \\(V: T \\rightarrow T\\). We know that \\(V\\) is an \\(F\\)-module. We will want to show that \\(V\\) can be written as an \\(F[x]\\)-module for some choice of \\(T\\). That is, we want an action \\(F[x] \\cross V \\rightarrow V\\).</p> <p>Now, for a given linear transformation \\(T\\), consider some polynomial \\(p(x) = a_n x^n + \\ldots + a_0\\) and some \\(v \\in V\\). We define \\(p(x) \\cross v\\) by$</p> \\[ p(x) \\cross v = a_n T^n(v) + a_{n-1} T^{n-1}(v) + \\ldots + a_0 v \\] <p>with \\(T^n\\) being defined as applying \\(T\\) a total of \\(n\\) times.</p> <p>Proposition. Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. Then, a subset \\(N\\) of \\(M\\) is a submodule of \\(M\\) if and only if</p> <ol> <li>\\(N \\neq \\emptyset\\)</li> <li>For all \\(r \\in R\\), \\(x, y \\in N\\), then \\(rx - y \\in N\\)</li> </ol> <p>Definition. Let \\(R\\) be a commutative ring with identity. An \\(R\\)-algebra is a ring \\(A\\) together with a ring homomorphism \\(f: R \\rightarrow A\\) such that \\(\\varphi(1_R) = 1_A\\). Thus, the subring \\(f(R) \\subseteq A\\) is contained in the center of \\(A\\).</p> <p>Recall. The center of a ring \\(A\\) is the subring \\(A'\\) such that for all \\(x, y \\in R'\\), then \\(xy = yx\\). In other words, it is the commutative subring of \\(A\\).</p> <p>Definition. Given two \\(R\\)-algebras \\(A, B\\), an *\\(R\\)-algebra homomorphhism$ is a ring homomorphism \\(\\varphi: A \\rightarrow B\\) that maps \\(1_A \\rightarrow 1_B\\) such that \\(\\varphi(ra) = r\\varphi(a)\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-102-quotient-modules-and-module-homomorphisms","title":"Section 10.2 - Quotient Modules and Module Homomorphisms","text":"<p>Definition. Let \\(R\\) be a ring and \\(M, N\\) be \\(R\\)-modules. then a ring homomorphhism \\(\\varphi: M \\rightarrow N\\) is an \\(R\\)-module homomorphism if for all \\(r \\in R\\), \\(\\varphi(rx) = r\\varphi(x)\\).</p> <p>Theorem. An \\(R\\)-module homomorphism is an isomorphism if it is 1-1 and onto, and said modules are isomorphic.</p> <p>Definition. Let \\(M, N\\) be \\(R\\)-modules. The set \\(\\Hom_R(M, N)\\) is the set of all homomorphisms from \\(M\\) to \\(N\\).</p> <p>Promposition. Let \\(M\\), \\(N\\), and \\(L\\) be \\(R\\)-modules. Then,</p> <ol> <li>A function \\(\\varphi: M \\rightarrow N\\) is an \\(R\\)-module homomorphism if and only if \\(\\varphi(rx + y) = r\\varphi(x) + \\varphi(y)\\) for all \\(x, y \\in M\\) and \\(r \\in R\\).</li> <li>Let \\(\\varphi, \\psi \\in \\Hom_R(M, N)\\). Then, define \\(\\varphi + \\psi\\) as</li> </ol> \\[ (\\varphi + \\psi)(m) = \\varphi(m) + \\psi(m) \\] <p>Then, \\(\\varphi + \\psi \\in \\Hom_R(M, N)\\). Additionally, if \\(R\\) is commutative, with \\((r\\varphi)(m) = r(\\varphi(m))\\), then \\(r\\varphi \\in \\Hom_R(M,N)\\) 3. If \\(\\varphi \\in \\Hom_R(L, M)\\) and \\(\\psi \\in \\Hom_R(M, N)\\), then \\(\\psi \\circ \\varphi \\in \\Hom_R(L, N)\\) 4. \\(\\Hom_R(M, M)\\) is a ring with identity. With \\(R\\) being commutative, \\(\\Hom_R(M, M)\\) is an \\(R\\)-algebra.</p> <p>Proposition. Let \\(R\\) be a ring, \\(M\\) an \\(R\\)-module, and \\(N \\subseteq M\\) an \\(R\\)-submodule. then, \\(M/N\\) can be made into an \\(R\\)-module by defining addition. With \\(r \\in R\\) and \\(x + N \\in M/N\\),</p> \\[ r(x + N) = (rx) + N \\] <p>That is,</p> \\[ r \\overline{x} = \\overline{rx} \\] <p>Definition. Let \\(A, B\\) be submodules  of the \\(R\\)-module \\(M\\). Then, the sum of \\(A\\) and \\(B\\) is defined as</p> \\[ A + B = {a + b | a \\in A, b \\in B} \\] <p>This is the smallest submodule that contains both \\(A\\) and \\(B\\).</p> <p>Theorem. First Isomorphism Theorem. Let \\(M, N\\) be \\(R\\)-modules, and \\(\\varphi: M \\rightarrow N\\) be an \\(R\\)-module homomorphhiism. Then, \\(\\ker \\varphi\\) is a submodule of \\(M\\), and \\(M / \\ker \\varphi \\cong \\varphi(M)\\).</p> <p>Theorem. Second Isomorphism Theorem. Let \\(A, B\\) be submodules of the \\(R\\)-module \\(M\\). Then, \\((A + B)/B \\cong A/(A \\cap B)\\).</p> <p>Theorem. Third Isomorphism Theorem. Let \\(M\\) be an \\(R\\)-module, and \\(A \\subseteq B\\) be submodules of \\(M\\). Then, \\(\\frac{M/A}{B/A} \\cong M/B\\).</p> <p>Theorem. Lattice Isomorphism Theorem. Let \\(N\\) be a submodule of the \\(R\\)-module \\(M\\). Then, there is a bijection between submoudles of \\(M\\) containing \\(N\\) and submodules of \\(M/N\\). This is given by \\(A \\leftrightarrow A/N\\), for \\(A \\supseteq N\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-103-generation-of-modules-direct-sums-and-free-modules","title":"Section 10.3 - Generation of Modules, Direct Sums, and Free Modules","text":"<p>Definition. Let \\(M\\) be an \\(R\\)-module and \\(N_1, \\ldots, N_n\\) be submodules of \\(M\\).</p> <ol> <li>The sum of \\(N_1, \\ldots, N_n\\) is the set of all finite sums of elements from the sets \\(N_i\\). That is, \\(N_1, \\ldots, N_n := \\{a_1 + a_2 + \\ldots + a_n | a_i \\in N_i\\}\\)</li> <li>For any subset \\(A\\) of \\(M\\), let \\(RA = \\{r_1 a_1 + r_2 a_2 + \\ldots + r_m a_m | r_i \\in R, a_i \\in A\\}\\). If \\(N\\) is a submodule of \\(M\\) such that \\(N = RA\\), then \\(A\\) is called the generating set for \\(N\\).</li> <li>A submodule \\(N\\) of \\(M\\) is finitely generaated if there is some finite subset \\(A\\) of \\(M\\) such that \\(N = RA\\). That is, \\(N\\) is generated by some finite subset.</li> <li>A submodule of \\(M\\) (up to equality) is \\(cyclic\\) if there exists some element \\(a \\in M\\) such that \\(N = Ra = \\{ra | r \\in R\\}\\).</li> </ol> <p>Definition. Let \\(M_1, \\ldots, M_k\\) be a collection of \\(R\\)-modules. Then, the direct product is defined as</p> \\[ M_1 \\otimes \\ldots M_k = (m_1, \\ldots, m_k), m_i \\in M_i \\] <p>This direct product is in itself an \\(R\\)-module.</p> <p>Proposition. Let \\(N_1, \\ldots, N_n\\) be submodules of the \\(R\\)-module \\(M\\). Then, the following are equivalent:</p> <ol> <li>The map \\(\\pi: N_1 \\otimes \\ldots \\otimes N_k \\rightarrow N_1 + \\ldots + N_k\\) defined by \\(\\pi(a_1, \\ldots, a_n) = a_1 + \\ldots + a_n\\) is an isomorphism</li> <li>\\(N_j \\cup (N+1 + \\ldots + N_{j-1} + N{j+1} + \\ldots + N_n) = 0\\) for all \\(j \\in \\{1, 2, \\ldots, k\\}\\)</li> <li>Every \\(x \\in N_1 + \\ldots + N_n\\) can be written uniquely in the form \\(a_1 + \\ldots + a_n\\), with \\(a_i \\in N_i\\)</li> </ol> <p>Definition. An \\(R\\)-module \\(F\\) is said to be free on the subset \\(A\\) of \\(F\\) if for every nonzero \\(x \\in F\\), there exists nonzero elements \\(r_1, \\ldots, r_n\\) of \\(R\\) and unique \\(a_1, \\ldots, a_n\\) such that \\(x = r_1 a_1 + \\ldots + r_n a_n\\) for some \\(n \\in \\mathbb{Z}^+\\). That is, \\(A\\) is a basis or set of free generators of \\(F\\).</p> <p>Theorem. For any set \\(A\\), there is a free \\(R\\)-module \\(F(A)\\) on \\(A\\) such that \\(F(A)\\) satisfies the universal property: if \\(M\\) is any \\(R\\)-module, and \\(\\varphi: A \\rightarrow M\\) is a map of sets, there eixts a unique \\(R\\)-module homomorphism: \\(\\Phi: F(A) \\rightarrow M\\) such that \\(\\Phi(a) = \\varphi(a)\\) for all \\(a \\in A\\).</p> <p>Collary. If \\(F_1\\) and \\(F_2\\) are free modules on \\(A\\), then there is a unique isomorphism between \\(F_1\\) and \\(F_2\\), which is the identity map on A.</p> <p>Collary. If \\(F\\) is a free \\(R\\)-module with basis \\(A\\), then \\(F \\cong F(A)\\).</p> <p>Definition For a free module \\(F\\) with basis \\(A\\), if \\(R\\) is commutative, tthen the rank of \\(F\\) is the cardinality of \\(A\\).</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-104-tensor-products-of-modules","title":"Section 10.4 - Tensor Products of Modules","text":"<p>Skipped</p>"},{"location":"math/abstract-algebra/DF-10-modules/#section-105-exact-sequences-projective-injective-and-flat-modules","title":"Section 10.5 - Exact Sequences - Projective, Injective, and Flat Modules","text":"<p>Skipped</p>"},{"location":"math/abstract-algebra/DF-12-modules-pids/","title":"Chapter 12 - Modules over Principal Ideal Domains","text":""},{"location":"math/abstract-algebra/DF-12-modules-pids/#section-121-the-basic-theory","title":"Section 12.1 The Basic Theory","text":"<p>Definition. The left \\(R\\)-module \\(M\\) is said to be a Noetherian \\(R\\)-module if there are no infinitely increasing chains of submodules. That is, given</p> \\[ M_1 \\subseteq M_2 \\subseteq \\ldots \\] <p>there exists some \\(k \\in \\mathbb{N}\\) such thaht given any \\(n \\in \\mathbb{N}\\) with \\(n \\geq k\\), then \\(M_n = M_k\\).</p> <p>Definition. A ring \\(R\\) is Noetherian if it is Noetherian when viewed as a left \\(R\\)-module over itself.</p> <p>Theorem. Let \\(R\\) be a ring and \\(M\\) a left \\(R\\)-module. Then, the following are equivalent:</p> <ol> <li>\\(M\\) is Noetherian</li> <li>Every nonempty set of submodules of \\(M\\) contains a maximal element under inclusion</li> <li>Every submodule of \\(M\\) is finitely-generated</li> </ol> <p>Collary. If \\(R\\) is a principal ideal domain (PID), then all nonempty set of ideals of \\(R\\) has a maximal element. Additionally, \\(R\\) is as Noetherian ring.</p> <p>Proposition. Let \\(R\\) be an integral doman, and \\(M\\) be a free \\(R\\)-module of rank \\(n &lt; \\infty\\). Then, given \\(S\\) is subset \\(M\\) with \\(|S| &gt; n\\), the elements of \\(S\\) are \\(R\\)-linearly dependent.</p> <p>Definition. Given \\(R\\) an integral domain and \\(M\\) an \\(R\\)-module,</p> \\[ \\Tor(M) = \\{ x \\in M | rx = 0 \\text{ for any } r \\neq 0 \\} \\] <p>This is the torsion submodule of \\(M\\). If \\(\\Tor(M)\\) is empty, then \\(M\\) is torsion-free.</p> <p>Definition Let \\(R\\) be an integral domain and \\(M\\) be an \\(R\\)-module. Then, given a submodule \\(N\\),</p> \\[ \\Ann_R(N) = \\{r \\in R | rn = 0 \\text{ for all } n \\in N \\} \\] <p>This ideal of \\(R\\) is the annihilator of \\(N\\). That is, \\(\\Ann(N)\\) is the set of elements of \\(R\\) such that \\((r)N = \\{ 0 \\}\\).</p> <p>Note that if \\(N\\) is not a torsion submodule of \\(M\\), then \\(\\Ann(N) = (0)R\\). Additionally, given \\(N, L\\) are submodules of \\(M\\) with \\(N \\subseteq L\\), then \\(\\Ann(N) \\subseteq \\Ann(L)\\).</p> <p>Additionally, if \\(R\\) is a PID, as \\(\\Ann_R(N)\\) is an ideal, \\(\\Ann(N) = (n)R\\) and \\(\\Ann(L) = (l)R\\) for some \\(n, l \\in R\\) such that \\(n | l\\).</p> <p>Definition. Given any integral domain \\(R\\), the rank of an \\(R\\)-module \\(M\\) is the maximum number of \\(R\\)-linearly independent elements of M.</p> <p>Collary. The rank of a free module is the number of generating elements.</p> <p>Theorem. Let \\(R\\) be a principal ideal domain, and \\(M\\) be a free \\(R\\)-module of finite rank \\(m\\), and \\(N\\) be a submodule of \\(M\\). Then,</p> <ol> <li>\\(N\\) is a free submodule with rank \\(n \\leq m\\).</li> <li>There exiss a basis \\(y_1, y_2, \\ldots, y_m\\) of \\(M\\) so that \\(r_1 y_1, r_2 y_2, \\ldots, r_m y_n\\) is a basis of \\(N\\) for some \\(r_i \\in R\\) and \\(r_1 | r_2 | \\ldots | r_n\\)</li> </ol>"},{"location":"math/diffeq/1-intro/","title":"Section 1 - Basic Concepts","text":""},{"location":"math/diffeq/1-intro/#section-11-definitions","title":"Section 1.1 - Definitions","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. A differential equation is an equation that describes a function in terms of its derivatives. Examples of differential equations include Newton's Laws, among others.</p> <p>Definition. The order of a differential equation is the largest derivative present in the equation with a non-zero constant.</p> <p>Definition. A differential equation that only involves derivatives with respect to one variable is called an ordinary differential equation (ODE).</p> <p>Definition. A differential equation that describes a function in terms of derivatives with respect to more than one linearly-independent variable is called a partial equation.</p> <p>Definition. A linear differential equation is any differential equation that cn be written in the following form:</p> \\[ a_n(t)y^{(n)}(t) + a_{n-1}(t)+y^{n-1}(t) + \\ldots + a_1(t)y'(t) + a_0(t)y(t) = g(t) \\] <p>Note that \\(a_n(t)\\) does not depeond on any derivative of \\(y\\), so the presence of terms such as \\(e^y\\) or \\(\\sqrt{y'}\\) signal that the equation is nonlinear.</p> <p>Definition. The solution(s) to a differential equation over an inverval \\(\\alpha &lt; t &lt; \\beta\\) are any funcion(s) \\(y(t)\\) that satisfy the differential equation.</p> <p>Definition. The initial conditions are a condition or set of conditions that constrain the possible solution sets.</p> <p>Definition. An Initial Value Problem is a differential equation along with the appropriate boundary or initial conditions.</p> <p>Definition. The integral of validity for a solution to a differential equation is the largest possible interval containing the initial coniditions for which the solution is valid.</p> <p>Definition. The general solution to a differential equation is the most general form a solution to a differential equation can take without requiring the initial conditions.</p> <p>Definition. The actual solution to a differential equation is the specific solution that satisfies the differential equation and the boundary conditions.</p> <p>Definition. A solution is said to be explicit if it can be written in the form \\(y = y(t)\\). Otherwise, it is said to be implicit.</p>"},{"location":"math/diffeq/1-intro/#section-12-directional-fields","title":"Section 1.2 - Directional Fields","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. A directional field is the graph of a \\(t\\) vs. \\(y(t)\\), with vectors drawn at each point with a slope corresponding to \\(y'(t)\\). Notably, each arrow will be pointed right (towards increasing \\(t\\)).</p>"},{"location":"math/diffeq/2-1st-order/","title":"Section 2 - First Order Differential Equations","text":""},{"location":"math/diffeq/2-1st-order/#section-21-linear-differential-equations","title":"Section 2.1 - Linear Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the following first-order linear differential equation be given, with \\(p(t)\\) and \\(g(t)\\) continuos.</p> \\[ \\frac{dy}{dt} + p(t)y = g(t) \\]"},{"location":"math/diffeq/2-1st-order/#deriving-the-solution","title":"Deriving the Solution","text":"<p>Next, we let \\(\\mu(t)\\) be our integrating factor. Multiply both sides of the equation through by \\(\\mu(t)\\).</p> \\[ \\mu(t)\\frac{dy}{dt} + \\mu(t)p(t)y = \\mu(t)g(t) \\] <p>Now, define \\(\\mu(t)\\) so that \\(\\mu(t)p(t) = \\mu'(t)\\). Thus, we can state the following:</p> \\[ \\mu(t)\\frac{dy}{dt} + \\mu'(t)y = \\mu(t)g(t) \\] <p>The left of the preceeding equation is simply the product rule, so we can write \\((\\mu(t)y(t))' = \\mu(t)g(t)\\). Take the integral of both sides.</p> \\[\\begin{align}     \\int (\\mu(t)y(t))' dt &amp;= \\int \\mu(t)g(t) \\\\     \\mu(t)y(t) + C &amp;= \\int \\mu(t)g(t) dt \\\\     y(t) &amp;= \\frac{\\int \\mu(t)g(t) dt - C}{\\mu(t)} \\end{align}\\] <p>Let \\(C\\) absorb the negative sig, and we see the following.</p> \\[ y(t) = \\frac{\\int \\mu(t)g(t) dt + C}{\\mu(t)} \\] <p>This is the general solution to the differential equation. However, it is incomplete, as we do not know \\(\\mu(t)\\)</p> <p>To derive the function, recall that we defined \\(\\mu(t)p(t) = \\mu'(t)\\). Thus, we can rewrite this equation.</p> \\[\\begin{align}     \\frac{\\mu'(t)}{\\mu(t)} &amp;= p(t) \\\\     (\\ln \\mu(t))' &amp;= p(t) \\\\ \\end{align}\\] <p>Integrate both sides.</p> \\[\\begin{align}     \\ln \\mu(t) + k = \\int p(t) dt \\\\     \\mu(t) &amp;= e^{\\int p(t) dt + k} \\\\     &amp;= e^k e^{\\int p(t) dt} \\end{align}\\] <p>As \\(k\\) is an unknown constant, rewrite this as \\(\\mu(t) = k \\exp(\\int p(t) dt)\\).</p>"},{"location":"math/diffeq/2-1st-order/#summary","title":"Summary","text":"<p>The following differential equation is given.</p> \\[ \\frac{dy}{dt} + p(t)y = g(t) \\] <p>To find a solution to this differential equation, construct the integrating factor \\(\\mu(t)\\).</p> \\[\\mu(t) = k \\exp(\\int p(t) dt)\\] <p>Thus, the solution to the differential equation can be written as the following.</p> \\[ y(t) = \\frac{\\int \\mu(t)g(t) dt + C}{\\mu(t)} \\]"},{"location":"math/diffeq/2-1st-order/#section-22-separable-differential-equations","title":"Section 2.2 - Separable Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the following differential equation of the following forms be given.</p> <p>\\begin{align}     N(y) \\frac{dy}{dx} &amp;= M(x)     \\frac{dy}{dx} &amp;= \\frac{M(x)}{N(y)} \\     \\frac{dy}{dx} &amp;= \\frac{N(y)}{M(x)} \\     \\frac{dy}{dx} &amp;= N(y)M(x) \\ \\end{align}.</p> <p>For the sake of simplicty, select the following form:</p> \\[ N(y) \\frac{dy}{dx} = M(x) \\] <p>Thus, integrate both sides with respect to \\(x\\).</p> \\[ \\int N(y) \\frac{dy}{dx} dx = \\int M(x) dx \\] <p>Since \\(y\\) is really \\(y(x)\\), we can make the following substitution:</p> \\[ u = y(x) \\text{ and } du = y'(x)dx = \\frac{dy}{dx}{dx} \\] <p>This reduces the integral to the following:</p> \\[ \\int N(u) du = \\int M(x) dx \\] <p>This is solvable from here.</p>"},{"location":"math/diffeq/2-1st-order/#section-24-bernoulli-equations","title":"Section 2.4 - Bernoulli Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let a differential equation of the following form be given, with \\(n \\in \\mathbb{N}; n \\geq 2\\)</p> \\[ y' + p(x)y = q(x)y^n \\] <p>This is a Bernoulli equation.</p> <p>Divide by \\(y^n\\).</p> \\[ y^{-n}y' + p(x)y^{1-n} = q(x) \\] <p>Now, make the substitution \\(v = y^{1-n}\\). Thus, the derivative is as follows.</p> \\[ v' = (1-n)y^{-n}y' \\] <p>Substituting into the first equation yields the following.</p> \\[ \\frac{1}{1-n}v' + p(x)v = q(x) \\] <p>After solving, be sure to rewrite in terms of \\(y\\).</p>"},{"location":"math/diffeq/3-2nd-order/","title":"Section 3 - Second Order Differential Equations","text":""},{"location":"math/diffeq/3-2nd-order/#section-31-basic-concepts","title":"Section 3.1 - Basic Concepts","text":"<p>This section is from Paul's Online Math Notes.</p> <p>All second-order differential equations can be written in the following form:</p> \\[ p(t) y'' + q(t) y' + r(t) y = g(t) \\] <p>In the case where \\(p(t)\\), \\(q(t)\\), and \\(r(t)\\) are constants, we write the equation as the following:</p> \\[ ay'' + by' + cy = g(t) \\] <p>This is a second-order differential equation with constant coefficients.</p> <p>Definition. In the event that \\(g(t) = 0\\), we say the equation is homogenous. Otherwise, the equation is nonhomogenous.</p> <p>Definition. Principal of Superposition. Let \\(y_1(t)\\) and \\(y_2(t)\\) be solutions to a linear, homogenous differential equation. Then, any linear combination of said solutions is also a solution to the differential equation. In other words, with \\(c_1, c_2 \\in \\mathbb{R}\\), the following is a solution to a differential equation.</p> \\[ y(t) = c_1 y_1(t) + c_2 y_2(t) \\] <p>Given a second-order homogenous differential equation with constant coeffictions, we assume solutions of the following form:</p> \\[ y(t) = e^{rt} \\] <p>Substituting this equation into the differential equationm, we see the following:</p> \\[ e^{rt}(ar^2 + br + c) = 0 \\] <p>Thus, we allow the charactaristic equation of the differential equation to be as follows:</p> \\[ ar^2 + br + c = 0 \\]"},{"location":"math/diffeq/3-2nd-order/#section-32-real-distinct-roots","title":"Section 3.2 - Real &amp; Distinct Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>When the two roots to the charactaristic equation are discrete roots in the real numbers, we see the following solutions.</p> \\[ y_1(t) = e^{r_1 t} \\] \\[ y_2(t) = e^{r_2 t} \\] <p>Thus,</p> \\[ y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t} \\]"},{"location":"math/diffeq/3-2nd-order/#section-33-complex-roots","title":"Section 3.3 - Complex Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Let the solutions to the charactaristic equation be of the following form:</p> \\[ r_{1,2} = \\lambda \\pm \\mu i \\] <p>Thus, our two solutions are</p> \\[ y_1(t) = e^{(\\lambda + \\mu i)t} \\] \\[ y_2(t) = e^{(\\lambda - \\mu i)t} \\] <p>Recall Euler's Formula:</p> \\[ e^{i \\theta} = \\cos \\theta + i \\sin \\theta \\] <p>A colliloquy of Euler's formula is the following:</p> \\[ e^{-i \\theta} = \\cos(-\\theta) + i \\sin(-\\theta) = \\cos \\theta - i \\sin \\theta \\] <p>Thus, we can write our solutions as the following:</p> \\[\\begin{align}     y_1(t) &amp;= e^{(\\lambda + \\mu i)t} &amp;= e^{\\lambda t} e^{i \\mu t} &amp;= e^{\\lambda t}(\\cos(\\mu t) + i \\sin(\\mu t)) \\\\     y_2(t) &amp;= e^{(\\lambda - \\mu i)t} &amp;= e^{\\lambda t} e^{-i \\mu t} &amp;= e^{\\lambda t}(\\cos(\\mu t) - i \\sin(\\mu t)) \\end{align}\\] <p>A linear combination of the two solutions can be written as the following:</p> \\[ y(t) = c_1 e^{\\lambda t} \\cos(\\mu t) + c_2 e^{\\lambda t} \\sin(\\mu t) \\]"},{"location":"math/diffeq/3-2nd-order/#section-34-repeated-roots","title":"Section 3.4 - Repeated Roots","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume the solutions to the charactaristic equations are \\(r = r_1 = r_2\\). Thus, the two equations \\(y_t(t)\\) and \\(y_2(t)\\) are not linearly independent.</p> <p>After a lot of algebra, we see that</p> \\[y_1(t) = e^{rt}\\] \\[y_2(t) = t e^{rt}\\]"},{"location":"math/diffeq/3-2nd-order/#section-35-reduction-of-order","title":"Section 3.5 - Reduction of Order","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Skipped.</p>"},{"location":"math/diffeq/3-2nd-order/#section-36-fundamental-set-of-solutions-wronskian","title":"Section 3.6 - Fundamental Set of Solutions, Wronskian","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. Given two functions \\(f(t)\\), \\(g(t)\\), the Wronskian is defined as</p> \\[ W(f,g) = \\det \\begin{vmatrix}   f(t) &amp; g(t) \\\\   f'(t) &amp; g'(t) \\end{vmatrix} \\] <p>Definition. If \\(W(f, g) \\neq 0\\), then \\(f(t)\\) and \\(g(t)\\) are said to form a fundamental set of solutions, and can be superimposed to form the general solution.</p>"},{"location":"math/diffeq/3-2nd-order/#section-38-nonhomogenous-differential-equations","title":"Section 3.8 - Nonhomogenous Differential Equations","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume we have the differential equation as follows:</p> \\[ y'' + p(t) y' + q(t) y = g(t) \\] <p>The equivilent homogenous differential equation is</p> \\[ y'' + p(t) y' + q(t) y = 0 \\] <p>Theorem. Assume \\(Y_1(t)\\), \\(Y_2(t)\\) are solutions to the nonhomogenous differential equations. Then, \\(Y_1(t) - Y_2(t)\\) is a solution to the homogenous differential equation. This can be proved by substitution.</p> <p>Thus, with \\(y_h(t)\\) the solution to the homogenous problem, and \\(y_p(t)\\) the solution to this particular problem, we can say that the general form of the solution to this differential equation is</p> \\[ y(t) = y_h(t) + y_p(t) \\]"},{"location":"math/diffeq/3-2nd-order/#section-39-undetermined-coefficients","title":"Section 3.9 - Undetermined Coefficients","text":"<p>This section is from Paul's Online Math Notes.</p> <p>We know the following guesses for functions.</p> \\(g(t)\\) \\(y_p\\) guess \\(\\alpha e^{\\beta t}\\) \\(A e^{\\beta t}\\) \\(a \\cos(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) \\(b \\sin(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) \\(a \\cos(\\beta t) + \\sin(\\beta t)\\) \\(A \\cos(\\beta t) + B \\sin(\\beta t)\\) n-th degree polynomial \\(A_nt^n + A_{n-1}t^{n-1} + A_1 t + A_0\\) <p>Combine this with the following:</p> <p>Theorem. Given \\(y_{p_1}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_1(t)\\) and \\(y_{p_2}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_2(t)\\), then the function \\(y_{p_1}(t) + y_{p_2}(t)\\) is a solution to \\(y'' + p(t)y' + q(t)y = g_1(t) + g_2(t)\\)</p>"},{"location":"math/diffeq/3-2nd-order/#section-310-variation-of-parameters","title":"Section 3.10 - Variation of Parameters","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Assume we have the differential equation as follows:</p> \\[ y'' + p(t) y' + q(t) y = g(t) \\] <p>The equivilent homogenous differential equation is</p> \\[ y'' + p(t) y' + q(t) y = 0 \\] <p>For this method, we must have \\(y_1(t)\\) and \\(y_2(t)\\) known. Through a lot of math, we see that</p> \\[ y_p = -y_1 \\int \\frac{y_2(t)g(t)}{W(y_1, y_2)} dt + y_2 \\int \\frac{y_1(t)g(t)}{W(y_1, y_2)} dt \\]"},{"location":"math/diffeq/4-laplace/","title":"Section 4 - Laplace Transformations","text":""},{"location":"math/diffeq/4-laplace/#section-41-definition","title":"Section 4.1 - Definition","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Definition. The Laplace transform of a function is given by the following:</p> \\[ \\mathcal{L} \\{f(t)\\}(s) = F(s) = \\int_0^{\\infty} e^{-st}f(t) dt \\]"},{"location":"math/diffeq/4-laplace/#section-42-properties","title":"Section 4.2 - Properties","text":"<p>This section is from Paul's Online Math Notes.</p> <p>The Laplace Transformation is a linear transformation over functions in \\(\\mathbb{R}[t]\\). That is, given \\(a, b \\in \\mathbb{R}, f(t), g(t) \\in \\mathbb{R}[t]\\), we know that</p> \\[ \\mathcal{L} \\{a f(t)\\ + b g(t) \\}(s) = a F(s) + b G(s) \\]"},{"location":"math/diffeq/4-laplace/#section-43-inverse-laplace-transformation","title":"Section 4.3 - Inverse Laplace Transformation","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Given \\(F(s)\\), we define the Inverse Laplace Transformation as the following;</p> \\[ f(t) = \\mathcal{L}^{-1} \\{F(s)\\} \\]"},{"location":"math/diffeq/4-laplace/#section-44-step-function","title":"Section 4.4 - Step Function","text":"<p>The step/Heaviside function \\(u_c(t)\\) is defined as 0 if \\(t &lt; c\\), and 1 if \\(t &gt; c\\).</p> <p>Alternatively, \\(u(t - c) = H(t - c)\\) is 0 if \\(t &lt; c\\), and 1 if \\(t &gt; c\\).</p> <p>Applying this to the Laplace transform,</p> \\[ \\begin{align}     \\mathcal{L} \\{ u_c(t) f(t-c) \\} &amp;= \\int_0^{\\infty} e^{-st}u_c(t)f(t) dt \\\\     &amp;= \\int_c^{\\infty} e^{-st}f(t) dt \\end{align} \\] <p>If we let \\(u = t - c\\),</p> \\[ \\begin{align}     \\mathcal{L} \\{ u_c(t) f(t-c) \\} &amp;= \\int_0^{\\infty} e^{-s(u+c)}f(u) du \\\\     &amp;= \\int_0^{\\infty} e^{-su}e^{-cs}f(u) du \\\\     &amp;= e^{-cs} \\int_0^{\\infty} e^{-su}f(u) du \\\\     &amp;= e^{-cs} F(s) \\end{align} \\]"},{"location":"math/diffeq/4-laplace/#section-45-laplace-transformation-applied-to-ivps","title":"Section 4.5 - Laplace Transformation applied to IVPs","text":"<p>This section is from Paul's Online Math Notes.</p> <p>Theorum. Given a function \\(f(t)\\) with \\(C^n\\) continuity, then</p> \\[ \\mathcal{L} \\{ f^{(n)} (t) \\} = s^n F(s) - s^{n-1} f(0) - s^{n-2} f'(0) - \\ldots - s f^{(n-2)} (0) - f^{(n-1)} (0) \\] <p>For \\(n=1, 2\\) we see that</p> \\[ \\begin{align}     \\mathcal{L} \\{ y' \\} &amp;= sY(s) - y(0) \\\\     \\mathcal{L} \\{ y'' \\} &amp;= s^2 Y(s) - s y(0) - y'(0) \\end{align} \\] <p>We can take the Laplace transformation of an IVP, solve for \\(Y(s)\\), then take the inverse to find the solution.</p>"},{"location":"math/diffeq/4-laplace/#section-46-nonconstant-coefficient-ivps","title":"Section 4.6 - Nonconstant Coefficient IVPs","text":"<p>This section is from Paul's Online Math Notes.</p>"},{"location":"math/real-analysis/11-metric-spaces/","title":"Chapter 11 - Metric Spaces","text":""},{"location":"math/real-analysis/11-metric-spaces/#section-114-netric-spaces","title":"Section 11.4 - Netric Spaces","text":"<p>Definition. A metric on set \\(S\\) is a function \\(d: S \\otimes S \\rightarrow \\mathbb{R}\\) that satifies the following properties for all \\(x, y, z \\in S\\),</p> <ul> <li>\\(d(x, y) \\geq 0\\)</li> <li>\\(d(x, y) = 0 \\; \\text{ if and only if } x = y\\)</li> <li>\\(d(x, y) = d(y, x)\\)</li> <li>\\(d(x, y) \\leq d(x, z) + d(z, y)\\)</li> </ul> <p>Definition. A metric space \\((S, d)\\) is a set \\(S\\), with elements called points, together with a metric \\(d\\).</p> <p>Definition. With metric space \\((S, d)\\), if \\(A \\subset S\\), then \\((A, d)\\) is a subspace of \\((S, d)\\).</p> <p>Definition. The discrete metric is provided by</p> \\[ d(x, y) = \\begin{cases}   0 \\; \\text{ if } x = y \\\\   1 \\; \\text{ if } x \\neq y \\end{cases} \\] <p>Definition. Let \\((S, d)\\) be a metric space. Then, for each \\(\\epsilon &gt; 0\\), the \\(\\epsilon\\)-neighborhood or \\(\\epsilon\\)-ball of a point \\(a \\in S\\) is the set</p> \\[ V_\\epsilon(a) = {x \\in S | d(a, x) &lt; \\epsilon} \\] <p>Definition. Let \\((S, d)\\) be a metric space. Then, a subset \\(G \\subseteq S\\) is open if for each \\(x \\in G\\), there exists some \\(\\epsilon &gt; 0\\) so that \\(V_\\epsilon(x) \\subseteq G\\).</p> <p>Definition. Let \\((S, d)\\) be a metric space. Then, a subset \\(G \\subseteq S\\) is closed if its complement \\(C(G) = S - G = S \\ F\\) is closed.</p> <p>Definition. Let \\((S, d)\\) be a metric space. A point \\(c \\in S\\) is a *cluster point$ of a set \\(A \\subseteq S\\) if every \\(\\epsilon\\)-neighborhood of \\(c\\) contains some point \\(a \\in A\\) such that \\(a \\neq c\\).</p> <p>Theorem. Every \\(\\epsilon\\)-neighborhood of a point is an open set.</p>"},{"location":"math/real-analysis/2-reals/","title":"Chapter 2 - The Real Number Line","text":""},{"location":"math/real-analysis/2-reals/#section-21-the-algebraic-and-order-properties-of-real-numbers","title":"Section 2.1 - The Algebraic and Order Properties of Real Numbers","text":"<p>Proposition. 2.1.1: \\(\\mathbb{R}\\) is a field, with zero element \\(0\\) and identity \\(1\\).</p> <p>Definition. The rational numbers \\(\\mathbb{Q}\\) is the field of fractions of the natural numbers \\(\\mathbb{N}\\).</p> <p>Theorem. 2.1.4: There does not exist a rational number \\(r\\) such that \\(r^2 = 2\\).</p> <p>Definition. An ordered field is a field \\(F\\) together with subset \\(F^+\\) such that</p> <ol> <li>\\(F+\\) is closed under addition and multiplication</li> <li>If \\(a \\in F\\), then exclusively \\(a \\in F^+\\), \\(a = 0\\), or \\(-a \\in F^+\\).</li> </ol> <p>Theorem. In any ordered field \\(F\\), the following hold</p> <ol> <li>\\(1 \\in F^+\\)</li> <li>\\(\\mathbb{N} \\subseteq F^+\\)</li> <li>If \\(a \\in F^+\\), then \\(\\frac{1}{a} \\in F^+\\)</li> </ol> <p>Definition The order relation \\(a &gt; b\\) and \\(b &lt; a\\) is defined by \\(a - b \\in F^+\\).</p> <p>Theorem. If \\(a, b, c \\in F\\), then</p> <ol> <li>One of \\(a &gt; b\\), \\(a = b\\), or \\(a &lt; b\\) hold (trichotomy)</li> <li>If \\(a &gt; b\\) and \\(b &gt; c\\), then \\(a &gt; c\\) (transitivity)</li> <li>If \\(a &gt; b\\), then \\(-a &lt; -b\\)</li> <li>If \\(a &gt; b\\) and \\(c &gt; 0\\), then \\(ac &gt; bc\\)</li> <li>If \\(a &gt; b\\) and \\(c &lt; 0\\), then \\(ac &lt; bc\\)</li> <li>If \\(a &gt; b &gt; 0\\), then \\(\\frac{1}{b} &gt; \\frac{1}{a} &gt; 0\\)</li> </ol> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded above if there exists some \\(u \\in F\\) such that \\(s \\leq u\\) for all \\(s \\in S\\). Then, said element \\(u\\) is an upper bound of \\(S\\).</p> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded below if there exists some \\(u \\in F\\) such that \\(s \\geq u\\) for all \\(s \\in S\\). Then, said element \\(u\\) is a lower bound of \\(S\\).</p> <p>Definition. Let \\(S\\) be a nonempty subset of ordered field \\(F\\). Then, \\(S\\) is bounded if it is bounded both above and below.</p> <p>Definition. Given field \\(F\\) and nonempty subset \\(S \\subseteq F\\), an element \\(u \\in F\\) is a supremum or least upper bound of \\(S\\) if \\(u\\) is an upper bound of \\(S\\), and given any other upper bound \\(v\\), then \\(u &lt; v\\)</p> <p>Definition. Given field \\(F\\) and nonempty subset \\(S \\subseteq F\\), an element \\(u \\in F\\) is an infimum or greatest lower bound of \\(S\\) if \\(u\\) is a lower bound of \\(S\\), and given any other lower bound \\(v\\), then \\(u &gt; v\\)</p> <p>Definition. Given an ordered field \\(F\\), the field has the supremum/infimum property if given any nonempty subset \\(S\\), if \\(S\\) is bounded above/below, \\(S\\) has a supremum/infimum.</p>"},{"location":"math/real-analysis/2-reals/#section-22-absolute-value-and-the-real-line","title":"Section 2.2 - Absolute Value and the Real Line","text":"<p>Definition. Absolute value is defined as normal (piecewise). Multiline function in LaTeX are hard.</p> <p>Theorem. Given any \\(a, b \\in \\mathbb{R}\\), we know that</p> <ol> <li>\\(|a| &gt; 0\\) for \\(a \\neq 0\\)</li> <li>\\(|ab| = |a||b|\\)</li> <li>\\(|a + b| \\leq |a| + |b|\\)</li> </ol> <p>Collary. Givem \\(a, b \\in \\mathbb{R}\\), then \\(\\abs{\\abs{a} - \\abs{b}} \\leq \\abs{a - b}\\).</p> <p>Remark. Every field has at least one absolute value function.</p> <p>Theorem. In an ordered field \\(F\\), for any \\(r &gt; 0\\), we know that</p> <ol> <li>\\(\\abs{x = r}\\) if and only if \\(x = r\\) or \\(x = -r\\)</li> <li>\\(\\abs{x &lt; r}\\) if and only if \\(-r &lt; x &lt; r\\)</li> <li>\\(\\abs{x &gt; r}\\) if either \\(x &gt; r\\) or \\(x &lt; -r\\)</li> </ol> <p>Definition. The standard distance function or metric on the real numbers \\(\\mathbb{R}\\) given \\(a, b\\) is \\(\\abs{a - b}\\).</p> <p>Theorem. For any real numbers \\(a, b, c\\),</p> <ol> <li>\\(\\abs{a - b} &gt; 0\\) if and only if \\(a \\neq b\\) and \\(\\abs{a - b} = 0\\) if and only if \\(a = b\\)</li> <li>\\(\\abs{a - b} = \\abs{b - a}\\)</li> <li>\\(\\abs{a - c} \\leq \\abs{a - b} + \\abs{b + c}\\)</li> </ol> <p>Definition A set together with a function satisfying these three properties is known as a metric space.</p> <p>Definition The \\(\\epsilon\\)-neighborhood of \\(a \\in \\mathbb{R}\\), denoted \\(V_\\epsilon(a)\\) is the set of all real numbers \\(x \\in \\mathbb{R}\\) such that \\(\\abs{x - a} &lt; \\epsilon\\). That is,</p> \\[ V_\\epsilon(a) = (a - \\epsilon, a + \\epsilon) \\] <p>Decimals. Let \\(x \\in \\mathbb{R}\\) such that \\(x &gt; 0\\). By the archimedian property, there exists some \\(b_0 \\in \\mathbb{N} \\cup {0}\\) such that \\(b_0 &lt; x &lt; b_0 + 1\\). We can repeat this to see</p> \\[ x = b_0 + \\frac{b_1}{10} + \\frac{b_2}{100} + \\ldots + \\frac{b_n}{100^n} + \\ldots \\] <p>Definition. The decimal expansion of \\(x\\) is denoted \\(b_0.b_1 b_2 b_3 \\ldots\\).</p>"},{"location":"math/real-analysis/2-reals/#section-25-intervals","title":"Section 2.5 - Intervals","text":"<p>Definition. A subset \\(I\\) is an interval if and only if, given \\(a, b \\in I\\), then \\([a, b] \\subseteq I\\).</p> <p>Definition. Intervals \\(I_1, I_2, \\ldots, I_n, \\ldots\\) are nested if and only if \\(I_1 \\subseteq I_2 \\subseteq \\ldots \\subseteq I_n \\subseteq \\ldots\\).</p> <p>Theorem. Nested Intervals Property. If \\(I_n = [a_n, b_n]\\) is a set of nested intervals that are closed and bound, then there exists some number \\(z \\in \\mathbb{R}\\) such that \\(z \\in I_n\\) for all \\(n\\).</p> <p>Theorem. If \\(a &lt; b\\), then the interval \\([a, b]\\) is an uncountable set.</p> <p>Collary. \\(\\mathbb{R}\\) is uncountable.</p>"},{"location":"math/real-analysis/3-sequences-series/","title":"Chapter 3 - Sequences and Series","text":""},{"location":"math/real-analysis/3-sequences-series/#section-31-sequences-and-their-limits","title":"Section 3.1 - Sequences and their Limits","text":"<p>Definition. A sequence in \\(\\mathbb{R}\\) is a function \\(X: \\mathbb{N} \\rightarrow \\mathbb{R}\\), typically notated as \\(X\\) or \\((x_n)\\), with \\(x_n\\) being referred to as the terms of the sequence. The set \\({x_n | n \\in \\mathbb{N}}\\) is the range of this sequence.</p> <p>Definition. The sequence is bounded if its range is a bounded subset of \\(\\mathbb{R}\\).</p> <p>Example. The constant sequence \\(C = (c) = (c, c, c, \\ldots)\\).</p> <p>Example. The harmonic sequence \\(\\frac{1}{n} = (1, \\frac{1}{2}, \\frac{1}{3}, \\ldots)\\)</p> <p>Example. The geometric sequence, given base \\(a \\in \\mathbb{R}\\) and ratio \\(r \\in \\mathbb{R}\\)</p> \\[ (x_n) = (a, ar,  ar^2, ar^3, \\ldots) \\] <p>Example. The arithmatic sequence, given base \\(a \\in \\mathbb{R}\\) and distance \\(d \\in \\mathbb{R}\\),</p> \\[ (x_n) = (a, a + d, a + 2d, a + 3d, \\ldots) \\] <p>Example. Decimal expansions are bounded sequences.</p> <p>Definition. A sequence \\(X = (x_n)\\) is said to converge to a number \\(x \\in \\mathbb{R}\\) if when given any \\(\\epsilon &gt; 0\\), there exists some \\(K \\in \\mathbb{N}\\) such that for every \\(n \\in \\mathbb{N}\\) with \\(n \\geq K\\),</p> \\[ \\abs{x_n - x} &lt; \\epsilon \\] <p>If this is the case, we say that \\(X\\) converges to  \\(x\\), and \\(x\\) is a limit of X. This can be written as</p> \\[ \\lim X = x \\text{ or } \\text \\lim(x_n) = x \\text{ or }  x_n \\rightarrow x \\] <p>Definition. If a sequence does not have a limit, it is divergent.</p> <p>Theorem. A sequence can have at most one limit. That is, if a limit exists, it is unique.</p> <p>Theorem. If a limit is convergent, then it is bounded.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-32-limit-theorems","title":"Section 3.2 - Limit Theorems","text":"<p>Theorem. Suppose there exists some \\(X\\) such that \\((x_n) \\rightarrow x\\) and \\(Y\\) such that \\((y_n) \\rightarrow y\\). Then,</p> <ol> <li>\\(x_n + y_n \\rightarrow x + y\\)</li> <li>\\(x_n \\cdot y_n \\rightarrow xy\\)</li> <li>If \\(x_n \\neq 0\\) for all \\(n\\), then \\(\\frac{1}{x_n} \\rightarrow \\frac{1}{x}\\)</li> </ol> <p>Theorem. Suppose \\((x_n)\\) aand \\((y_n)\\) are convergent sequences and \\(N \\in \\mathbb{N}\\). Then,</p> <ol> <li>If \\(x_n \\leq y_n\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\leq \\lim(y_n)\\)</li> <li>If \\(x_n \\leq a\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\leq a\\)</li> <li>If \\(x_n \\geq a\\) for all \\(n \\geq N\\), then \\(\\lim(x_n) \\geq a\\)</li> </ol> <p>Theorem. Squeeze Theorem. Suppose \\((x_n), (y_n), (z_n)\\) are all sequences of real numbers, and \\(\\lim(x_n) = \\lim(z_n) = a\\). Then, if for some \\(N in \\mathbb{N}\\),</p> \\[ \\text{If } x_n \\leq y_n \\leq z_n, \\text{ then } \\lim(y_n) = a \\] <p>Theorem. Suppose \\((x_n)\\) is a sequence if real numbers. Then,</p> <ol> <li>If \\(x_n \\rightarrow x\\), then \\(\\abs{x_n} \\rightarrow \\abs{x}\\)</li> <li>If \\(\\abs{x_n} \\rightarrow 0\\), then \\(x_n \\rightarrow 0\\)</li> <li>\\(x_n \\rightarrow x\\) if and only if \\(\\abs{x_n - n} \\rightarrow 0\\)</li> </ol> <p>Theorem. Suppose \\((x_n)\\) is a sequence if real numbers, with each \\(x_n \\geq 0\\). Then, given some \\(k \\in \\mathbb{N}\\), if \\(x_n \\rightarrow x\\), then \\(\\sqrt[k]{x_n} \\rightarrow \\sqrt[k]{x}\\).</p>"},{"location":"math/real-analysis/3-sequences-series/#section-33-monotonic-sequences","title":"Section 3.3 - Monotonic Sequences","text":"<p>Definition. A sequence \\((x_n)\\) is monotonically increasing if \\(x_{n+1} \\geq x_n\\) for all \\(n \\in \\mathbb{N}\\).</p> <p>Definition. A sequence \\((x_n)\\) is monotonically decreasing if \\(x_{n+1} \\leq x_n\\) for all \\(n \\in \\mathbb{N}\\).</p> <p>Definition. A sequence is monotonic if it is either monotonically increasing or decreasing.</p> <p>Theorem. A monotonic sequence is converging if and only if it is bound.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-34-subsequences","title":"Section 3.4 - Subsequences","text":"<p>Definition. Let \\(X = (x_n)\\) be a sequence in \\(\\mathbb{R}\\). Then, the sequence</p> \\[ X_{n_k} = (x_{n_1}, x_{n_2}, \\ldots) \\] <p>is a subsequence of \\(X\\),</p> <p>Theorem. If a sequence converges to \\(x\\), then every subsequence also converges to \\(x\\).</p> <p>Theorem. Every sequence of real numbers \\((x_n)\\) contains a monotonic subsequence \\((x_{n_k})\\).</p> <p>Collary. Bolzano-Weierstrass Theorem. Every bounded sequence of real numbers has a convergent subsequence.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-35-the-cauchy-criterion","title":"Section 3.5 - The Cauchy Criterion","text":"<p>Definition. A sequence \\((x_n)\\) is said to be a Cauchy sequence such that for any given \\(\\epsilon\\), there exists a natural number \\(H\\) such that all natural numbers \\(m, n \\geq H\\), then</p> \\[ \\abs{x_m - x_n} \\leq \\epsilon \\] <p>Theorem. If \\((x_n)\\) is a Cauchy sequence, then \\((x_n)\\) is convergent.</p>"},{"location":"math/real-analysis/3-sequences-series/#section-37-series","title":"Section 3.7 - Series","text":"<p>Definition. Let \\((x_n)\\) be a sequence in \\(\\mathbb{R}\\). Then, the infinite series genearted by \\(X\\) is the sequence \\(S = (s_n)\\) with terms</p> \\[ s_1 = x_1; \\; s_{n+1} = s_n + x_{n+1} \\] <p>In other words, \\(s_n = \\sum_{i=1}^n x_i\\). We denote this series as \\(\\sum x_n\\).</p> <p>Definition. If this series is convergent to some number \\(s\\), we say that \\(s\\) is the sum of the series.</p> <p>For natural numbers \\(n &gt; m\\), note that</p> \\[ s_n - s_m = \\sum_{i=m + 1}^n x_i \\] <p>In particular, \\(s_n - s_{n - 1} = x^n\\). Thus, the Cauchy criteria takes the form</p> <p>Theorem. Cauchy Criteria for Series. The series \\(\\sum x_n\\) converges if and only if, for a given \\(\\epsilon\\), there exists some natural number \\(H\\) such that when \\(m &gt; n &gt; H\\),</p> \\[ \\abs{s_m - s_n} = \\abs{\\sum_{i = m + 1}^n x_i} &lt; \\epsilon \\] <p>Collary. \\(n\\)-th Term Test. If \\(\\sum x_n\\) converges, then \\(x_n \\rightarrow 0\\).</p> <p>Collary. Absolute Convergence Test. If \\(\\sum \\abs{x_n}\\) converges, then \\(\\sum x_n\\) converges.</p> <p>Theorem. A series with non-negative terms converges if and only if its sequence of partial sums is bounded.</p> <p>Theorem. \\(e = \\lim_{n \\rightarrow \\infty} (1+\\frac{1}{n})^n = \\sum_{n=0}^\\infty \\frac{1}{n!}\\)</p>"},{"location":"math/real-analysis/4-limits/","title":"Chapter 4 - Limits","text":""},{"location":"math/real-analysis/4-limits/#secrion-41-limits-of-functions","title":"Secrion 4.1 - Limits of Functions","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\). Then, a point \\(c \\in \\mathbb{R}\\) is a cluster point of \\(A\\) if for every \\(\\delta &gt; 0\\), the \\(\\delta\\)-neighborhood of \\(c\\) contains a point \\(a \\in A\\) such thhat \\(a \\neq c\\). That is, there exists some \\(a\\) such that \\(0 &lt; |a - c| &lt; \\delta\\).</p> <p>Theorem. A real number \\(c\\) is a cluster point for a set \\(A\\) if and only if there exists a sequence \\((a_n)\\) in \\(A\\\\ \\{c\\}\\) such that \\(a_n \\rightarrow c\\)</p> <p>Collary. A real number \\(c\\) is a cluster point of a set \\(A\\) if and only if every \\(\\delta\\)-neighborhood conttains infinitely many points of \\(A\\).</p> <p>Definition. The set of every cluster point of \\(A\\) is called the derived set of \\(A\\), and denoted \\(A'\\).</p> <p>Collary. A set \\(A\\) is closed if and only if \\(A' \\subseteq A\\).</p> <p>Remark. If \\(A'\\) is the derived set of \\(A\\), then \\(A'' \\subseteq A'\\).</p> <p>Remark. Intervals involving infinity and square brackets for the constant are closed.</p> <p>Definition. Suppose \\(f: A \\rightarrow \\mathbb{R}\\) is a function with domain \\(A \\subseteq \\mathbb{R}\\), and let \\(c \\in A\\) be a cluster point of \\(A\\). then, a real number \\(L\\) is a limit of \\(f\\) at \\(c\\) if goven any \\(\\epsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such that</p> \\[ 0 &lt; |x-c| &lt; \\delta \\Rightarrow |f(x) - L| &lt; \\epsilon \\] <p>Therorem. For a given function and cluster point, there can be at most one limit at said point.</p> <p>Theorem. Let \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightarrow \\mathbb{R}\\). Then, to show that \\(lim_{x \\rightarrow c} f(x) = L\\), it suffices to show that for every sequence \\((a_n)\\) in \\(A\\\\ \\{c\\}\\), the sequence \\((f(a_n))\\) converges tto \\(L\\).</p> <p>Definition. The extended real numbers are \\(\\hat{\\mathbb{R}} = \\mathbb{R} \\cup \\{ \\infty, -\\infty \\}\\) are a totally-ordered set witth supremum and infimum. Note that this set is no longer a field.</p> <p>Definition. At any point \\(c\\), the limitt of \\(f\\) at \\(c\\) is infinite if given some \\(\\alpha\\),  there exists some \\(V_\\delta(c)\\) such that forr all \\(x \\in V_\\epsilon(c)\\), then \\(f(x) \\in V_\\alpha(\\infty)\\).</p> <p>Definition. The limit of a function at infinity is defined if for a given \\(\\epsilon\\), there exists some \\(\\alpha\\) so that there exists some \\(V_\\delta(c)\\) such that for all \\(x \\in A\\),</p> \\[ x &gt; \\alpha \\Rightarrow |f(x) - L| &lt; \\epsilon \\]"},{"location":"math/real-analysis/4-limits/#section-42-limit-theorems","title":"Section 4.2 - Limit Theorems","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\) and \\(c \\in \\mathbb{R}\\) be a cluster point of \\(A\\). Then, a function \\(f: A \\rightarrow \\mathbb{R}\\) is bounded on a neighborhood of \\(c\\) if there exists some \\(\\delta\\)-neighborhood \\(V_\\delta(c)\\) of \\(c\\) and some constant \\(M &gt; 0\\) such that for all \\(x \\in A \\cap V_\\delta(c)\\), then \\(|f(x)| \\leq M\\).</p> <p>Theorem. If \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightarrow \\mathbb{R}\\) has a finite limit at \\(c \\in \\mathbb{R}\\), then \\(f\\) is bounded on some neighborhood of \\(c\\).</p> <p>Theorem. With \\(A \\subseteq \\mathbb{R}\\), and \\(f, g: A \\rightarrow \\mathbb{R}\\), with \\(c \\in \\mathbb{R}\\) a cluster point of \\(A\\), then if \\(\\lim_{x \\rightarrow c} f(x) = L\\) and \\(\\lim_{x \\rightarrow c} g(x) = M\\), then:</p> \\[\\lim_{x \\rightarrow c} (f(x) + g(x)) = L + M\\] \\[\\lim_{x \\rightarrow c} (f(x)g(x)) = LM\\] <p>Additionally, if \\(g(x) \\neq 0\\) for all \\(x \\in A\\), and \\(M \\neq 0\\), then</p> \\[ \\lim_{x \\rightarrow c} \\frac{f(x)}{g(x)} = \\frac{L}{M} \\] <p>Collary. If \\(p, q \\in \\mathbb{R}[x]\\), and \\(q(c) \\neq 0\\) for some \\(c \\in \\mathbb{R}\\), then</p> \\[ \\lim_{x \\rightarrow c} p(x) = p(c) \\] \\[ \\lim_{x \\rightarrow c} \\frac{p(x)}{q(x)} = \\frac{p(c)}{q(c)} \\] <p>Theorem. Squeeze Theorem. Let \\(A \\subseteq \\mathbb{R}\\). Then, if \\(f, g, h: A \\rightarrow \\mathbb{R}\\) and with \\(c \\in \\mathbb{R}\\) being a cluster point of \\(A\\), then if both</p> \\[ \\lim_{x \\rightarrow c} f(x) = \\lim_{x \\rightarrow c} h(x) = L \\] \\[ f(x) \\leq g(x) \\leq h(x) \\; \\text{ for all } x \\in A, x \\neq c \\] <p>Then, \\(\\lim_{x \\rightarrow c} g(x) = L\\).</p>"},{"location":"math/real-analysis/5-continuity/","title":"Chapter 5 - Continuiy","text":""},{"location":"math/real-analysis/5-continuity/#section-51-continuous-functions","title":"Section 5.1 - Continuous Functions","text":"<p>Definition. Let \\(A \\subseteq \\mathbb{R}\\), and \\(f: A \\rightarrow \\mathbb{R}\\). Then, if \\(a \\in A\\), \\(f\\) is continuous at \\(a\\) if, given any \\(\\epsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such that for all \\(x \\in A\\),</p> \\[ |x - a| &lt; \\delta \\Rightarrow |f(x) - f(a)| &lt; \\epsilon \\] <p>Note that if \\(a\\) is an isolateed point of \\(A\\), that is, not a cluster point, then \\(a\\) is automatically continuous.</p> <p>If \\(a\\) is a cluster point of \\(A\\), then this definition collapses to the definition of \\(\\lim_{x \\rightarrow a} f(x) = f(a)\\).</p> <p>Note that a function cannot be continuous at a point outside of its domain, even if the limit exists.</p> <p>Definition. \\(f\\) is continuous on \\(A\\) if it is continuous at every point \\(a \\in A\\).</p> <p>Theorem. \\(f\\) is continuous if and only if for every sequence \\((x_n)\\) in \\(A\\) that converges to \\(a\\), the sequence \\((f(x_n))\\) converges to \\(f(a)\\).</p> <p>Definition. Let \\((S, d_S)\\) and \\((T, d_T)\\) be metric spaces. A function \\(f: S \\rightarrow T\\) is continuous at a point \\(a \\in S\\) if given any \\(\\epsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) such thaat for all \\(x \\in S\\),</p> \\[ d_S(x, a) &lt; \\delta \\Rightarrow d_T(f(x),  f(a)) &lt; \\epsilon \\] <p>Theorem. A function \\(f: S \\rightarrow T\\) is continuous at a point \\(a \\in A\\) if and only if given some neighborhood \\(V(f(a)) \\in B\\), there xists some \\(U(a) \\in A\\) such that \\(f(U) \\subseteq V\\).</p>"},{"location":"math/real-analysis/5-continuity/#section-52-combinations-of-continuous-functions","title":"Section 5.2 - Combinations of continuous Functions.","text":"<p>Theorem. Let \\(f, g: A \\rightarrow \\mathbb{R}\\) be continuous at \\(a \\in A\\). Then, </p> <ul> <li>\\(f + g\\) and \\(fg\\) are continuous at \\(a\\)</li> <li>If \\(g(x) \\neq 0\\) for all \\(x \\in A\\), then \\(\\frac{f}{g}\\) is continuous at \\(a\\).</li> </ul> <p>As a consequence, every polynomial, rational, and basic trigonometric function are continuous on its domains.</p> <p>Theorem. Lett \\(A, B \\subseteq \\mathbb{R}\\), such that \\(f: A \\rightarrow B\\) and \\(g: B \\rightarrow \\mathbb{R}\\). Then, if \\(c\\) is a cluster point of \\(A\\) such that \\(\\lim_{x \\rightarrow c} f(x) = L \\in B\\) and \\(g\\) is continuous at \\(L\\), then</p> \\[ \\lim_{x \\rightarrow c} g(f(x)) = g(L) = g(\\lim_{x \\rightarrow c} f(x)) \\] <p>Collary. let \\(A, B \\subseteq  \\mathbb{R}\\), with \\(f: A \\rightarrow B\\) and \\(g: B \\rightarrow \\mathbb{R}\\). If \\(f\\) is continuous at \\(a \\in A\\) and \\(g\\) is continuous at \\(f(a) \\in B\\), then \\(g(f(x))\\) is continuous at \\(a\\).</p>"},{"location":"math/real-analysis/5-continuity/#section-53-continuous-functions-on-intervals","title":"Section 5.3 - continuous functions on Intervals","text":"<p>Theorem. Let \\(S, T\\) be metric spaces with \\(A \\subseteq S\\) and \\(f: A \\rightarrow T\\). If \\(A\\) is a compact subset of \\(S\\), then \\(f(A)\\) is a compact subset of \\(T\\).</p> <p>Collary. Let \\(f: A \\rightarrow \\mathbb{R}\\) be a continuous function, with \\(A\\) being a compact subset of metric space \\(S\\). Then, \\(f(A)\\) is closed and bounded. Moreover, there exists a \\(p, q \\in A\\) such that \\(f(p)\\) and \\(f(q)\\) are the supremum and infimum of \\(f(A)\\).</p> <p>Collary. Maximum-Minimum Theorem. If \\(I = [a, b]\\) is a closed and bounded interval and \\(f: I \\rightarrow \\mathbb{R}\\) is continuous on \\(I\\), then \\(f\\) has an absolute minumum and maximum on \\(I\\).</p> <p>Theorem. Let \\(S, T\\) be metric spaces and \\(A \\subseteq S\\). Then, if \\(f: A \\rightarrow T\\) is continuous on \\(A\\), and \\(A\\) is a connected subset of \\(S\\), then \\(f(A)\\) is a connected subset of \\(T\\).</p> <p>Collary. Suppose that \\(I\\) is an interval. Let \\(f: I \\rightarrow \\mathbb{R}\\) be continuous on \\(I\\). Then, \\(f(I)\\) is an intterval.</p> <p>Theorem. (Bolzano's) Invermediate Value Theorem. Suppose \\(f: [a, b] \\rightarrow \\mathbb{R}\\) is continuous on \\([a, b]\\) with \\(a \\neq b\\). Then, given some \\(k\\) such that \\(f(a) &lt; k &lt; f(b)\\), there exists some \\(c \\in (a, b)\\) such that \\(k = f(c)\\).</p> <p>Definition. Let \\(A \\subseteq R\\). Then, a function \\(f: A \\rightarrow \\mathbb{R}\\) is uniformly continuous if given any \\(\\epsilon &gt; 0\\), there exists some \\(\\delta &gt; 0\\) depending only on \\(\\epsilon\\) such that for any \\(x, y \\in A\\),</p> \\[ |x - y| &lt; \\delta \\Rightarrow |f(x) - f(y)| &lt; \\epsilon \\] <p>Note that if \\(f\\) is uniformly continuous, it must be continuous on \\(A\\).</p> <p>Theorem. Let \\(I = [a, b]\\) be a closed and bounded interval. If \\(f: I \\rightarrow \\mathbb{R}\\) is continuous on \\(I\\), then \\(f\\) is uniformly continuous.</p> <p>Remark. If \\(S, T\\) are metric spaces, \\(K\\) is a compact subset of \\(S\\), and \\(f: K \\rightarrow T\\) is continuous on \\(K\\), then \\(f\\) is uniformly continuous.</p> <p>Theorem. Suppose \\(A \\subseteq \\mathbb{R}\\) and \\(f: A \\rightaarrow \\mathbb{R}\\) is uniformly continuous. Then, if \\((x_n)\\) is a Cauchy sequence in \\(A\\), \\((f(x_n))\\) is a Cauchy sequence in \\(\\mathbb{R}\\).</p> <p>Remark. Suppose \\(S, T\\) are metric spaces and \\(f: S \\rightaarrow T\\) is uniformly continuous. Then, if \\((x_n)\\) is a Cauchy sequence in \\(S\\), \\((f(x_n))\\) is a Cauchy sequence in \\(T\\).</p>"},{"location":"physics/electrostatics/1-math/","title":"Chapter 1 - Mathematics","text":""},{"location":"physics/electrostatics/1-math/#15-dyads-and-tensors","title":"1.5 - Dyads and Tensors","text":"<p>Definition. A dyadic is a representation of two-ish vectors.</p> \\[ \\stackrel{\\leftrightarrow}{\\vb{D}} = \\begin{matrix}     D_{xx} \\vu{x}\\vu{x} &amp;+ D_{xy} \\vu{x}\\vu{y} &amp;+ D{xz} \\vu{x}\\vu{z} \\\\     + D_{yx} \\vu{y}\\vu{x} &amp;+ D_{yy} \\vu{y}\\vu{y} &amp;+ D{yz} \\vu{y}\\vu{z} \\\\     + D_{zx} \\vu{z}\\vu{x} &amp;+ D_{zy} \\vu{z}\\vu{y} &amp;+ D{zz} \\vu{z}\\vu{z} \\end{matrix} \\] <p>Definition. If a dyadic can be written as a composition of two vectors \\(\\vb{A}\\) and \\(\\vb{B}\\), it is called a dyad.</p> \\[ \\vb{AB} = \\begin{matrix}     A_x B_x \\vu{x}\\vu{x} &amp;+ A_x B_y \\vu{x}\\vu{y} &amp;+ A_x B_z \\vu{x}\\vu{z} \\\\     + A_y B_x \\vu{y}\\vu{x} &amp;+ A_y B_y \\vu{y}\\vu{y} &amp;+ A_y B_z \\vu{y}\\vu{z} \\\\     + A_z B_x \\vu{z}\\vu{x} &amp;+ A_z B_y \\vu{z}\\vu{y} &amp;+ A_z B_z \\vu{z}\\vu{z} \\end{matrix} \\] <p>The dot product of a dyad \\(\\stackrel{\\leftrightarrow}{\\vb{D}} = \\vb{AB}\\) and vector \\(\\va{v}\\) can be written as follows:</p> \\[ (\\vb{AB}) \\vdot \\va{v} = \\vb{A} (\\vb{B} \\vdot \\va{v}) \\] <p>Definition. A symmetric/antisymmetric dyadic is defined the same way that a matrix is.</p> <p>Definition. The identity dyadic is \\(\\stackrel{\\leftrightarrow}{\\vb{I}} = \\vu{x}\\vu{x} + \\vu{y}\\vu{y} + \\vu{z}\\vu{z}\\).</p> <p>Definition. FOr a tensor, with coordinates \\(u^i\\), we have two sets of basis vectors:</p> \\[ \\vb{e}_i = \\pdv{\\vb{r}}{u^i} \\] \\[ \\vb{e}^i = \\grad{u^i} \\]"},{"location":"physics/electrostatics/1-math/#19-helmholtz-theorem","title":"1.9 - Helmholtz Theorem","text":"<p>Given an arbitrary vector field \\(\\vb{F}(\\vb(r))\\), we can write said field as a composition of a curl-free component \\(\\vb{\\Phi}(\\vb{r})\\) and a divergence-free component \\(\\vb{A}(\\vb{r})\\) as follows:</p> \\[ \\vb{F}(\\vb{r}) = - \\grad{\\vb{\\Phi}(\\vb{r})} + \\curl{\\vb{A}(\\vb{r})} \\] <p>Definition. Here, the gradient of the scalar potential is \\(\\grad{\\vb{\\Phi}(\\vb{r})}\\) and the curl of the vector potential is \\(\\curl{\\vb{A}(\\vb{r})}\\). Thus, the scalar potential is \\(\\vb{\\Phi}(\\vb{r})\\) and the vector potential is \\(\\vb{A}(\\vb{r})\\).</p> <p>Letting said field be over bounded volume \\(V\\) with closed surface \\(\\partial V\\), and the functions \\(\\vb{C}(\\vb{r}) = \\curl{\\vb{F}(\\vb{r})}\\) and \\(\\vb{D}(\\vb{r}) = \\div{\\vb{F}(\\vb{r})}\\) are known, we can say that</p> \\[ \\vb{\\Phi}(\\vb{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{D(\\vb{r}')}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{V'} - \\frac{1}{4 \\pi} \\int_{\\partial V} \\frac{\\vb{F}(\\vb{r}') \\vdot \\va{n}'}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{S'} \\] \\[ \\vb{A}(\\vb{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{C(\\vb{r}')}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{V'} - \\frac{1}{4 \\pi} \\int_{\\partial V} \\va{n}' \\cross \\frac{\\vb{F}(\\vb{r}')}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{S'} \\] <p>Now, assume that \\(\\lim(\\frac{\\vb{F}(\\vb{r})}{\\vb{r}}) = 0\\) as \\(\\vb{r} \\rightarrow \\infty\\), with a large enough volume, we see that the second terms vanish.</p> \\[ \\vb{\\Phi}(\\vb{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{D(\\vb{r}')}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{V'} \\] \\[ \\vb{A}(\\vb{r}) = \\frac{1}{4 \\pi} \\int_V \\frac{C(\\vb{r}')}{\\abs{\\vb{r}-\\vb{r}'}} \\dd{V'} \\]"},{"location":"physics/electrostatics/2-coulomb/","title":"Chapter 2 - Coulomb's Laws, Electric and Magnetic Fields","text":""},{"location":"physics/electrostatics/2-coulomb/#section-22-parallel-treatment-of-electric-and-magnetic-fields","title":"Section 2.2 - Parallel Treatment of Electric and Magnetic Fields","text":"<p>Consider two point charges, \\(q\\) and \\(Q\\), with the latter being at the origin of the coordinate system. Let \\(q\\) be located at point \\(\\vb{r}\\) relative to the origin.</p> <p>Thus, according to Coulomb's Law,</p> \\[ \\begin{align}     F^e_{qQ}(\\vb{r}) &amp;= \\frac{q_e Q_e}{4 \\pi \\epsilon_0} \\frac{\\vu{r}}{\\abs{\\vb{r}}^2} \\\\     F^m_{qQ}(\\vb{r}) &amp;= \\frac{q_m Q_m}{4 \\pi \\mu_0} \\frac{\\vu{r}}{\\abs{\\vb{r}}^2} \\end{align} \\] <p>Divide by the charge \\(q\\) to obtain the electric or magnetic field at point \\(\\vb{r}\\).</p> \\[ \\begin{align}     E(\\vb{r}) &amp;= \\frac{Q_e}{4 \\pi \\epsilon_0} \\frac{\\vu{r}}{\\abs{\\vb{r}}^2} \\\\     H(\\vb{r}) &amp;= \\frac{Q_m}{4 \\pi \\mu_0} \\frac{\\vu{r}}{\\abs{\\vb{r}}^2} \\end{align} \\] <p>Now, let \\(Q\\) be at point \\(\\vb{r'}\\). Then, the unit vector becomes \\(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}}\\), and we see the following.</p> \\[ \\begin{align}     E(\\vb{r}) &amp;= \\frac{Q_e}{4 \\pi \\epsilon_0} \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\\\     H(\\vb{r}) &amp;= \\frac{Q_m}{4 \\pi \\mu_0} \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\end{align} \\] <p>With multiple charges, we can apply the superposition principal to see the following:</p> \\[ \\begin{align}     E(\\vb{r}) &amp;= \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i=1}^N Q_e \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\\\     H(\\vb{r}) &amp;= \\frac{1}{4 \\pi \\mu_0} \\sum_{i=1}^N Q_m \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\end{align} \\] <p>We can convert this to an integral as \\(N\\) goes to infinity.</p> \\[ \\begin{align}     E(\\vb{r}) &amp;= \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\rho_e(\\vb{r'}) \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\dd V' \\\\     H(\\vb{r}) &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\vb{r'}) \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\dd V' \\end{align} \\]"},{"location":"physics/electrostatics/2-coulomb/#section-23-divergence-and-curl-of-the-electrostatic-or-magnetostatic-field","title":"Section 2.3 - Divergence and Curl of the Electrostatic or Magnetostatic Field","text":"<p>From a lot of advanced math, we know that</p> \\[ \\div{\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3}} = 4 \\pi \\delta(\\vb{r}-\\vb{r'}) \\] <p>Now, apply the divergence operator over \\(\\vb{r}\\) to the electrostatic and magnetostatic fields.</p> \\[ \\begin{align}     \\div{E(\\vb{r})} &amp;= \\div{(\\frac{1}{4 \\pi \\epsilon_0} \\int_V \\rho_e(\\vb{r'}) \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\dd V')} \\\\     \\div{H(\\vb{r})} &amp;= \\div{(\\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\vb{r'}) \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3} \\dd V')} \\end{align} \\] <p>As the divergence operator does not operate on \\(\\vb{r'}\\), we see that</p> \\[ \\begin{align}     \\div{E(\\vb{r})} &amp;= \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\rho_e(\\vb{r'}) \\div{(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3})} \\dd V' \\\\         &amp;= \\frac{1}{4 \\pi \\epsilon_0} 4 \\pi \\int_V \\rho_e(\\vb{r'}) \\delta(\\vb{r}-\\vb{r'}) \\dd V' \\\\         &amp;= \\frac{\\rho_e(\\vb{r})}{\\epsilon_0} \\\\     \\div{H(\\vb{r})} &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\vb{r'}) \\div{(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3})} \\dd V' \\\\         &amp;= \\frac{1}{4 \\pi \\mu_0} 4 \\pi \\int_V \\rho_m(\\vb{r'}) \\delta(\\vb{r}-\\vb{r'}) \\dd V' \\\\         &amp;= \\frac{\\rho_m(\\vb{r})}{\\mu_0} \\end{align} \\] <p>The curl of an electrostatic or magnetostatic is relatively simple.</p> \\[ \\begin{align}     \\curl{E(\\vb{r})} &amp;= \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\rho_e(\\vb{r'}) \\curl{(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3})} \\dd V' \\\\     \\curl{H(\\vb{r})} &amp;= \\frac{1}{4 \\pi \\mu_0} \\int_V \\rho_m(\\vb{r'}) \\curl{(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3})} \\dd V' \\\\ \\end{align} \\] <p>Additionally, we know \\(\\curl{f\\vb{A}} = f \\curl{\\vb{A}} + \\grad{f}\\cross\\vb{A}\\). Thus,</p> \\[ \\begin{align}     \\curl{(\\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^3})} &amp;= \\frac{1}{\\abs{\\vb{r}-\\vb{r'}}^3} \\curl{(\\vb{r}-\\vb{r'})} + (\\curl{\\frac{1}{\\abs{\\vb{r}-\\vb{r'}}^3}}) \\cross (\\vb{r}-\\vb{r'}) \\\\ \\end{align} \\] <p>We can verify that \\(\\curl{(\\vb{r}-\\vb{r'})} = 0\\), cancelling the first term. Additionally, \\(\\curl{\\frac{1}{\\abs{\\vb{r}-\\vb{r'}}^3}} = -3 \\frac{\\vb{r}-\\vb{r'}}{\\abs{\\vb{r}-\\vb{r'}}^5}\\), which when crossed with \\(\\vb{r}-\\vb{r'}\\), will cancel. Thus, all terms in the curl cancel, so for a static field, the curl is zero.</p>"},{"location":"physics/electrostatics/2-coulomb/#section-24-eletric-and-magnetic-flux-densities","title":"Section 2.4 - Eletric and Magnetic Flux Densities","text":"<p>The electric and magnetic flux density vectors are given by \\(\\epsilon_0 \\vb{E}\\) and \\(\\mu_0 \\vb{H}\\).</p> <p>Now, given \\(S\\) is a surfance enclosing \\(Q_e\\) or \\(Q_m\\) total charge, we denotate flux as following:</p> \\[ \\Phi_e = \\epsilon_0 \\int_S \\vb{E} \\vdot \\vu{n} \\dd = Q_e S \\text{ or } \\Phi_m = \\mu_0 \\int_S \\vb{H} \\vdot \\vu{n} \\dd S = Q_m \\] <p>Thus, applying divergence theorem,</p> \\[ Q_e = \\Phi_e = \\epsilon_0 \\int_S \\vb{E} \\vdot \\vu{n} \\dd = \\epsilon_0 \\int_V \\div{\\vb{E}} \\dd V \\] \\[ Q_m = \\Phi_m = \\mu_0 \\int_S \\vb{H} \\vdot \\vu{n} \\dd = \\epsilon_0 \\int_V \\div{\\vb{H}} \\dd V \\] <p>Since \\(Q_e = \\int_V \\rho_e \\dd V\\) and \\(Q_m = \\int_V \\rho_m \\dd V\\), we see that</p> \\[ \\begin{align}     \\int_V \\rho_e \\dd V &amp;= \\epsilon_0 \\int_V \\div{\\vb{E}} \\dd V \\\\     \\rho_e &amp;= \\epsilon_0 \\int_V \\div{\\vb{E}} \\dd V \\\\     \\int_V \\rho_m \\dd V &amp;= \\mu_0 \\int_V \\div{\\vb{H}} \\dd V \\\\     \\rho_m &amp;= \\mu_0 \\int_V \\div{\\vb{H}} \\dd V \\\\ \\end{align} \\] <p>Definition. This is known as Gauss' Law.</p> <p>With applicable symnetry, the integral factor becomes simply \\(E(r)*A\\), where \\(A\\) is the area of the surface at \\(r\\).</p>"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/","title":"Chapter 3 - Electric and Magnetic Scalar Potentials","text":""},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-31-work-and-energy-in-electrostatics-and-magnetostatics","title":"Section 3.1 - Work and Energy in Electrostatics and Magnetostatics","text":"<p>The force on charge \\(q\\) is given by \\(\\vb{F}(\\vb{r}) = q_e \\vb{E}(\\vb{r})\\) or \\(\\vb{F}(\\vb{r}) = q_m \\vb{H}(\\vb{r})\\). If this charge is moved \\(\\dd{\\vb{l}} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}\\), the change in internal energy (work) this produces can be written as</p> \\[ \\dd{U}= - \\vb{F} \\vdot \\dd{\\vb{l}} \\] <p>Rewriting this, \\(\\vb{F} = -\\grad{U}\\), with \\(U\\) as potential energy. Now, we can denote this change in internal energy in terms of \\(q\\) as follows:</p> \\[ \\vb{E}(\\vb{r}) = \\frac{1}{q_e} \\vb{F_e}(\\vb{r}) = - \\frac{1}{q_e} \\grad{U_e(\\vb{r})} = -\\grad{V_e(\\vb{r})} \\] <p>The units of electrostatic potential is Joule/Coublomb, also known as a Volt. Thus, the units of the electric field should be expressed in Volts/meter. Similarly,</p> \\[ \\vb{H}(\\vb{r}) = \\frac{1}{q_m} \\vb{F_m}(\\vb{r}) = - \\frac{1}{q_m} \\grad{U_m(\\vb{r})} = -\\grad{V_m(\\vb{r})} \\] <p>The units of magnetostatic potential is Joule/Weber, also known as an Ampere. Thus, the units of the magnetic field can be written as Amperes/meter.</p> <p>With this, we can calculate work. Moving a charge \\(q\\) from \\(A\\) to \\(B\\), we see that</p> \\[ \\delta W = \\int_A^B \\vb{F} \\vdot \\dd{\\vb{l}} = q_e \\int_A^B \\vb{E} \\vdot \\dd{\\vb{l}} = -q_e \\int_A^B \\grad{\\vb{V}} \\vdot \\dd{\\vb{l}} = -q_e \\delta V_e \\] <p>Strictly speaking, this is a potential difference. To find the absolute potential, assume a point charge \\(Q\\) at the origin, and a charge \\(q\\). We take the work as \\(q\\) moves from \\(\\vb{r'} = \\vb{\\infty}\\) to \\(\\vb{r'} = \\vb{r}\\). Thus,</p> \\[ W = -q_e \\frac{Q_e}{4 \\pi \\epsilon_0} \\int_{\\infty}^0 \\frac{\\vu{r'}}{r'^2} \\vdot (\\vu{r'}) \\dd{r'} = -q_e \\frac{Q_e}{4 \\pi \\epsilon_0} [\\frac{-1}{r'}]_{\\infty}^{r'} = q_e \\frac{Q_e}{4 \\pi \\epsilon_0} \\frac{1}{r} \\] \\[ W = -q_m \\frac{Q_m}{4 \\pi \\mu_0} \\int_{\\infty}^0 \\frac{\\vu{r'}}{r'^2} \\vdot (\\vu{r'}) \\dd{r'} = -q_m \\frac{Q_m}{4 \\pi \\mu_0} [\\frac{-1}{r'}]_{\\infty}^{r'} = q_m \\frac{Q_m}{4 \\pi \\mu_0} \\frac{1}{r} \\] <p>Letting the potential as \\(\\vb{r} \\rightarrow \\infty\\) equal \\(0\\) be our reference and dividing out \\(q\\), we find that the voltage for arrangement is the following:</p> \\[ V_e(\\vb{r}) = \\frac{Q_e}{4 \\pi \\epsilon_0 r} \\text{ and } V_m(\\vb{r}) = \\frac{Q_m}{4 \\pi \\mu_0 r} \\] <p>Now, if we let the stationary charge \\(Q\\) be located at \\(\\vb{r'}\\), we see that</p> \\[ V_e(\\vb{r}) = \\frac{Q_e}{4 \\pi \\epsilon_0 \\abs{\\vb{r} - \\vb{r'}}} \\text{ and } V_m(\\vb{r}) = \\frac{Q_m}{4 \\pi \\mu_0 \\abs{\\vb{r} - \\vb{r'}}} \\] <p>If we allow multiple charges, this becomes</p> \\[ V_e(\\vb{r}) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1}^N \\frac{Q_ei}{\\abs{\\vb{r}-\\vb{r_i}}} \\] <p>Taking this to its natural limit,</p> \\[ V_e(\\vb{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V'} \\frac{\\rho_e(\\vb{r'})}{\\abs{\\vb{r}-\\vb{r'}}} \\dd{V'} \\] \\[ V_m(\\vb{r}) = \\frac{1}{4 \\pi \\mu_0} \\int_{V'} \\frac{\\rho_m(\\vb{r'})}{\\abs{\\vb{r}-\\vb{r'}}} \\dd{V'} \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-32-energy-of-a-charge-distribution","title":"Section 3.2 - Energy of a Charge Distribution","text":"<p>Given two point charges \\(Q_{e1}, Q_{e2}\\) we know the work to bring them together is</p> \\[ W_2 = W_{21} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q_{e1} Q_{e2}}{\\abs{\\vb{r_2} - \\vb{r_1}}} \\] <p>Superposition applies here. The energy to create \\(N\\) charges is</p> \\[ W_n = \\frac{1}{2} \\frac{4 \\pi \\epsilon_0} \\sum_{i = 1}^{N} \\sum_{j &gt; i}^{N} \\frac{Q_{ei}Q_{ej}}{\\abs{\\vb{r_i}-\\vb{r_j}}} \\] <p>For the sake of symnetry, sum overall charges and divide by 2.</p> \\[ W_n = \\frac{1}{2} \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^{N} \\sum_{j \\neq i}^{N} \\frac{Q_{ei}Q_{ej}}{\\abs{\\vb{r_i}-\\vb{r_j}}} \\] <p>Rearranging, we see the following:</p> \\[ W_n = \\frac{1}{2} \\sum_{i = 1}^{N}Q_{ei} \\sum_{i \\neq j}^{N} \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q_{ej}}{\\abs{\\vb{r_i}-\\vb{r_j}}} = \\frac{1}{2}\\sum_{i = 1}^{N} Q_{ei} V(\\vb{r_i}) \\] <p>We can rewrite this as a Reimann sum and convert to an integral.</p> \\[ W_e = \\frac{1}{2} \\int_V p_e(\\vb{r}) V_e(\\vb{r}) \\dd V ; \\quad W_m = \\frac{1}{2} \\int_V p_m(\\vb{r}) V_m(\\vb{r}) \\dd V \\] <p>We can also express this as</p> \\[ W_e = \\frac{1}{2} \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\int_{V'} \\frac{\\rho_e(\\vb{r})\\rho_e(\\vb{r'})}{\\abs{\\vb{r} - \\vb{r'}}} \\dd{V'} \\dd{V} \\] \\[ W_m = \\frac{1}{2} \\frac{1}{4 \\pi \\mu_0} \\int_V \\int_{V'} \\frac{\\rho_m(\\vb{r})\\rho_m(\\vb{r'})}{\\abs{\\vb{r} - \\vb{r'}}} \\dd{V'} \\dd{V} \\] <p>Note the \\(\\frac{1}{2}\\) is the same anti-double-counting factor introduced previously. If we were to determine the potential based on a different set of charges, the factor would be absent.</p> <p>We can now write an expression for energy of a charge density in terms of the field that it produces.</p> \\[ W = \\frac{\\epsilon_0}{2} \\int_V (\\div{\\vb{E}(\\vb{r})}) V(\\vb{r}) \\dd V \\] <p>Simplifying, we see that</p> \\[ W_e = \\frac{\\epsilon_0}{2} \\int_{V} E^2(\\vb{r}) \\dd V ; \\quad W_m = \\frac{\\mu_0}{2} \\int_{V} H^2(\\vb{r}) \\dd V \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-33-the-poisson-and-laplace-equations","title":"Section 3.3 - The Poisson and Laplace Equations","text":"<p>We know that \\(\\vb{E}(\\vb{r}) = -\\div{V_e(\\vb{r})}\\) and \\(\\vb{H}(\\vb{r}) = -\\div{V_m(\\vb{r})}\\)</p> <p>Combinind this, as well as the first of the Maxwell equations, we see that</p> \\[ \\div{\\vb{E}} = -\\div{\\grad{V_e}} = - \\laplacian{V_e} = \\frac{\\rho_e}{\\epsilon_0} \\] \\[ \\div{\\vb{H}} = -\\div{\\grad{V_m}} = - \\laplacian{V_m} = \\frac{\\rho_m}{\\mu_0} \\] <p>The last inequatlity is called the Poisson Equation, or the inhomogenous Laplace equation.</p> <p>To solve this equation, we define a Green function as follows:</p> \\[ \\laplacian G(\\vb{r}, \\vb{r'}) = \\delta(\\vb{r} - \\vb{r'}) \\] <p>Now, we can construct a potential function in terms of said green function that satisfies the lapalce equation.</p> \\[ V_e(\\vb{r}) = - \\int_V G(\\vb{r}, \\vb{r'}) \\frac{\\rho_e(\\vb{r'})}{\\epsilon_0} \\dd{V'} \\] <p>This is the specific solution. Let \\(\\psi(\\vb{r})\\) be a solution to the homogenous equation. We can state the following:</p> \\[ V_e(\\vb{r}) = \\psi(\\vb{r}) - \\int_V G(\\vb{r}, \\vb{r'}) \\frac{\\rho_e(\\vb{r'})}{\\epsilon_0} \\dd{V'} \\] <p>We will consider the potential of a point charge. THat is, the limit of potential is zero as distance approaches infinity.</p> <p>Recall the potential of a point charge:</p> \\[ V_e(\\vb{r}) = \\frac{Q_e}{\\epsilon_0} \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r}}} \\] <p>We know that \\(- \\laplacian{V(\\vb{r})} = \\div{\\vb{E}(\\vb{r})}\\). Thus, recall the electric field of a point charge.</p> \\[ \\vb{E}(\\vb{r}) = -\\grad{V(\\vb{r})} = \\frac{Q_e}{\\epsilon_0} \\grad({\\frac{-1}{4\\pi \\abs{\\vb{r} - \\vb{r'}}}}) = \\frac{Q_e}{\\epsilon_0} \\frac{\\vb{r} - \\vb{r'}}{\\abs{\\vb{r} - \\vb{r'}}^3} \\] <p>Taking the divergence, we find that</p> \\[ - \\laplacian{V(\\vb{r})} = G(\\vb{r}, \\vb{r'}) \\frac{Q_e}{\\epsilon_0} = \\frac{Q_e}{\\epsilon_0} \\laplacian({\\frac{-1}{4\\pi \\abs{\\vb{r} - \\vb{r'}}}}) =  \\frac{Q_e}{\\epsilon_0} \\div \\frac{\\vb{r} - \\vb{r'}}{\\abs{\\vb{r} - \\vb{r'}}^3} = \\frac{Q_e}{\\epsilon_0} \\delta(\\vb{r} - \\vb{r'}) \\] <p>Thus, we see that</p> \\[ \\laplacian {\\frac{-1}{4\\pi \\abs{\\vb{r} - \\vb{r'}}}} = \\delta(\\vb{r} - \\vb{r'}) \\quad \\Rightarrow \\quad G(\\vb{r}, \\vb{r'}) = {\\frac{-1}{4\\pi \\abs{\\vb{r} - \\vb{r'}}}} \\] <p>Finally,</p> \\[ V_e(\\vb{r}) = \\int_{V'} \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r}}} \\frac{\\rho_e}{\\epsilon_0} \\dd{V'} \\] \\[ V_m(\\vb{r}) = \\int_{V'} \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r}}} \\frac{\\rho_m}{\\mu_0} \\dd{V'} \\]"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-34-the-laplace-and-poisson-equations-with-boundary-conditions","title":"Section 3.4 - The Laplace and Poisson Equations with Boundary Conditions","text":"<p>Theorem. The Mean Value Theorem states that if a function satisfies the laplace equation for every point within a region, then the value of the function at the center of the applicable region is equal to the average of the function along the boundary of said region.</p> <p>This can be extended to the Method of Relaxations, in which each point in a mesh is defined by the average of its neighboring points. This is only useful in computer graphics.</p> <p>An interesting consequence of this states that there are no local minima or maxima within said region. The global maximum and minimum must be located at the boundary.</p> <p>Theorem. This leads to Earnshaw's Theorem. To make an electric trap to hold charges, more than zero forces must be applied to the charge, so that if the charge leaves its dedicated position it is forced back. Depending on the sign, at said point, potential must increase or decrease in all directions. However, this would force a local extrema. This cannot be possible.</p> <p>Theorem. The solution to a Laplace or Poisson equation is unique.</p>"},{"location":"physics/electrostatics/3-electro-magnetic-potentials/#section-35-multipole-expansion-of-the-electrostatic-or-magnetostatic-field","title":"Section 3.5 - Multipole Expansion of the Electrostatic or Magnetostatic Field","text":"<p>We want a simple way to write the Green function.</p> <p>Let us assume all charge is contained in a sphere with radius \\(R\\) centered at the origin. Then, for points \\(r\\) far from the origin, the Green function can be written as</p> \\[ \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r'}}} = \\frac{1}{4\\pi \\sqrt{r^2 - 2\\vb{r} \\vdot \\vb{r'} + r'^2}} = \\frac{1}{4\\pi r}(1 - 2 \\vu{r} \\vdot \\vu{r'} + \\frac{r'}{r} + \\frac{r'^2}{r^2})^{-\\frac{1}{2}} \\] <p>This inverse square root term \\((1 - 2 \\vu{r} \\vdot \\vu{r'} + \\frac{r'}{r} + \\frac{r'^2}{r^2})^{-\\frac{1}{2}}\\) can be expanded as a power series in \\(\\frac{r'}{r}\\).</p> <p>The first two terms of this power series are simple enough.</p> \\[ G(\\vb{r}, \\vb{r'}) = \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r'}}} \\approx \\frac{1}{4 \\pi r} ( 1 + \\frac{\\vu{r} \\vdot \\vb{r'}}{r}); \\quad \\text{ for}  r &gt; r' \\] <p>Applying this to the equation for voltage, we see that</p> \\[ V_e(r) = \\frac{1}{\\epsilon_0} \\int_{V'} G(\\vb{r}, \\vb{r'}) p_e(\\vb{r'}) \\dd{V'} \\approx \\frac{1}{4 \\pi \\epsilon_0 r} \\int_{V'}  (1 + \\frac{\\vu{r} \\vdot \\vb{r'}}{r}) p_e(\\vb{r'}) \\dd{V'} = \\frac{Q_e}{4 \\pi \\epsilon_0 r} + \\frac{\\vu{r} \\vdot \\vb{p}}{4 \\pi \\epsilon_0 r^2} \\] <p>Definition. The first and second terms of this equation are the monopole and dipole terms respectively.</p> <p>Definition. We define \\(\\vb{p}\\) as the electric dipole moment, and in the magnetic version, \\(\\vb{m}\\) as the magnetic dipole moment as follows:</p> \\[ \\vb{p} = \\int_{V'} \\vb{r'} \\rho_e(\\vb{r'}) \\dd{V'} \\] <p>Notably, the moments only depend on the charge density, not the point at which the field is being examined. That is, this integral only needs to be computed once.</p> <p>To compute higher-order terms, let \\(\\epsilon = 2\\frac{r'}{r}\\vu{r}\\vdot\\vu{r'}-(\\frac{r'}{r})^2\\). Now we can expand \\((1-\\epsilon)^{-\\frac{1}{2}}\\).</p> \\[ (1-\\epsilon)^{-\\frac{1}{2}} = 1 + \\frac{1}{2}\\epsilon + \\frac{3}{8}\\epsilon^2 + \\frac{5}{16}\\epsilon^3 + \\ldots \\] <p>However, we want an expansion in terms of \\(t = \\frac{r'}{r}\\). To do this, we write the expansion as</p> \\[ \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r'}}} = \\frac{1}{4 \\pi r} \\sum_{n=0}^{\\infty} (\\frac{r'}{r}) P_n(\\vu{r} \\vdot \\vu{r'}) \\] <p>Here, \\(P_n(\\vu{r} \\vdot \\vu{r'})\\) is a polynomial. Because \\(\\abs{\\vb{r} - \\vb{r'}}\\) is symmetric, we can say that if \\(r' &gt; r\\) instead, simply switch the two. Thus the equation becomes</p> \\[ \\frac{1}{4 \\pi \\abs{\\vb{r} - \\vb{r'}}} = \\frac{1}{4 \\pi} \\sum_{n=0}^{\\infty} (\\frac{r'^n_{&lt;}}{r^{n+1}_{&gt;}}) P_n(\\vu{r} \\vdot \\vu{r'}) \\] <p>Where \\(r_&gt;\\) is the greater of \\(r, r'\\), and \\(r_&lt;\\) the lesser.</p> <p>Definition. The polynomials \\(P_n(x)\\) are Legendre Polynomials. We define them as follows:</p> \\[ (1 - 2tx + t^2)^{-\\frac{1}{2}} = \\sum_{n=0}^\\infty t^n P_n(x) \\] <p>Note that as a quirk of the function, \\(P_n(1) = 1\\) for all \\(n\\).</p> <p>We can apply these quadrupole and beyond terms to the volate or other equations, however, this becomes very messy.</p>"},{"location":"physics/electrostatics/4-conductors/","title":"Chapter 4 - Conductors and Static Electric Fields","text":""},{"location":"physics/electrostatics/4-conductors/#section-41-introduction","title":"Section 4.1 - Introduction","text":"<p>We will focus primarily on electric fields and charges. For the purposes for this section, we will assume insulators are perfect.</p>"},{"location":"physics/electrostatics/4-conductors/#section-42-electrostatic-properties-of-a-conductor","title":"Section 4.2 - Electrostatic Properties of a Conductor","text":"<p>In a metal or conductor, there are plentiful charges not bound to a particular atom and are thus free to move throughought the material.</p> <p>We note that there is no electric fiend inside a conductor, as charges internal to the material would move under the force it generates until they find a configuration that eliminates the field. This may happen, but not in electrostatics.</p> <p>Additionally, as the field is zero, it follows from Maxwell's equations that there is no charge inside a conductor. However, charge may be present at the surface. For sufficiently symnetric charges, this charge may be calculated.</p> <p>Consider any two points internal to the conductor. The voltage between said points is defined as \\(\\int_A^B \\vb{E} \\vdot \\dd{\\vb{l}}\\). Since \\(\\vb{E} = 0\\) inside the conductor, the volage difference must be zero. Thus, any two points in or on the surface (TODO: Why on the surface?) of a conductor must be at the same potential.</p> <p>The electric field at the surface of a conductor is perpendicular to its surface. Consider some displacement \\(\\dd{\\vb{l}}\\). Now, \\(\\vb{E} \\vdot \\dd{\\vb{l}} = \\vb{E}_s \\vdot \\dd{\\vb{l}}_s + \\vb{E}_p \\vdot \\dd{\\vb{l}}_p = \\dd{V_s} + \\dd{V_p}\\), in terms of parallel and perpendicular components. The paralell voltage difference is zero, so the electric field must be zero.</p> <p>Consider the surface of a conductor with surface charge density \\(\\sigma_e\\). A cyliner with one end inside and one end outside said surface, with its axis normal to said surface, will be a Gaussian \"pillbox\", which will show that with V being the volume of the pillbox, \\(\\int_V \\div{\\vb{E}} \\dd{V} = \\frac{Q_e}{\\epsilon_0} = \\frac{A\\sigma_e}{\\epsilon_0}\\). Thus, \\(\\sigma_e = \\epsilon_0 E\\).</p>"},{"location":"physics/electrostatics/4-conductors/#section-43-exercises-involving-conductors-at-fixed-potentials","title":"Section 4.3 - Exercises involving conductors at fixed potentials","text":"<p>Consider a square with left and right potentials \\(V(0, y) = V(l, y) = V_1\\) and \\(V(x, 0) = V(x, l) = V_2\\). Since we are uniform in \\(z\\), we can say that \\(V(x, y) = X(x)Y(y)\\) and apply separation of variables.</p> <p>In spherical polar coordinates, we see that with azimuthal symnetry, \\(V(r, \\theta) = \\sum_{l=0}^\\infty a_l r^l P_l(cos\\theta)\\) where \\(P_l(x)\\) are Legendre polynomials.</p> <p>Theorem. 4.3.3: A Laplace equation's solution must be unique inside a volume \\(\\Omega\\) if \\(\\int_{\\dd{\\Omega}}[\\Phi(\\vb{r})\\grad{\\Phi{\\vb{r}}} \\vdot \\vu{n} \\dd{S} = 0]\\). With this, consider a surface \\(\\dd{\\Omega}\\) that surrounds conductors. The integral vanishes if a) the potential is specified on each conductor or b) the total charge on each conductor is specified.</p> <p>Now, define \\(\\Phi(\\vb{r})\\) as the difference between any two potential solutions to the Laplace equation at point \\(\\vb{r}\\). Since potential must be a constant,</p> \\[ \\int_{\\dd{\\Omega}}[\\Phi(\\vb{r})\\grad{\\Phi{\\vb{r}}}] \\vdot \\vu{n} \\dd{S} = \\sum_{i=1}^N \\Phi_i \\int_{\\dd{\\Omega_i}} \\grad{\\Phi{\\vb{r}}} \\vdot \\vu{n} \\dd{S} \\] <p>Thus, if potential is specified, \\(\\Phi_i\\) vanishes for that conductor. If the total charge is instead specified, the gradient vanishes because there is no difference in charge between any two points.</p>"},{"location":"physics/electrostatics/4-conductors/#section-44-electric-field-polarization-field-and-flux-density-in-the-presence-of-conductors","title":"Section 4.4 - Electric Field, Polarization Field, and Flux Density in the Presence of Conductors","text":"<p>Definition. A bound charge is any charge in a conductor that is bound to an atom and not free to be redistributed at the surface. We say that bound charges are the source of the polarization field \\(\\vb{P}\\). Additionally, we note the charge density of bound charges is \\(\\rho_{eb}\\). Thus,</p> \\[ \\div \\vb{P} = - \\rho_{eb} \\] <p>This field is zero outside of a material, and if non-zero inside a material, will drop to zero at the surface discontinuously. If there is a component perpendicular to the surface, the discontinuity will generate curl. If there is a component parallel to the surface, it will generate divergence.</p> <p>Definition. Charges not bound are called free, with density denoted as \\(\\rho_{ef}\\). Combined with \\(\\rho_{eb}\\), they form the basis of the electric field. THat is,</p> \\[ \\epsilon_0 \\div \\vb{E} = \\rho_{ef} + \\rho_{eb} \\] <p>Definition. The electric flux density field \\(\\vb{D}\\) is defined as</p> \\[ \\vb{D} = \\epsilon_0 \\vb{E} + \\vb{P} \\] <p>Both \\(\\vb{D}\\) and \\(\\vb{P}\\) have units of Coulombs/m^2. Additionally, we see that</p> \\[ \\div \\vb{D} = \\div (\\epsilon_0 \\vb{E} + \\vb{P}) = \\div \\epsilon_0 \\vb{E} + \\div \\vb{P} = (\\rho_{ef} + \\rho_{eb}) - \\rho_{eb} = \\rho_{ef} \\]"},{"location":"physics/electrostatics/4-conductors/#section-45-induced-electric-charges-their-potentials-and-fields","title":"Section 4.5 - Induced Electric Charges, their Potentials and Fields","text":"<p>This is an application chapter.</p>"},{"location":"physics/electrostatics/4-conductors/#section-46-capacitance","title":"Section 4.6 - Capacitance","text":"<p>Definition. The capacitance of an object \\(C\\) is the charge per volt, such that</p> \\[ C := \\frac{Q}{V} \\] <p>This unit, \\(\\frac{C}{V}\\), is known as a Farad. For a sphere, \\(C = 4 \\pi \\epsilon_0 R\\). For a parallel plate capacitor, this reduces to \\(C = \\frac{epsilon_0 A}{d}\\).</p>"},{"location":"physics/electrostatics/4-conductors/#section-47-forces-on-charged-conductors-in-electric-fields","title":"Section 4.7 - Forces on Charged Conductors in Electric Fields","text":"<p>We know that \\(\\vb{F} = \\int \\vb{E}_{ext}(\\vb{r}) \\rho_e(\\vb{r}) dV\\), where \\(\\vb{E}_{ext}(\\vb{r})\\) is the external electric field and \\(\\rho_e(\\vb{r})\\) is the charge density of the object.</p>"},{"location":"physics/electrostatics/5-moving-charges/","title":"Chapter 5 - Electrodynamics with Moving Charges","text":""},{"location":"physics/electrostatics/5-moving-charges/#section-51-currents-in-steady-state-regine","title":"Section 5.1 - Currents in Steady-State Regine","text":"<p>We want to work in a steady-state system. Thus, we restrict ourselves to currents that do not change in time.</p> <p>With math, we see that \\(\\div \\vb{J}(\\vb{r}) = -\\frac{\\partial \\rho(\\vb{r})}{\\partial t}\\). Since we are only considering a steady-state system, \\(\\div \\vb{J}_e = \\div \\vb{J}_m = 0\\).</p> <p>Definition. The conductance of a material is \\(G = \\frac{1}{R}\\), where \\(R\\) is the resistance of a material.</p> <p>For a wire of uniform cross-sectional area, we see that \\(G = \\sigma \\frac{A}{L}\\), where \\(A\\) is the cross-sectional area, \\(L\\) is the length of the wire, and \\(\\sigma\\) is the conductivity of a wire. Inverted, we see that \\(R\\) = \\(\\rho \\frac{L}{A}\\), where \\(\\rho = \\frac{1}{\\sigma}\\) is the resistivity of the wire.</p> <p>Definition. Ohm's Law can be written as \\(I = G V\\), or inverted, \\(V = IR\\). In a wire, we see that current density \\(\\vb{} = \\frac{I}{A} = \\sigma \\frac{V}{L} = \\sigma \\vb{E}\\)</p>"},{"location":"physics/electrostatics/5-moving-charges/#section-52-currents-and-curling-fields","title":"Section 5.2 - Currents and Curling Fields","text":"<p>We know that \\(\\vb{J}_e = \\curl{\\vb{H}}\\) and \\(\\vb{J}_m = -\\curl{\\vb{E}}\\). That is, current densities cause the opposing field to curl.</p> <p>For a wire with current \\(I_e\\), we see that applying Stoke's theorem to the first equation,</p> <p>$ \\int_S \\curl{\\vb{H}} \\vdot \\vu{n} \\dd{S} = \\int_{\\partial S} = \\vb{H} \\vdot \\dd{\\vb{l}}$. Apply the identity \\(\\curl{\\vb{H}} = \\vb{J}_e\\) to the left side to see that \\(\\int_S \\curl{\\vb{H}} \\vdot \\vu{n} \\dd{S} = \\int_S \\vb{J}_e \\vdot \\vu{n} \\dd{S} = (I_e)_S\\), or the current passing through the cross-sectional area. By the original equation, we see that \\((I_e)_S = \\vb{H} \\vdot \\dd{\\vb{l}}\\).</p> <p>If we assume cylindrical coordinates and that \\(\\vb{H}(vb{r}) = H_\\varphi(s) \\vu{\\varphi}\\), then \\(\\vb{H} \\vdot \\dd{\\vb{l}} = \\int_0^{2\\pi} H_\\varphi(S) s \\dd{\\varphi}\\), so then \\((I_e)_S = \\int_0^{2\\pi} H_\\varphi(S) s \\dd{\\varphi}\\). Thus, for \\(s &gt; a\\) (where \\(a\\) is the radius of the wire), \\(2\\pi s H_\\varphi = I_e\\), and for \\(s &lt; a\\), \\(2\\pi s H_\\varphi = I_e \\frac{s^2}{a^2}\\).</p> <p>By Helmholtz Theorem, we know that \\(\\vb{H}(\\vb{r}) = \\curl{\\vb{A}(\\vb{r})}\\). For a current-carying wire, \\(\\vb{A}(\\vb{r}) = \\frac{I_e}{4\\pi} \\int_{\\text{wire}} \\frac{\\dd{\\vb{l'}}}{|\\vb{r}-\\vb{r'}|}\\). Applying identities, we see the *Law of Biot and Savart$, where</p> \\[ \\vb{H}(\\vb{r}) = \\int{I_e}{4\\pi}\\int_{\\text{wire}} \\frac{-(\\vb{r}-\\vb{r'}) \\cross \\dd{\\vb{l'}}}{|\\vb{r}-\\vb{r'}|^3} \\] <p>Consider a current loop instead, on the \\(x-y\\) plane and current \\(I\\). Then, \\(r = z \\vu{z}\\) and \\(\\dd{\\vb{l'}} = R \\vu{\\varphi'} \\dd{\\phi'}\\), and the magnetic field collapses to \\(\\vb{H}(s = 0, z) = \\frac{I_e R^2}{2(R^2 + z^2)^{\\frac{3}{2}}} \\vu{z}\\)</p> <p>Consider some infinite bar magnet with height \\(h\\) and width \\(w\\). Then, the top and bottom surfaces will have a magnetic charge with density \\(\\vb{J}_m^+ = M_0 \\vb{b} \\delta(z - h)\\) and \\(\\vb{J}_m^- = -M_0 \\vb{v} \\delta(z)\\) respectively. By definition, \\(I_m = M_0 w v\\).</p> <p>Now, consider a loop around only the top of the conductor. Then,</p> \\[ \\int_S \\vb{J}_m \\vdot \\vu{n} \\dd{S} = I_m = M_o w v \\] <p>By definition,</p> \\[ \\int_S \\vb{J}_m \\vdot \\vu{n} \\dd{S} = -\\int_S (\\curl{\\vb{E}}) \\vdot \\vu{n} \\dd{S} \\] <p>Applying Stokes theorem,</p> \\[ \\int_S (\\curl{\\vb{E}}) \\vdot \\vu{n} \\dd{S} = M_0 w v \\]"},{"location":"physics/electrostatics/5-moving-charges/#section-53-forces-on-moving-charges-and-current","title":"Section 5.3 - Forces on Moving Charges and Current","text":"<p>Consider an electric charge moving with velocity \\(\\vb{v}\\) in a magnetic parallel plate capacitor with charge densities \\(\\plusminus \\sigma_m\\). That is, \\(\\mu_0 \\vb{H} = \\sigma_m \\vu{z}\\). Then, we can apply theorems to see the resulting force.</p> <p>Theorem. Lorentz Force Law states that \\(\\vb{F} = q_e \\vb{v} \\cross \\u_0 \\vb{H}\\) in the presence of a magnetic field. In the presence of both an electic andmagnetic field, \\(\\vb{F} = q_e (\\vb{E} + \\vb{v} \\cross \\u_0 \\vb{H})\\).</p> <p>Theorem. Ampere's Force Law states that generalizing the previous theorem, we can see that</p> \\[ \\dd{\\vb{F}} = I_e \\dd{\\vb{L}} \\cross \\u_0 \\vb{H}(\\vb{r}) \\]"},{"location":"physics/electrostatics/5-moving-charges/#section-54-multipole-expansion-of-a-vector-potential","title":"Section 5.4 - Multipole Expansion of a Vector Potential","text":"<p>This is messy. Skipped.</p>"},{"location":"recipes/bread/","title":"Breads","text":""},{"location":"recipes/bread/#pumpkin-bread","title":"Pumpkin Bread","text":"<ul> <li>Preheat oven to \\(350 \\degree\\) F.</li> <li>Combine \\(1 \\frac{2}{3}\\) cups flour, \\(1 \\frac{1}{2}\\) cups sugar, 1 tsp. baking soda, 1 tsp cinnamon, \\(\\frac{3}{4}\\) tsp. salt, \\(\\frac{1}{2}\\) tsp. baking powder, \\(\\frac{1}{2}\\) tsp. nutmeg, \\(\\frac{1}{4}\\) tsp cloves.</li> <li>In a separate bowl, combine 2 eggs, 1 can of pumpkin, \\(\\frac{1}{2}\\) cup canola oil, and \\(\\frac{1}{2}\\) cups water.</li> <li>Combine. Mix in \\(\\frac{1}{2}\\) cups of walnuts.</li> <li>Add to a greased 9x5 pan. Bake at \\(350 \\degree\\) F for 65-80 minutes.</li> </ul>"},{"location":"recipes/cookies/","title":"Cookies","text":""},{"location":"recipes/cookies/#chocolate-chip-cookies","title":"Chocolate Chip Cookies","text":"<p>Original: Link</p> <ul> <li>Preheat oven to \\(350 \\degree\\) F.</li> <li>In a small bowl, combine \\(1 \\frac{1}{2}\\) cups flour and \\(\\frac{1}{4}\\) tsp. salt</li> <li>In a medium bowl, mix together \\(\\frac{1}{2}\\) cup butter, \\(\\frac{1}{2}\\) cup sugar, \\(\\frac{1}{2}\\) cup brown sugar.</li> <li>To the wet ingredients, add 1 egg and 1 tsp. vanilla.</li> <li>To the wet ingredients, add a combination of 1 tsp. hot water and \\(\\frac{1}{2}\\) tsp. baking soda.</li> <li>Mix the wet and dry ingredients. Stir in 1 cup chocolate chips and optionallt \\(\\frac{1}{2}\\) cup walnuts.</li> <li>Bake for 10m at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/cookies/#peanut-butter-cookies","title":"Peanut Butter Cookies","text":"<ul> <li>Preheat oven to \\(350 \\degree\\) F.</li> <li>Mix \\(\\frac{1}{2}\\) cup butter, \\(\\frac{1}{2}\\) cup peanut butter, \\(\\frac{1}{2}\\) cup sugar, \\(\\frac{1}{2}\\)  cup brown sugar.</li> <li>Whisk in 1 egg.</li> <li>In a separate bowl, prepare \\(1 \\frac{1}{4}\\) cup sugar, \\(\\frac{3}{4}\\) tsp. baking soda, \\(\\frac{1}{2}\\) tsp. baking powder, and \\(\\frac{1}{4}\\) tsp. salt.</li> <li>Combine. Bake for 10 minutes at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/cookies/#lemon-cookies","title":"Lemon Cookies","text":"<ul> <li>Preheat oven to \\(350 \\degree\\) F.</li> <li>Mix 2 cups flour, \\(\\frac{1}{2}\\) baking soda, \\(\\frac{1}{2}\\) tsp. salt.</li> <li>In a separate bowl, mix \\(\\frac{1}{2}\\) cup butter, 1 cup sugar</li> <li>Whisk 1 egg, 1 tsp. vanilla into wet ingredients.</li> <li>Combine. Mix in a lemon worth of zest and juice.</li> <li>Bake for 10m at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/cookies/#glaze-optional","title":"Glaze (Optional)","text":"<ul> <li>Combine 2 cups of powdered sugar, 2 tbsp. lemon zest, and \\(\\frac{1}{3}\\) cup of lemon juice.</li> </ul>"},{"location":"recipes/cookies/#snickerdoodle-cookies","title":"Snickerdoodle Cookies","text":"<ul> <li>Preheat oven to \\(400 \\degree\\) F.</li> <li>Blend together \\(\\frac{3}{4}\\) cup sugar, \\(\\frac{1}{4}\\) cup butter (softened), $\\frac{1}{4} cup shortening.</li> <li>Add 1 egg, 1 tsp. vanilla</li> <li>In a separate bowl, combine \\(1 \\frac{1}{4}\\) cups flour, 1 tsp. cream of tartar, \\(\\frac{1}{2}\\) tsp. baking soda, and \\(\\frac{1}{4}\\) tsp. salt</li> <li>Combine</li> <li>Coat in a mix of 1 tbsp. sugar : 1 tsp. cinnamon</li> <li>Bake for 10 minutes at \\(400 \\degree\\) F.</li> </ul>"},{"location":"recipes/cupcakes/","title":"Cupcakes","text":""},{"location":"recipes/cupcakes/#lemon-cupcakes","title":"Lemon Cupcakes","text":"<ul> <li>Preheat oven to \\(350 \\degree\\) F</li> <li>Combine \\(1 \\frac{1}{4}\\) cups flour, \\(\\frac{3}{4}\\) tsp. baking powder, \\(\\frac{1}{4}\\) tsp. baking soda.</li> <li>In a separate bowl, combine \\(\\frac{1}{4}\\) cup butter, \\(\\frac{3}{4}\\) cup sugar, \\(\\frac{1}{4}\\) cup vegetable oil, \\(\\frac{1}{4}\\) tsp. vaninna</li> <li>Add 2 eggs.</li> <li>Combine with half of the dry mixture. Add 6 tbsp. milk, 1 lemon of zest and juce. Stir. Add the rest of the dry ingredients.</li> <li>Bake for 15-18 minutes at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/cupcakes/#frosting","title":"Frosting","text":"<ul> <li>Beat 1 cup butter and 4 cups of powdered sugar. Add \\(\\frac{1}{2}\\) tsp. vanilla, 4-5 tbsp. lemon juice, 1 tsp. zest, and salt to taste.</li> </ul>"},{"location":"recipes/meals/","title":"Meals","text":""},{"location":"recipes/meals/#rice-porridge","title":"Rice Porridge","text":"<ul> <li>Boil 2 cups of chicken broth. Add onion and garlic powder.</li> <li>Add 1 cup of rice. Boil till soft.</li> <li>Mix ground beef with \\(\\frac{1}{2}\\) cups of chicken broth. Add to rice, boil for approx. 5 minutes.</li> <li>Add salt, pepper, fish sauce.</li> </ul>"},{"location":"recipes/meals/#banh-bao","title":"Banh Bao","text":"<p>Source: link.</p> <ul> <li> <p>For the filling, combine \\(\\frac{3}{4}\\) lb. ground pork, \\(3\\) tbsp. wood ear mushrooms, \\(\\frac{1}{4}\\) cup minced yellow onions, \\(2\\) tsp. fish sauce, \\(2\\) tsp. oyster sauce, \\(1\\) tsp sugar, \\(\\frac{1}{2}\\) tsp. salt, \\(\\frac{1}{2}\\) tsp. pepper, 2 sliced Chinese sausages, and \\(4\\) boiled eggs, cut into quarters.</p> </li> <li> <p>For the dough, add \\(2 \\frac{1}{2}\\) cup flour and \\(2\\) tsp baking powder to a bowl</p> </li> <li>Heat \\(\\frac{3}{4}\\) cup whole milk to \\(\\approx 100 \\deg \\F\\), add instant yeast. Set aside for \\(\\approx 10\\) minutes or until foams.</li> <li>Add \\(1\\) tbsp vegetable oil and \\(\\frac{1}{2}\\) cup sugar</li> <li>Combine wet and dry ingredients</li> <li>Knead on floured surface for \\(\\approx 10\\) minutes.</li> <li> <p>Transfer to an oiled bowl, cover, and wait for approx. 1 hour</p> </li> <li> <p>Bring water to a simmer, add \\(1\\) tsp. rice wine vinegar. Steam for \\(\\approx 15-17\\) minutes.</p> </li> </ul>"},{"location":"recipes/pies/","title":"Pies","text":""},{"location":"recipes/pies/#pie-crust-1","title":"Pie Crust 1","text":"<ul> <li>Whisk \\(1 \\frac{1}{4}\\) cups of flour with \\(\\frac{1}{4}\\) tsp. salt</li> <li>Cut in \\(\\frac{1}{2}\\) cups of cubed butter (chilled), \\(\\frac{1}{4}\\) cups cold water.</li> <li>Refridgerate</li> </ul>"},{"location":"recipes/pies/#pie-crust-2","title":"Pie Crust 2","text":"<ul> <li>Mix \\(\\frac{1}{3}\\) cup flour, \\(\\frac{1}{3}\\) tsp. salt.</li> <li>Cut in \\(\\frac{1}{2}\\) cups of shortening, 3 tbsp. cold water.</li> <li>Refridgerate</li> </ul>"},{"location":"recipes/pies/#pumpkin-pie-1","title":"Pumpkin Pie 1","text":"<ul> <li>Preheat oven to \\(425 \\degree\\) F.</li> <li>Whisk together 1 can pumpkin, 1 can sweetened-condensed milk, and two eggs.</li> <li>Add 1 tsp. cinnamon, \\(\\frac{1}{2}\\) tsp. ginger, \\(\\frac{1}{2}\\) tsp. nutmeg, \\(\\frac{1}{2}\\) tsp. salt.</li> <li>Add to pie crust.</li> <li>Bake for 15 minutes at \\(245 \\degree\\) F, then for 35-40 minutes at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/pies/#pumpkin-pie-2","title":"Pumpkin Pie 2","text":"<p>From: Link</p> <ul> <li>Fits with Pie Crust 2</li> <li>Preheat oven to \\(425 \\degree\\) F.</li> <li>Whisk together 2 can pumpkin, 2 cans evaporated milk, two eggs, and \\(\\frac{3}{4}\\) cups brown sugar.</li> <li>Add \\(\\frac{1}{2}\\) tsp. cinnamon, \\(\\frac{1}{2}\\) tsp. ginger, \\(\\frac{1}{2}\\) tsp. nutmeg, \\(\\frac{1}{2}\\) tsp. salt.</li> <li>Add to pie crust.</li> <li>Bake for 15 minutes at \\(245 \\degree\\) F, then for 35-40 minutes at \\(350 \\degree\\) F.</li> </ul>"},{"location":"recipes/pies/#apple-hand-pie","title":"Apple Hand Pie","text":"<p>From: Link</p>"},{"location":"recipes/pies/#filling","title":"Filling","text":"<ul> <li>Cut 2 apples into slices.</li> <li>Mix \\(\\frac{1}{4}\\) cups sugar, 2 tbsp. brown sugar, \\(1 \\frac{1}{4}\\) tsp. cinnamon, and \\(\\frac{1}{4}\\) tsp. salt together with 1 tsp. water.</li> <li>Brown 2 tbsp. butter on mild heat.</li> <li>Mix in apples, add sugar mixture.</li> <li>Wait until apples are softened (approx. 5 minutes).</li> </ul>"},{"location":"recipes/pies/#hand-pies","title":"Hand Pies","text":"<ul> <li>Preheat oven to \\(400 \\degree\\) F.</li> <li>Split pie crust into 4. Place fillin in crust, fold.</li> <li>Sprinkle with \\(\\frac{1}{4}\\) tsp. white sugar.</li> <li>Whisk 2 tsp. milk, 1 egg. Brush pastries.</li> <li>Bake at \\(400 \\degree\\) F. for 25-30 minutes.</li> </ul>"},{"location":"recipes/snacks/","title":"Snacks","text":""},{"location":"recipes/snacks/#seasoned-chex-mix","title":"Seasoned Chex Mix","text":"<ul> <li>Preheat oven to \\(250 \\degree F\\).</li> <li>Combine 9-10 cups Chex (approx. 1 box).</li> <li>Melt 8 tbsp. butter. COmbine with 2 tbsp. worcestershire sauce, 2 tbsp. seasoned salt, 1 tsp. garlic power, and 1 tsp. onion powder.</li> <li>Pour half of mixture into mixed chex, stir, add other half, repeat.</li> <li>Bake at \\(250 \\degree F\\) for approx. 45-60 minutes using 2-3 oven trays, stirring every 15 minutes.</li> </ul>"},{"location":"recipes/snacks/#buckeyes","title":"Buckeyes","text":"<ul> <li>Mix \\(1 \\frac{1}{2}\\) cup peanut butter with 1 cup butter (softened), \\(\\frac{1}{2}\\) tsp. vanilla, and 4 cups of powdered sugar.</li> <li>Refridgerate.</li> <li>Melt 4 cups of chocolate chips. Roll peanut butter mixture into balls, coat in chocolate.</li> </ul>"},{"location":"recipes/snacks/#chocolate-fudge","title":"Chocolate Fudge","text":"<ul> <li>Melt 1 bag of chocolate chips on low. Stir in 1 can of sweeteneed condensed milk. Pour into buttered or oiled tray.</li> </ul>"},{"location":"recipes/snacks/#peanut-butter-fudge","title":"Peanut Butter Fudge","text":"<ul> <li>Melt \\(\\frac{1}{2}\\) cup of butter on medium heat. Stir in a 16oz bag of brown sugar, \\(\\frac{1}{2}\\) cup of milk.</li> <li>Remove from heat. Stir in \\(\\frac{3}{4}\\) cups of peanut butter and 1 tsp banilla.</li> <li>Add to \\(3 \\frac{1}{2}\\) cups of powdered sugar</li> <li>Pour into buttered or oiled tray.</li> </ul>"}]}